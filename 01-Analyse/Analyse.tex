%\documentclass[draft]{article}
\documentclass[table]{article}
\usepackage[francais]{babel}
\usepackage{float}
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{geometry}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{sectsty}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{animate}
\usepackage{braket}

\input{structure.tex}

\title{
\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt}
\rule{\textwidth}{0.4pt}\\[\baselineskip]
\textbf{DE L'ANALYSE À LA THÉORIE DES NOMBRES}\\
Petit recueil de démonstrations et d'exemples
\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt}
\rule{\textwidth}{1.6pt}\\[\baselineskip]
}
\author{
\small{Loïc HUGUEL}\\
\small{\href{mailto:loic.huguel88@gmail.com}{loic.huguel88@gmail.com}}\\
\vspace{0.5cm}\\
\LaTeX\\
TikZ version : \pgfversion
}

\begin{document}

\maketitle

Nous débuterons par la notion centrale de \textbf{dérivation complexe}, qui nous amènera ensuite à définir les \textbf{fonctions holomorphes}, c'est-à-dire les fonctions dérivables au sens complexe et qui vérifient donc \textbf{les équations de Cauchy-Riemann}. Nous verrons que toute fonction holomorphe est une \textbf{fonction harmonique} et est également une \textbf{fonction analytique}.\\

On abordera la construction de la \textbf{sphère de Riemann} et étudierons ses automorphismes, les \textbf{transformations de Möbius}. On se familiarisera avec les développements en \textbf{séries de Taylor-Laurent}, ce qui nous permettra d'introduire au passage les \textbf{nombres de Bernoulli}. Nous aborderons le très important \textbf{théorème des résidus} et le \textbf{principe de l'argument}. Nous aborderons la notion passionnante de \textbf{prolongement analytique}.\\

On démontrera la célèbre formule de \textbf{sommation d'Euler-Maclaurin}, puis les \textbf{formules de sommations d'Abel}. Le problème du calcul du produit de deux séries nous permettra d'aborder la \textbf{convolution de Dirichlet}. La convolution de Dirichlet nous amènera à définir \textbf{la fonction de Mobius} avec la célèbre formule d'inversion, ainsi que d'autres fonctions arithmétiques.\\

On démontrera la \textbf{formule de Jensen} ce qui nous conduira à une démonstration du \textbf{théorème fondamental de l'algèbre}. Ainsi que la \textbf{factorisation de Weierstrass} qui nous permet de construire une fonction par la connaissance de ses zéros.\\ 

Nous rappellerons quelques notions d'analyse fréquentielle avec les \textbf{séries et transformées de Fourier} qui nous seront utiles pour calculer certaines sommes. Nous aborderons également les \textbf{transformées de Mellin}.\\

Nous serons alors armés pour faire l'étude de quelques fonctions passionnantes, tel que les fonctions : \textbf{exponentielle, logarithme, exponentielle intégrale, logarithme intégral}... puis quelques fonctions spéciales, tel que les : \textbf{fonction Gamma}, \textbf{fonction Beta}... On abordera le calcul de certaines intégrales remarquables : \textbf{intégrales d'Euler}, \textbf{intégrales de Fresnel}, \textbf{intégrales de Dirichlet}, \textbf{intégrales de Gauss}, \textbf{intégrales de Wallis}, \textbf{intégrales de Fourier}. Certaines de ses fonctions sont à la frontière entre \textbf{la théorie des nombres et l'analyse}, elles sont encore mal comprises et semblent êtres les gardiennes de nombreux secrets. La plus célèbre est sans doute la fonction \textbf{zêta de Riemann} qui fait le lien étroit entre l'analyse et l'arithmétique. Nous terminerons par une démonstration, s'appuyant sur la notion de transformée de Mellin, de la formule \textbf{explicite de Riemann}.\\

Bonne lecture !\\

\newpage
\tableofcontents
\newpage
\listoffigures

\newpage
\section{Nombres complexes}
\subsection{Définition}

Commençons simplement par rappeler que les nombres complexes  notés $\mathbb{C}$, forment \textbf{un groupe}, formé au dessus \textbf{du groupe} des nombres réels noté $\mathbb{R}$ par l'ajout d'un élément noté $i$ tel que :

\begin{equation}
\boxed{i^2=-1}
\end{equation}

Tout nombre complexe s'écrit alors comme:

\begin{equation}
\boxed{z=a+i b \quad\quad avec\quad \quad a,b\in\mathbb{R}}
\end{equation}

Ou $a$ et $b$ sont respectivement les parties réelle et imaginaire de $z$.\\

\subsection{Opérations et propriétés élémentaires}


\subsubsection{Addition, produit, commutativité}

Soit $z=a+ib$ et $w=c+id$ alors on a trivialement :

\[
\boxed{z+w=w+z= (a+c)+i(b+d)}
\]

Calculons le produit:
\[
\begin{array}{ll}
z w&=(a+ib)(c+id)\\
&=ac + i (ad+bc) + bd i^2\\
\end{array}
\]
Or $i^2=-1$ et on constate que si on commute le produit, le résultat ne change pas, donc :
\[
\boxed{z w=w z= ac-bd + i (ad+bc)}
\]

\subsubsection{Conjugué, norme et inverse}

Soit $z=a+ib$, on définit le conjugué de $z$:

\[
\boxed{\bar{z}= a-ib}
\]

On définit également la norme de $z$ par :

\[
\boxed{|z|=\sqrt{z\bar{z}} \quad \quad soit \quad \quad |z|=\sqrt{a^2+b^2} }
\]

L'inverse d'un nombre complexe de norme \textbf{non nul} se calcul alors par:
\[
\frac{1}{z}=\frac{\bar{z}}{z\bar{z}}
\]
Soit
\[
	\boxed{\frac{1}{z}=\frac{\bar{z}}{|z|^2}=\frac{a}{\sqrt{a^2+b^2}}-i\frac{b}{\sqrt{a^2+b^2}}}
\]

\subsubsection{Propriété du conjugué\todo}

Soit $z$ et $w$ deux nombres complexes, alors:

\[
\boxed{\overline{w z}=\bar{w} \bar{z}\quad\quad et\quad\quad \overline{w+z}=\bar{w}+\bar{z}}
\]

Si on pose $z=a+ib$ et $w=c+id$ alors, en effet on a:

\[
\begin{array}{lcl}
\bar{w} \bar{z} &=& (\overline{a+ib})(\overline{c+id})\\
&=& (a-ib)(c-id)\\
&=& ac-bd-i(bc+da)\\
&=&\overline{ac-bd+i(bc+da)}\\
&=&\overline{w z}\\
\end{array}
\begin{array}{ccc}
\bar{w}+\bar{z} &=& (\overline{a+ib})+(\overline{c+id})\\
&=& (a-ib)+(c-id)\\
&=& a+c-i(b+d)\\
&=& \overline{a+c+i(b+d)}\\
&=& \overline{w+z}\\
\end{array}
\]

\newpage
\section{Dérivation complexe}

\subsection{Définition}
\label{derive_definition}

Soit $f$ une fonction de $\mathbb{C}$ dans $\mathbb{C}$, on peut définir la dérivée de $f$ en $z$ de plusieurs manières équivalentes :

\begin{equation}
\boxed{f_n(z,\epsilon)=\left\{
\begin{array}{l}
f_1(z,\epsilon)=\frac{f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}\\
f_2(z,\epsilon)=\frac{f(z+\epsilon)-f(z)}{\epsilon}\\
f_3(z,\epsilon)=\frac{f(\epsilon)-f(z)}{\epsilon-z}\\
...\\
\end{array}
\right.
\quad\quad
f'(z)=\left\{
\begin{array}{l}
\lim_{\epsilon\to 0} f_1(z,\epsilon)\\
\lim_{\epsilon\to 0} f_2(z,\epsilon)\\
\lim_{\epsilon\to z} f_3(z,\epsilon)\\
...\\
\end{array}
\right.}
\end{equation}
\vspace{0.5cm}

Les estimateurs $f_n(z,\epsilon)$ ci dessus, sont les plus fréquemment utilisés. Les limites définies ci-dessus sont équivalentes et donnent ce que l'on nomme la dérivée de la fonction $f$ en $z$, notée $f'(z)$.\\

La notion de dérivabilité au sens complexe \textbf{est beaucoup plus forte et contraignante} sur la fonction étudiée que la notion de dérivabilité au sens réel. \textbf{Il existe en effet une infinité de manière d'approcher la limite en un point}. Si toutes les limites mènent à la même valeur $f'(z)$ alors la fonction est dérivable en ce point $z$. On dira que $f$ est \textbf{holomorphe} sur $D$ si $\forall z\in D$ , la dérivée $f'(z)$ existe.\\

Au sens des opérateurs, on définira \textbf{l'opérateur de dérivation} $\op{D}$. tel que :

\begin{equation}
\boxed{\left\{
\begin{array}{l}
\applyop{D}{f}{(z)}=f'(z)\\
\applyopp{D}{n}{f}{(z)}=f^{(n)}(z)\\
\end{array}
\right.}
\end{equation}

\subsection{Propriétés}

\subsubsection{Linéarité}
\label{derive_linearite}
D'après la définition :
\[
(af(z)+bg(z))'=\lim_{\epsilon\to 0} \frac{ af(z+\epsilon)+bg(z+\epsilon)-(af(z-\epsilon)+bg(z-\epsilon))}{2\epsilon}
\]
Et si on regroupe les $f(z)$ et les $g(z)$ :
\[
(af(z)+bg(z))'=\lim_{\epsilon\to 0} \frac{ a(f(z+\epsilon)-f(z-\epsilon))}{2\epsilon}+\frac{b(g(z+\epsilon)-g(z-\epsilon))}{2\epsilon}
\]
Soit si on sépare les limites :
\[
(af(z)+bg(z))'=a \lim_{\epsilon\to 0} \frac{ f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}+b\lim_{\epsilon\to 0}\frac{g(z+\epsilon)-g(z-\epsilon)}{2\epsilon}
\]
Soit
\[
(af(z)+bg(z))'=a f'(z)+ b g'(z)
\]

Finalement du point de vue de l'opérateur on peut noter:
\begin{equation}
\boxed{\applyop{D}{af(z)+bg(z)}{(z)}=a\applyop{D}{f(z)}{(z)}+b\applyop{D}{g(z)}{(z)}}
\end{equation}

\subsubsection{Produit}
\label{derive_produit}
D'après la définition :
\[
(f(z)g(z))'=\lim_{\epsilon\to 0} \frac{ f(z+\epsilon) g(z+\epsilon)-f(z-\epsilon) g(z-\epsilon)}{2\epsilon}
\]
Mais on peut remarquer que : 
\[
(f(z)g(z))'=\lim_{\epsilon\to 0} \frac{ (f(z+\epsilon)-f(z-\epsilon)) g(z+\epsilon)+(g(z+\epsilon)-g(z-\epsilon)) f(z-\epsilon)}{2\epsilon}
\]
Et si on sépare les limites :
\[
(f(z)g(z))'=\lim_{\epsilon\to 0} \frac{ (f(z+\epsilon)-f(z-\epsilon))}{2\epsilon} g(z+\epsilon)+\lim_{\epsilon\to 0}\frac{(g(z+\epsilon)-g(z-\epsilon))}{2\epsilon} f(z-\epsilon)
\]
Finalement comme $\lim_{\epsilon\to 0} f(z-\epsilon)=f(z)$ et $\lim_{\epsilon\to 0} g(z+\epsilon)=g(z)$:

\[
(f(z)g(z))'=f'(z)g(z)+g'(z)f(z)
\]

Du point de vue de l'opérateur on peut noter:
\begin{equation}
\boxed{\applyop{D}{f(z)g(z)}{(z)}=g(z)\applyop{D}{f(z)}{(z)}+f(z)\applyop{D}{g(z)}{(z)}}
\end{equation}

\subsubsection{Inverse}
\label{derive_inverse}
D'après la définition :
\[
\left(\frac{1}{f(z)}\right)'=\lim_{\epsilon\to 0} \frac{ \frac{1}{f(z+\epsilon)}-\frac{1}{f(z-\epsilon)}}{2\epsilon}
\]
Et si on met sous le même dénominateur :
\[
\left(\frac{1}{f(z)}\right)'=\lim_{\epsilon\to 0} \frac{ f(z-\epsilon)-f(z+\epsilon)}{2\epsilon } \frac{1}{f(z-\epsilon)f(z+\epsilon)}
\]
Et lorsque on prend la limite on obtient finalement :

\[
\left(\frac{1}{f(z)}\right)'=-\frac{f'(z)}{f(z)^2}
\]

Du point de vue de l'opérateur on peut noter:
\begin{equation}
\boxed{\applyop{D}{\frac{1}{f(z)}}{(z)}= \frac{\applyop{D}{f(z)}{(z)}}{f(z)^2}}
\end{equation}

\subsubsection{Quotient}
\label{derive_quotient}
D'après le résultat sur les produits :
\[
\left(\frac{f(z)}{g(z)}\right)'=f'(z)\frac{1}{g(z)}+\left(\frac{1}{g(z)}\right)'f(z)
\]
Et d'après le résultat sur l'inverse :
\[
\left(\frac{f(z)}{g(z)}\right)'=f'(z)\frac{1}{g(z)}-\frac{g'(z)}{g(z)^2}f(z)
\]
Et si on met sous le même dénominateur :
\[
\left(\frac{f(z)}{g(z)}\right)'=f'(z)\frac{g(z)}{g(z)^2}-\frac{g'(z)}{g(z)^2}f(z)
\]
Soit
\[
\left(\frac{f(z)}{g(z)}\right)'=\frac{f'(z)g(z)-g'(z)f(z)}{g(z)^2}
\]
Du point de vue de l'opérateur on peut noter:
\begin{equation}
\boxed{\applyop{D}{\frac{f(z)}{g(z)}}{(z)}=\frac{g(z)\applyop{D}{f(z)}{(z)}-f(z)\applyop{D}{g(z)}{(z)}}{g(z)^2}}
\end{equation}

\subsubsection{Composée}
\label{derive_composition}
D'après seconde définition :
\[
(g(f(z)))'=\lim_{\epsilon\to 0} \frac{g(f(z+\epsilon))-g(f(z))}{\epsilon}
\]
On peut poser $k=f(z+\epsilon)-f(z)$, on obtient :
\[
(g(f(z)))'=\lim_{\epsilon\to 0} \frac{g(f(z)+k)-g(f(z))}{\epsilon}
\]
$k$ est un infinitésimal mais il est toujours supérieur à zéro en valeur absolue $|k|>0$, on peut donc écrire:
\[
(g(f(z)))'=\lim_{\epsilon\to 0} \frac{g(f(z)+k)-g(f(z))}{k}\frac{k}{\epsilon}
\]
Soit
\[
(g(f(z)))'=\lim_{\epsilon\to 0} \frac{g(f(z)+k)-g(f(z))}{k}\frac{f(z+\epsilon)-f(z)}{\epsilon}
\]
Comme $\lim_{\epsilon\to 0} k=0$, on peut écrire:

\[
(g(f(z)))'=g'(f(z))f'(z)
\]

Du point de vue de l'opérateur on peut noter:
\begin{equation}
\boxed{\applyop{D}{f(g(z))}{(z)}=f(z)\applyop{D}{g(z)}{(z)}-\applyop{D}{f(z)}{(z)}}
\end{equation}

\subsubsection{Puissance}
\label{derive_puissance}
D'après la relation obtenue pour les produits (voir \ref{derive_produit}):

\[
\begin{array}{ll}
(f^a)'&=(f^{a-1}f)'\\
&=(f^{a-1})'f+f'f^{a-1}\\
&=((f^{a-2})'f+f'f^{a-2})f+f'f^{a-1}\\
&=(f^{a-2})'f^2+f'f^{a-1}+f'f^{a-1}\\
&=(f^{a-2})'f^2+2f'f^{a-1}\\
&=...\\
\end{array}
\]
On peut alors voir que $\forall n$:
\[
	(f^a)'=(f^{a-n})'f^n+n f'f^{a-1}
\]
Et si on choisit $n=a$ alors on obtient:

\[
\begin{array}{ll}
(f^a)'&=(f^{a-a})'f^a+a f'f^{a-1}\\
&=(1)'f^a+a f'f^{a-1}\\
&=a f'f^{a-1}\\
\end{array}
\]
Donc finalement :

\[
(f(z)^a)'=a f(z)'f(z)^{a-1}
\]

Du point de vue de l'opérateur on peut noter:
\begin{equation}
\boxed{\applyop{D}{f(z)^a}{}=af(z)^{a-1}\applyop{D}{f(z)}{(z)}}
\end{equation}

\newpage
\subsection{Exemples}

\subsubsection{Fonction identité}
\label{deriv_func_identity}
Commençons très simplement en considérant $f(z)=z$, avec l'estimateur symétrique cela donne:

\begin{eqnarray*}
f'(z)&=&\lim_{\epsilon\to 0} f(z,\epsilon)\\
&=&\lim_{\epsilon\to 0}\frac{f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{z+\epsilon-z+\epsilon}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{2\epsilon}{2\epsilon}\\
&=&1\\
\end{eqnarray*}

Finalement:
\begin{equation}
\boxed{\applyop{D}{z}{(z)}=1}
\end{equation}

\subsubsection{Fonction inverse}
\label{deriv_func_inverse}
Considérons $f(z)=\frac{1}{z}$, avec l'estimateur symétrique cela donne:
\begin{eqnarray*}
f'(z)&=&\lim_{\epsilon\to 0} f(z,\epsilon)\\
&=&\lim_{\epsilon\to 0}\frac{f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{\frac{1}{z+\epsilon}-\frac{1}{z-\epsilon}}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{1}{2\epsilon}\left(\frac{z-\epsilon}{(z+\epsilon)(z-\epsilon)}-\frac{z+\epsilon}{(z+\epsilon)(z-\epsilon)}\right)\\
&=&\lim_{\epsilon\to 0}\frac{1}{2\epsilon}\left(\frac{-2\epsilon}{z^2-\epsilon^2}\right)\\
&=&\lim_{\epsilon\to 0}\frac{-1}{z^2-\epsilon^2}\\
&=&-\frac{1}{z^2}\\
\end{eqnarray*}

Finalement:
\begin{equation}
\boxed{\applyop{D}{\frac{1}{z}}{(z)}=-\frac{1}{z^2}}
\end{equation}

\subsubsection{Fonction puissance}
\label{deriv_func_power}
Plus généralement, considérons maintenant $f(z)=z^a$, d'après la relation obtenue sur les dérivés d'une puissance (voir \ref{derive_puissance}), on a :

\[
\applyop{D}{f(z)^a}{}=af(z)^{a-1}\applyop{D}{f(z)}{(z)}
\]
Soit dans notre cas simplement
\[
\applyop{D}{z^a}{}=a z^{a-1}\applyop{D}{z}{(z)}
\]
Et d'après le résultat précédent $\applyop{D}{z}{(z)}=1$ (voir \ref{deriv_identity}), on obtient finalement:

\begin{equation}
\boxed{\applyop{D}{z^a}{(z)}=a z^{a-1}}
\end{equation}



\subsubsection{Fonction géométrique}
\label{deriv_func_geom}
On part de la définition avec $f(z)=\frac{1}{1-z}$, avec l'estimateur symétrique cela donne:

\begin{eqnarray*}
f'(z)&=&\lim_{\epsilon\to 0} f(z,\epsilon)\\
&=&\lim_{\epsilon\to 0}\frac{f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{\frac{1}{1-(z+\epsilon)}-\frac{1}{1-(z-\epsilon)}}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{1}{2\epsilon}\left(\frac{1-(z-\epsilon)}{(1-(z+\epsilon))(1-(z-\epsilon))}-\frac{1-(z+\epsilon)}{(1-(z+\epsilon))(1-(z-\epsilon))}\right)\\
&=&\lim_{\epsilon\to 0}\frac{1}{2\epsilon}\left(\frac{1-z+\epsilon}{(1-z-\epsilon)(1-z+\epsilon)}-\frac{1-z-\epsilon}{(1-z-\epsilon)(1-z+\epsilon)}\right)\\
&=&\lim_{\epsilon\to 0}\frac{1}{(1-z)^2-\epsilon^2}\\
&=&\frac{1}{(1-z)^2}\\
\end{eqnarray*}

Finalement

\begin{equation}
\boxed{\applyop{D}{\frac{1}{1-z}}{(z)}=\frac{1}{(1-z)^2}}
\end{equation}

\subsubsection{Fonction polynôme}
\label{deriv_func_polynome}
Soit un polynôme $P(z)$ de degré $m$, notons:
\[
P(z)=\sum_{n=0}^m a_n z^n
\]

Pour commencer utilisons le résultat sur les puissances (voir \ref{deriv_func_power})

\[
	 \applyop{D}{a_n z^n}{(z)} = n a_n z^{n-1}
\]
Et en utilisant la linéarité (voir \ref{derive_linearite}), on obtient que:
\[
	\applyop{D}{\sum_{n=0}^m a_n z^n}{(z)}=\sum_{n=0}^m \applyop{D}{a_n z^n}{(z)}
\]
Soit 
\[
	\applyop{D}{\sum_{n=0}^m a_n z^n}{(z)}=\sum_{n=0}^m n a_n z^{n-1}
\]

Le premier terme de la somme est nul donc on peut le retirer:
\begin{equation}
	\boxed{\applyop{D}{\sum_{n=0}^m a_n z^n}{(z)}= \sum_{n=1}^m n a_n z^{n-1}}
\end{equation}

On peut aussi s'intéresser à la dérivée k-ième d'un polynôme:
\begin{eqnarray*}
	\applyop{D}{P(z)}{(z)}&=& \sum_{n=1}^m n a_n z^{n-1}\\
	\applyopp{D}{2}{P(z)}{(z)}&=& \sum_{n=2}^m n (n-1) a_n z^{n-2}\\
	\applyopp{D}{3}{P(z)}{(z)}&=& \sum_{n=3}^m n (n-1)(n-2) a_n z^{n-3}\\
	\applyopp{D}{k}{P(z)}{(z)}&=& \sum_{n=k}^m n(n-1)(n-2)...(n-(k-1)) a_n z^{n-k}\\
\end{eqnarray*}

Soit
\[
\applyopp{D}{k}{P(z)}{(z)}=\sum_{n=k}^m \left(\prod_{j=0}^{k-1}(n-j)\right) a_n z^{n-k}
\]
On peut aussi réécrire la factorielle descendante comme un rapport de factorielle:
\begin{equation}
	\boxed{\applyopp{D}{k}{\sum_{n=0}^m a_n z^n}{(z)}= \sum_{n=k}^m \frac{n!}{(n-k)!} a_n z^{n-k}}
\end{equation}

Vérification, soit:
\[
\begin{array}{l}
	P(z)=a_0+a_1 z+a_2 z^2+a_3 z^3+a_4 z^4\\
	\applyop{D}{P(z)}{(z)}=a_1 +2a_2 z+3a_3 z^2+4a_4 z^3\\
	\applyopp{D}{2}{P(z)}{(z)}=2a_2+ 6a_3 z+12a_4 z^2\\
\end{array}
\]

Or d'après la formule on obtient directement le résultat :
\[
	\begin{array}{ll}
	\applyopp{D}{2}{P(z)}{(z)}&= \sum_{n=2}^4 \frac{n!}{(n-2)!} a_n z^{n-2}\\
	&=\frac{2!}{(2-2)!} a_2 z^{2-2}+\frac{3!}{(3-2)!} a_3 z^{3-2}+\frac{4!}{(4-2)!} a_4 z^{4-2}\\
	&=2a_2+ 6a_3 z+12a_4 z^2\\
	\end{array}
\]

\subsubsection{Fonctions trigonométriques}

On part de la définition avec $f(z)=sin(z)$ ou $f(z)=cos(z)$, en utilisant les relations trigonométriques : 

\begin{eqnarray*}
sin(a+b)-sin(a-b)&=&2 cos(a)sin(b)\\
cos(a+b)-cos(a-b)&=&-2 sin(a)sin(b)\\
\lim_{\epsilon\to 0}\frac{sin(\epsilon)}{\epsilon}&=&1\\
\end{eqnarray*}

Nous ne démontrerons pas ses formules à ce stade, on obtient:

\begin{center}
\begin{tabular}{cc}
\begin{minipage}{8cm}
\begin{eqnarray*}
f'(z)&=&\lim_{\epsilon\to 0} f(z,\epsilon)\\
&=&\lim_{\epsilon\to 0}\frac{f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{sin(z+\epsilon)-sin(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{sin(\epsilon)cos(z)}
{\epsilon}\\
&=&cos(z)\\
\end{eqnarray*}
\end{minipage}
&
\begin{minipage}{8cm}
\begin{eqnarray*}
f'(z)&=&\lim_{\epsilon\to 0} f(z,\epsilon)\\
&=&\lim_{\epsilon\to 0}\frac{f(z+\epsilon)-f(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}\frac{cos(z+\epsilon)-cos(z-\epsilon)}{2\epsilon}\\
&=&\lim_{\epsilon\to 0}-\frac{sin(\epsilon)sin(z)}
{\epsilon}\\
&=&-sin(z)\\
\end{eqnarray*}
\end{minipage}\\
\end{tabular}
\end{center}

Finalement :

\begin{equation}
\boxed{ \applyop{D}{sin(z)}{(z)}=cos(z) \quad\quad et \quad\quad \applyop{D}{cos(z)}{(z)}=-sin(z)}
\end{equation}

\newpage
\section{Équations de Cauchy-Riemann}

Soit une fonction complexe $f(z)$ avec $z\in\mathbb{C}$ et $z=x+i y$, on peut alors écrire $f$ en fonction de deux fonctions réelles à deux variables $R(x,y)$ et $I(x,y)$:

\[
f(x,y)=R(x,y)+i I(x,y)
\]
Les équations de Cauchy-Riemann énoncent que $f$ est \textbf{holomorphe} (dérivable au sens complexe) si et seulement si :
\begin{equation}
	\boxed{\frac{\partial R(x,y)}{\partial x}=\frac{\partial I(x,y)}{\partial y} \quad et\quad \frac{\partial R(x,y)}{\partial y}=-\frac{\partial I(x,y)}{\partial x}}
\end{equation}
Si la fonction $f(z)$ vérifie les équations de Cauchy-Riemann quelque soit $z\in \mathbb{C}$, alors on dira que \textbf{$f$ est entière}. Et si f ne vérifie ces équations que sur un certain domaine $D$ alors on dira que $f$ est \textbf{holomorphe sur $D$}. Autrement dit une fonction entière est une fonction holomorphe sur $\mathbb{C}$.\\

\subsection{Démonstration}

\noindent
Si on part de la définition de la dérivée :
\[
f'(z)=\lim_{\epsilon\to 0}\frac{f(z+\epsilon)-f(z)}{\epsilon} \quad \quad \epsilon\in\mathbb{C}
\]
On peut approcher la limite avec n'importe quel angle $\theta$ :
\[
f'(z)=\lim_{\rho\to 0}\frac{f(z+\rho e^{i\theta})-f(z)}{\rho e^{i\theta}} \quad \quad \rho\in\mathbb{R}\quad et\quad \forall \theta\in\mathbb{R}
\]
En particulier pour $\theta=0$ et $\theta=\frac{\pi}{2}$, on obtient:
\[
\lim_{\rho\to 0}\frac{f(z+\rho)-f(z)}{\rho }=\lim_{\rho\to 0}\frac{f(z+i\rho)-f(z)}{i\rho } \quad \quad \rho\in\mathbb{R}
\]
Ici si on voit $f$ comme une fonction de deux variables réelles on peut reconnaitre les deux dérivées respectives par rapport à la partie réelle $x$ et par rapport à la partie imaginaire $y$:

\[
\frac{\partial f(x,y)}{\partial x}=\frac{1}{i}\frac{\partial f(x,y)}{\partial y}
\]
Soit
\[
\frac{\partial R(x,y)+i I(x,y)}{\partial x}=\frac{1}{i}\frac{\partial R(x,y)+i I(x,y)}{\partial y}
\]
Soit
\[
\frac{i\partial R(x,y)-I(x,y)}{\partial x}=\frac{\partial R(x,y)+i I(x,y)}{\partial y}
\]
Et si on égalise partie réelle et partie imaginaire, on obtient :
\[
\boxed{
\left\{
\begin{array}{c}
\frac{\partial R(x,y)}{\partial y}=-\frac{\partial I(x,y)}{\partial x}\\
\frac{\partial R(x,y)}{\partial x}=\frac{\partial I(x,y)}{\partial y}\\
\end{array}
\right.
}
\]

\subsection{Exemples}
\subsubsection{Fonction : $f(z)=z^2$}

Posons $z=x+iy$
\[
f(x,y)= (x+iy)^2= x^2-y^2 + i 2xy
\]
Si on pose $R(x,y)=x^2-y^2$ et $I(x,y)=2xy$, on obtient :
\[
f(x,y)= R(x,y) + i I(x,y)
\]
La fonction vérifie elle les équations de Cauchy-Riemann, pour le savoir, on calcul les dérivées partielles de $R(x,y)$ et $I(x,y)$, on obtient: 
\[
\boxed{
\left\{
\begin{array}{l}
\frac{\partial R(x,y)}{\partial y}=-2y\\
\frac{\partial I(x,y)}{\partial x}=2y\\
\frac{\partial R(x,y)}{\partial x}=2x\\
\frac{\partial I(x,y)}{\partial y}=2x\\
\end{array}
\right.
}
\]
Ce qui vérifie bien les équations de Cauchy-Riemann. De plus les dérivées sont calculables quelque soit $x$ et $y$. Donc la \textbf{fonction est entière}. 

\subsubsection{Fonction : $f(z)=\bar{z}$}

Posons $z=x+iy$
\[
f(x,y)= x-iy
\]
Si on pose $R(x,y)=x$ et $I(x,y)=-y$, on obtient :
\[
f(x,y)= R(x,y) + i I(x,y)
\]
La fonction vérifie elle les équations de Cauchy-Riemann, pour le savoir, on calcul les dérivées partielles de $R(x,y)$ et $I(x,y)$, on obtient: 
\[
\boxed{
\left\{
\begin{array}{l}
\frac{\partial R(x,y)}{\partial y}=0\\
\frac{\partial I(x,y)}{\partial x}=0\\
\frac{\partial R(x,y)}{\partial x}=1\\
\frac{\partial I(x,y)}{\partial y}=-1\\
\end{array}
\right.
}
\]
Les ECR ne sont pas respectées donc la \textbf{fonction n'est pas holomorphe}.

\subsubsection{Fonction : $f(z)=\frac{1}{1-z}$}

Posons $z=x+iy$
\[
f(x,y)= \frac{1}{(1-x)-iy}=\frac{(1-x)+iy}{(1-x)^2+y^2}=\frac{1-x}{(1-x)^2+y^2}+i\frac{y}{(1-x)^2+y^2}
\]
Si on pose $R(x,y)=\frac{1-x}{(1-x)^2+y^2}$ et $I(x,y)=\frac{y}{(1-x)^2+y^2}$, on obtient :
\[
f(x,y)= R(x,y) + i I(x,y)
\]
La fonction vérifie elle les équations de Cauchy-Riemann, pour le savoir, on calcul les dérivées partielles de $R(x,y)$ et $I(x,y)$, on obtient: 
\[
\boxed{
\left\{
\begin{array}{l}
\frac{\partial R(x,y)}{\partial y}=\frac{2 (x-1) y}{(1-2 x+x^2+y^2)^2}\\
\frac{\partial I(x,y)}{\partial x}=-\frac{2 (x-1) y}{(1-2 x+x^2+y^2)^2}\\
\frac{\partial R(x,y)}{\partial x}=-\frac{(1-x+y) (-1+x+y)}{(1-2 x+x^2+y^2)^2}\\
\frac{\partial I(x,y)}{\partial y}=-\frac{(1-x+y) (-1+x+y)}{(1-2 x+x^2+y^2)^2}\\
\end{array}
\right.
}
\]
Les ECR sont respectées. Mais on ne peut calculer les dérivées que si :
\[
(1-2 x+x^2+y^2)^2 \ne 0
\]
\[
1-2 x+x^2+y^2 \ne 0
\]
\[
(1-x)^2+y^2 \ne 0
\]
Si on défini le domaine D comme cercle centré en 1 et de rayon 1. alors f \textbf{est holomorphe sur D}.

\subsubsection{Fonction : $sin(z)$}
\label{entire-sinus}

On peut réécrire la fonction sinus avec des exponentielles complexes:
\[
sin(z)=\frac{e^{iz}-e^{-iz}}{2i}
\]
Posons $z=x+iy$
\[
sin(x,y)=\frac{e^{i(x+iy)}-e^{-i(x+iy)}}{2i}=\frac{e^{ix-y}-e^{-ix+y}}{2i}=\frac{e^{-y}e^{ix}-e^{y}e^{-ix}}{2i}
\]
Et $e^{ix}=cos(x)+isin(x)$ et $e^{-ix}=cos(x)-isin(x)$, ce qui donne :
\[
sin(x,y)=\frac{e^{-y}(cos(x)+isin(x))-e^{y}(cos(x)-isin(x))}{2i}
\]
\[
sin(x,y)=\frac{e^{-y}cos(x)+ie^{-y}sin(x)-e^{y}cos(x)+ie^{y}sin(x)}{2i}
\]
\[
sin(x,y)=\frac{-sin(x)(e^{y}+e^{-y})}{2}+i\frac{cos(x)(e^{-y}-e^{y}))}{2}
\]
Si on pose $R(x,y)=\frac{-sin(x)(e^{y}+e^{-y})}{2}$ et $I(x,y)=\frac{cos(x)(e^{-y}-e^{y}))}{2}$, on obtient :
\[
f(x,y)= R(x,y) + i I(x,y)
\]
La fonction vérifie elle les équations de Cauchy-Riemann, pour le savoir, on calcul les dérivées partielles de $R(x,y)$ et $I(x,y)$, on obtient: 
\[
\boxed{
\left\{
\begin{array}{l}
\frac{\partial R(x,y)}{\partial y}=\frac{-sin(x)(e^{y}-e^{-y})}{2}\\
\frac{\partial I(x,y)}{\partial x}=\frac{sin(x)(e^{y}-e^{-y}))}{2}\\
\frac{\partial R(x,y)}{\partial x}=\frac{-cos(x)(e^{y}+e^{-y})}{2}\\
\frac{\partial I(x,y)}{\partial y}=\frac{-cos(x)(e^{-y}+e^{y}))}{2}\\
\end{array}
\right.
}
\]
\noindent
La fonction $sin(z)$ obéit donc aux ECR. De plus les dérivées sont continues. donc $sin(z)$ est une \textbf{fonction entière}.

\subsection{Lien avec les fonctions harmoniques}
\label{eq_harmoniques_functions}
Montrons maintenant que toute \textbf{fonction holomorphe} $f$ (c'est à dire vérifiant les équations de Cauchy-Riemann), est également une \textbf{fonction harmonique}, c'est à dire que :

\begin{equation}
\boxed{\frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}=0}
\end{equation}

C'est à dire que les parties réelles $R(x,y)$ et imaginaires $I(x,y)$ sont harmoniques, si on part des équations de Cauchy-Riemann:
\[
\frac{\partial R(x,y)}{\partial x}=\frac{\partial I(x,y)}{\partial y} \quad et\quad \frac{\partial R(x,y)}{\partial y}=-\frac{\partial I(x,y)}{\partial x}
\]
Et que on dérive par rapport $x$ sur la première et par rapport à $y$ dans la seconde :
\[
\frac{\partial^2 R(x,y)}{\partial x^2}=\frac{\partial^2 I(x,y)}{\partial y\partial x} \quad et\quad \frac{\partial^2 R(x,y)}{\partial y^2}=-\frac{\partial^2 I(x,y)}{\partial x\partial y}
\]
Par conséquent :
\[
\boxed{\frac{\partial^2 R(x,y)}{\partial x^2}+\frac{\partial^2 R(x,y)}{\partial y^2}=0}
\]
Maintenant si on dérive par rapport $y$ sur la première et par rapport à $x$ dans la seconde :
\[
\frac{\partial^2 R(x,y)}{\partial x\partial y}=\frac{\partial^2 I(x,y)}{\partial y^2} \quad et\quad \frac{\partial^2 R(x,y)}{\partial y\partial x}=-\frac{\partial^2 I(x,y)}{\partial x^2}
\]
Par conséquent :
\[
\boxed{\frac{\partial^2 I(x,y)}{\partial y^2}+\frac{\partial^2 I(x,y)}{\partial x^2}=0}
\]
Les parties réelles et imaginaires sont donc harmoniques ce qui démontre que la fonction $f(x,y)=R(x,y)+iI(x,y)$, l'est également par linéarité.

\newpage
\section{Théorie de Cauchy}

\subsection{Théorème d'intégration de Cauchy}
\label{eq_cauchy_contour}

Si $f$ est une fonction holomorphe sur un domaine $D$. C'est à dire que $f$ satisfait les équations de Cauchy-Riemann sur $D$. Et $\gamma$ un lacet inclus dans ce domaine, calculons:
\[
I=\oint_\gamma f(z) dz
\]
En effet prenons la notation :
\[
f(x,y)=R(x,y)+i I(x,y)
\]
Et avec $dz=dx+idy$, l'intégrale devient:
\[
I=\oint_\gamma \left(R(x,y)+i I(x,y)\right) (dx +i dy)
\]
Si on développe et que on sépare les parties réelles et imaginaire :
\[
I=\oint_\gamma R(x,y)dx-I(x,y)dy + i \oint_\gamma I(x,y)dx+R(x,y)dy
\]
D'après le théorème de Green $\oint_C Udx +Vdy=\int\int_S \left( \frac{V}{\partial x}-\frac{U}{\partial y}\right)dx dy$ (voir \ref{misc_green}), l'intégrale devient:

\[
I=\int\int_S \left(-\frac{\partial I(x,y)}{\partial x}-\frac{\partial R(x,y)}{\partial y}\right)dxdy + i \int\int_S \left(\frac{\partial R(x,y)}{\partial x} - \frac{\partial I(x,y)}{\partial y}\right) dxdy
\]
 
D'après les équations de Cauchy-Riemann si $f$ est holomorphe alors $\frac{\partial R(x,y)}{\partial x}=\frac{\partial I(x,y)}{\partial y} \quad et\quad \frac{\partial R(x,y)}{\partial y}=-\frac{\partial I(x,y)}{\partial x}$, finalement l'intégrale est nulle:


\begin{equation}
\boxed{\oint_\gamma f(z) dz=0}
\end{equation}

\subsection{Formule intégrale de Cauchy}
\label{eq_cauchy_int}

Prenons de nouveau $f$, une fonction holomorphe sur un domaine $D$.Ainsi que $a\in D$ .Et $\gamma$ un lacet inclus dans ce domaine et contenant $a$ et pris dans le sans trigonométrique, calculons maintenant :

\[
I=\oint_\gamma \frac{f(z)}{z-a} dz
\]

L'intégrante $\frac{f(z)}{z-a}$ n'est pas holomorphe en $a$ donc la conclusion précédente ne s'applique pas. L'astuce consiste ici à décomposer l'intégrale en deux parties une qui contourne la singularité $S$ et une qui englobe cette singularité $C$, schématiquement:

\begin{center}
\begin{tabular}{ccccc}
\TikzPathBn{0.03}{1}{$a$}{$\gamma$}&
$=$&
\TikzPathAn{0.03}{1}{0.3}{$a$}{$S$}&
$+$&
\TikzPathBn{0.03}{0.3}{$a$}{$C$}\\
\end{tabular}
\end{center}

Pour C on prendra un cercle de rayon $r$ centré en $a$, avec $r$ aussi petit que l'on veut. On peut donc écrire
\[
I=\oint_\gamma \frac{f(z)}{z-a} dz=\oint_S \frac{f(z)}{z-a} dz+\oint_C \frac{f(z)}{z-a} dz
\]
D'après le théorème d'intégration de Cauchy démontré précédemment, l'intégrale sur le lacet $S$ est nulle, donc il ne reste plus qu'a integrer sur le cercle:

\[
I=\oint_C \frac{f(z)}{z-a} dz
\]

On peut paramétrer en posant $z=a+re^{i\theta}$ et $dz=r i e^{i\theta}d\theta$, on obtient:

\[
I=\int_0^{2\pi} \frac{f(a+re^{i\theta})}{re^{i\theta}} r i e^{i\theta}=\int_0^{2\pi} f(a+re^{i\theta}) d\theta
\]

Et si on prend la limite lorsque $r\to 0$, on obtient :

\[
I=\lim_{r\to 0} \int_0^{2\pi} f(a+re^{i\theta}) d\theta=f(a)\int_0^{2\pi}d\theta=2i\pi f(a)
\]

Finalement 

\begin{equation}
\boxed{f(a)= \frac{1}{2i\pi}\oint_\gamma \frac{f(z)}{z-a} dz}
\end{equation}

\subsection{Dérivée n-ième : Toute fonction holomorphe est analytique}
\label{eq_cauchy_deriv}

Nous allons maintenant démontrer une forme plus générale de l'intégrale de Cauchy. Partons des mêmes considérations si on essaye de calculer la dérivée première:

\[
	f^{1}(a)=\lim_{\epsilon\to 0} \frac{f(a+\epsilon)-f(a)}{\epsilon}
\]
\[
	f^{1}(a)=\lim_{\epsilon\to 0} \frac{1}{2i\pi}\frac{\oint_\gamma \frac{f(z)}{z-(a+\epsilon)} dz-\oint_\gamma \frac{f(z)}{z-a} dz}{\epsilon}
\]
\[
	f^{1}(z)=\lim_{\epsilon\to 0} \frac{1}{2i\pi\epsilon}\oint_\gamma \left(\frac{1}{z-a-\epsilon}- \frac{1}{z-a}\right) f(z) dz
\]
\[
	f^{1}(z)=\lim_{\epsilon\to 0} \frac{1}{2i\pi\epsilon}\oint_\gamma \left(\frac{\epsilon}{(z-a-\epsilon)(z-a)}\right) f(z) dz
\]
On peut simplifier par $\epsilon$
\[
	f^{1}(z)=\lim_{\epsilon\to 0} \frac{1}{2i\pi}\oint_\gamma \left(\frac{1}{(z-a-\epsilon)(z-a)}\right) f(z) dz
\]
Donc si on prend la limite:

\[
	f^{1}(a)=\frac{1}{2i\pi}\oint_\gamma \frac{f(z)}{(z-a)^2} dz
\]

Maintenant on peut réitérer en prenant la dérivée seconde:

\[
	f^{2}(a)=\lim_{\epsilon\to 0} \frac{f^1(a+\epsilon)-f^1(a)}{\epsilon}
\]
\[
	f^{2}(a)=\frac{1}{2i\pi} \lim_{\epsilon\to 0} \frac{\oint_\gamma \frac{f(z)}{(z-(a+\epsilon))^2} dz-\oint_\gamma \frac{f(z)}{(z-a)^2} dz}{\epsilon}
\]
\[
	f^{2}(a)=\frac{1}{2i\pi\epsilon} \lim_{\epsilon\to 0} \oint_\gamma \left(\frac{1}{(z-a-\epsilon)^2}-\frac{1}{(z-a)^2}\right)f(z) dz
\]
\[
	f^{2}(a)=\frac{1}{2i\pi\epsilon} \lim_{\epsilon\to 0} \oint_\gamma \left(\frac{(z-a)^2-(z-a-\epsilon)^2}{(z-a-\epsilon)^2(z-a)^2}\right)f(z) dz
\]
\[
	f^{2}(a)=\frac{1}{2i\pi\epsilon} \lim_{\epsilon\to 0} \oint_\gamma \left(\frac{\epsilon(2(z-a)-\epsilon)}{(z-a+\epsilon)^2(z-a)^2}\right)f(z) dz
\]
La on peut simplifier par $\epsilon$:
\[
	f^{2}(a)=\frac{1}{2i\pi} \lim_{\epsilon\to 0} \oint_\gamma \left(\frac{2(z-a)-\epsilon}{(z-a+\epsilon)^2(z-a)^2}\right)f(z) dz
\]
Si on prend la limite :
\[
	f^{2}(a)=\frac{1}{2i\pi} \oint_\gamma \frac{2(z-a)}{(z-a)^4} dz
\]
\[
	f^{2}(a)=\frac{2}{2i\pi} \oint_\gamma \frac{1}{(z-a)^3} dz
\]

Finalement, les dérivées n-ième d'une fonction en $a$ peuvent s'obtenir par l'intégrale: 


\begin{equation}
\boxed{f^{(n)}(a)= \frac{n!}{2i\pi}\oint_\gamma \frac{f(z)}{(z-a)^{n+1}} dz}
\end{equation}

Cette équation nous enseigne qu'une fonction holomorphe est \textbf{infiniment dérivable} elle est de classe $C^\infty$. Elle possède donc un développement en \textbf{série de Taylor-Laurent} (voir \ref{eq_taylor_laurent}) , ce qui signifie quelle est également \textbf{analytique}.

\subsection{Propriété de la moyenne}
\label{eq_cauchy_moy}
Si on part de la formule intégrale de Cauchy \ref{eq_cauchy_int}:

\[
f(a)= \frac{1}{2i\pi}\oint_\gamma \frac{f(z)}{z-a} dz
\]
Si on pose $z=a+r e^{i\theta}$ alors $dz=ri e^{i\theta} d\theta$, ce qui donne :
\[
f(a)= \frac{1}{2i\pi}\oint_0^{2\pi} \frac{f(a+r e^{i\theta})}{r e^{i\theta}} ri e^{i\theta} d\theta
\]
Après simplification on déduit finalement que :

\begin{equation}
\boxed{f(a)= \frac{1}{2i\pi}\oint_0^{2\pi} f(a+r e^{i\theta}) d\theta}
\end{equation}

Cette formule nous enseigne que la valeur en un point $a$ d'une fonction holomorphe est entièrement déterminée par la valeur moyenne des valeurs prises sur un cercle de rayon $r$ centré en $a$.\\

En fait cette équation est plus généralement vraie pour toute fonction harmonique. C'est à dire particulièrement pour les valeurs réelles et imaginaires des fonctions holomorphes qui sont des fonctions harmoniques (voir \ref{eq_harmoniques_functions}).

\subsection{Principe du maximum\todo}

\subsection{Intégrale de contour}
\label{eq_contour_int1}
Calculons l'intégrale remarquable :
\[
I=\oint_C (z-a)^n dz
\]
Ou C est le cercle de rayon $r$ centré en a. Posons $z=a+r e^{it}$ et $dz=ri e^{it}dt$, l'intégrale devient:
\[
I=\int_0^{2\pi} (r e^{it})^n ri e^{it}dt
\]
Simplifions:
\[
I=\int_0^{2\pi} i r^{n+1} e^{it(n+1)}dt
\]
Lorsque $n=-1$ on obtient:
\[
I=\int_0^{2\pi} i dt = 2i\pi
\]
Et si $n\ne-1$ la fonction exponentielle étant $2\pi$ périodique:
\[
I=\left[ \frac{i r^{n+1} e^{it(n+1)}}{i(n+1)} \right]_0^{2\pi}=0
\]

Pour résumer:
\begin{equation}
\boxed{
\oint_C (z-a)^n dz=\left\{\begin{array}{l}
2i\pi\ si\ n=-1\\
0\ sinon\\
\end{array}\right.}
\end{equation}



\newpage
\section{La sphère de Riemann : $\mathbb{S}$ \workon}

\subsection{Construction de la sphère de Riemann \fig \workon}
\label{eq_sphere_riemann}
L'intégrale de Cauchy (voir \ref{eq_cauchy_contour}), a pour conséquence fondamentale que \textbf{pour une fonction holomorphe sur un domaine D} quelque soit les points $A$ et $B$ du plan complexe (appartenant à $D$), le résultat est indépendant du chemin d'intégration $\gamma$ choisit pour aller de $A$ à $B$ inclus dans $D$. {\fig} \\

\begin{figure}[H]
\centering
\caption{Équivalence des chemins d'intégrations sur le plan complexe $\mathbb{C}$}
\end{figure}

En particulier on pourra considérer \textbf{l'infini comme un point particulier}. Cette extension du plan complexe par l'ajout d'un point à l'infini, porte le nom de \textbf{sphère de Riemann}, nous la noterons $\mathbb{S}$. On peut visualiser la projection du plan complexe $\mathbb{C}$, sur la sphère de Riemann $\mathbb{S}$:

\begin{figure}[H]
\centering
\caption{Projection du plan complexe $\mathbb{C}$, sur la sphère de Riemann $\mathbb{S}$}
\end{figure}

La discussion précédente affirme ainsi que si la fonction considérée est holomorphe sur D alors tous les chemins d'intégrations pour aller de $A$ à $\infty$ sont équivalents. {\fig} \\ 

\begin{figure}[H]
\centering
\caption{Équivalence des chemins d'intégrations sur la sphère de Riemann $\mathbb{S}$}
\end{figure}

Résonner sur $\mathbb{S}$ permet de résoudre certaines d'indétermination, comme:

\[
\left\{
\begin{array}{ll}
z+\infty=\infty&\forall z\\
z\infty=\infty&\forall z\ne 0\\
z/0=\infty& \forall z\ne 0\\
z/\infty=0& \forall z\ne \infty\\
\end{array}
\right.
\]
Les autres formes comme $\frac{\infty}{\infty}$, $\infty -\infty , 0\infty$ et $\frac{0}{0}$, restent indéfinies.

\subsection{Transformations de Möbius : Automorphismes de $\mathbb{S}$ }
La manière la plus générale possible d'établir une bijection de $z\in\mathbb{S}$ vers $w\in\mathbb{S}$, c'est à dire de la sphère de Riemann vers elle même, est donnée par la transformation de Möbius:

\begin{equation}
\boxed{w=\frac{a\cdot z+b}{c\cdot z+d}\quad\quad avec \quad\quad a,b,c,d \in\mathbb{S}\quad\quad et\quad\quad ad-bc=1}
\end{equation}

On dira que \textbf{les transformées de Möbius sont les automorphismes de $\mathbb{S}$}. La transformation inverse s'obtient alors trivialement :

\begin{equation}
\boxed{z=\frac{d\cdot w-b}{-c\cdot w+a} }
\end{equation}

Si les coefficients $a,b,c,d \ne0$ et $a,b,c,d\ne \infty$, alors la transformation de Möbius déplace le point origine et le point à l'infini selon: 

\[
\begin{array}{|c|c|}
\hline
z &w\\
\hline
0 & \frac{b}{d}\\
\infty & \frac{a}{c}\\
\frac{-d}{c}&\infty\\
\frac{-b}{a}&0\\
\hline
\end{array}
\]

\subsection{Points fixes}
\label{eq_mobius_pts_fixes}
Si il existe des points fixes alors ils sont les solutions dans $\mathbb{S}$ de l'équation du second degré:

\[
z=\frac{a z+b}{c z+d}\quad \quad \Rightarrow \quad\quad c z^2+(d-a) z-b=0
\]

\begin{align*}
\Delta&=(d-a)^2+4cb\\
&=(d+a)^2-4ad+4cb\\
&=(d+a)^2-4(ad-cb)\quad\quad Mais \quad \quad ad-bc=1\\
&=(d+a)^2-4\\
\end{align*}

Maintenant résumons les cas possibles:

\begin{enumerate}
\item Si $(d+a)\ne \pm 2\ et\ c\ne0$ alors il existe deux points fixes \textbf{fini} et \textbf{distincts}: 
\begin{equation}
\boxed{\xi_1=\frac{(a-d)+\sqrt{\Delta}}{2c} \quad \quad \xi_2=\frac{(a-d)-\sqrt{\Delta}}{2c}}
\end{equation}
\item Si $(d+a)=\pm 2\ et\ c\ne0$ alors il existe \textbf{un seul} points fixe:
\begin{equation}
\boxed{\xi=\frac{(a-d)}{2c}}
\end{equation}
\item Si $c=0$ alors les points fixes sont les solutions dans $\mathbb{S}$ de l'équation:
\[
dz=az+b\quad \quad \Rightarrow\quad\quad z=\frac{b}{d-a}\quad\quad car\quad\quad d=\frac{1}{a}
\] 
Les \textbf{deux points fixes} sont donc :
\begin{equation}
\boxed{\xi_1=\frac{ab}{1-a^2} \quad \quad \xi_2=\infty}
\end{equation}
\item Si $c=0\ et\ a=d$ alors il existe un seul point fixe $\xi=\infty$
\end{enumerate}

\subsection{Cross-ratios}

\subsubsection{Définition}
Étant donné quatre points $z_1,z_2,z_3,z_4 \in \mathbb{S}$ on définit le cross-ratio, par:

\begin{equation}
\boxed{[z_1,z_2;z_3,z_4]=\frac{(z_1-z_3)(z_2-z_4)}{(z_1-z_4)(z_2-z_3)}}
\end{equation}

\subsubsection{Invariance par transformée de Möbius}
\label{eq_cross_ratio_invariance}
Un cross-ratio est invariant par transformée de Möbius, en effet si on pose $w_i$ l'image par transformée de Möbius de $z_i$, alors:
\begin{align*}
(w_i-w_j)&=\frac{a z_i+b}{c z_i+d}-\frac{a z_j+b}{c z_j+d}\\
&=\frac{(a z_i+b)(c z_j+d)-(a z_j+b)(c z_i+d)}{(c z_i+d)(c z_j+d)}\\
&=\frac{(a z_i+b)c z_j+(a z_i+b)d-(a z_j+b)c z_i-(a z_j+b)d}{(c z_i+d)(c z_j+d)}\\
&=\frac{ac z_i z_j+bc z_j+ad z_i+bd-ac z_j z_i-bc z_i-ad z_j-bd}{(c z_i+d)(c z_j+d)}\\
&=\frac{bc z_j+ad z_i-bc z_i-ad z_j}{(c z_i+d)(c z_j+d)}\\
&=\frac{(ad-bc) (z_i-z_j)}{(c z_i+d)(c z_j+d)}\quad\quad Mais\quad \quad ad-bc=1\\
&=\frac{z_i-z_j}{(c z_i+d)(c z_j+d)}\\
\end{align*}
Et si maintenant on calcul le cross ratio on obtient :
\begin{align*}
[w_1,w_2;w_3,w_4]&=\frac{(w_1-w_3)(w_2-w_4)}{(w_1-w_4)(w_2-w_3)}\\
&=\frac{\frac{z_1-z_3}{(c z_1+d)(c z_3+d)}\frac{z_2-z_4}{(c z_2+d)(c z_4+d)}}{\frac{z_1-z_4}{(c z_1+d)(c z_4+d)}\frac{z_2-z_3}{(c z_2+d)(c z_3+d)}}\\
&=\frac{(z_1-z_3)(z_2-z_4)}{(z_1-z_4)(z_2-z_3)}
\end{align*}

C'est à dire que finalement :
\begin{equation}
\boxed{[w_1,w_2;w_3,w_4]=[z_1,z_2;z_3,z_4]} \quad \quad avec\quad\quad w_i=\frac{a z_i+b}{c z_i+d}
\end{equation}
\subsubsection{Symétries}

Il existe 4!=24 cross-ratios associés à 4 nombres complexes mais ils sont tous liés par symétries:

\begin{align*}
[z_1,z_2;z_3,z_4] = [z_2,z_1;z_4,z_3] = [z_3,z_4;z_1,z_2] = [z_4,z_3;z_2,z_1] &= \lambda\\
[z_1,z_2;z_4,z_3] = [z_2,z_1;z_3,z_4] = [z_3,z_4;z_2,z_1] = [z_4,z_3;z_1,z_2] &=\frac{1}{\lambda}\\
[z_1,z_3;z_2,z_4] = [z_2,z_4;z_1,z_3] = [z_3,z_1;z_4,z_2] = [z_4,z_2;z_3,z_1] &=1-\lambda\\
[z_1,z_3;z_4,z_2] = [z_2,z_4;z_3,z_1] = [z_3,z_1;z_2,z_4] = [z_4,z_2;z_1,z_3] &=\frac{1}{1-\lambda}\\
[z_1,z_4;z_2,z_3] = [z_2,z_3;z_1,z_4] = [z_3,z_2;z_4,z_1] = [z_4,z_1;z_3,z_2] &=\frac{\lambda-1}{\lambda}\\
[z_1,z_4;z_3,z_2] = [z_2,z_3;z_4,z_1] =  [z_3,z_2;z_1,z_4] = [z_4,z_1;z_2,z_3] &=\frac {\lambda}{\lambda-1}\\
\end{align*}

\subsubsection{Point à l'infini}

Si un des points est le point à l'infini, alors il suffit de prendre la limite, on obtient ainsi:

\begin{align*}
[\infty,z_2;z_3,z_4]&=\frac{(z_2-z_4)}{(z_2-z_3)}\\
[z_1,\infty;z_3,z_4]&=\frac{(z_1-z_3)}{(z_1-z_4)}\\
[z_1,z_2;\infty,z_4]&=\frac{(z_2-z_4)}{(z_1-z_4)}\\
[z_1,z_2;z_3,\infty]&=\frac{(z_1-z_3)}{(z_2-z_3)}\\
\end{align*}

\subsection{Forme normale \fig\proof}

Etant donnée le triplet $z_1,z_2,z_3\in\mathbb{S}$ et le triplet $w_1,w_2,w_3\in\mathbb{S}$, il existe une unique transformation de Möbius qui envoie le triplet $\{z_1,z_2,z_3\}$ sur $\{w_1,w_2,w_3\}$. (Cela démontre également que la transformation de Möbius transforme les cercles en cercles).


Pour construire cette transformation de Möbius, considérons l'égalité suivante:

\[
[z,z_1;z_2,z_3]=[w,w_1;w_2,w_3]
\]

Cette égalité doit en effet être vérifiée par la transformation recherchée. Car comme nous l'avons démontré (voir \ref{eq_cross_ratio_invariance}) les cross-ratios sont invariant par transformation de Möbius.\\

Si on suppose qu'il existe deux points fixes, on considère le cross-ratio construit sur ces points fixes (voir \ref{eq_mobius_pts_fixes}):
\[
[z,\infty;\xi_1,\xi_2]=[w,\frac{a}{c};\xi_1,\xi_2]
\]
Les points fixes ne bougent pas et le point $\infty$ et projeté en $\frac{a}{c}$, si on développe:

\[
\frac{z-\xi_1}{z-\xi_2}=\frac{(w-\xi_1)(\frac{a}{c}-\xi_2)}{(w-\xi_2)(\frac{a}{c}-\xi_1)}
\]

Si on pose $\boxed{K=\frac{a-c\xi_1}{a-c\xi_2}}$ alors on obtient:

\begin{equation}
\boxed{\frac{w-\xi_1}{w-\xi_2}=K\frac{z-\xi_1}{z-\xi_2}}
\end{equation}

On obtient ici la forme normale d'une transformation de Möbius. $K$ est appelé le "multiplicateur". Développons son expression:

\[
K=\frac{a-c\xi_1}{a-c\xi_2}
\]

Si on se rappelle de l'expression des points fixes $\xi_{1,2}=\frac{(a-d)\pm\sqrt{(a+d)^2-4}}{2c}$ obtenue (voir \ref{eq_mobius_pts_fixes}), cela donne :

\[
K=\frac{2a-(a-d)-\sqrt{(a+d)^2-4}}{2a-(a-d)+\sqrt{(a+d)^2-4}}
\]
Soit
\[
K=\frac{a+d-\sqrt{(a+d)^2-4}}{a+d+\sqrt{(a+d)^2-4}}
\]
Et si on pose $T=a+d$ qui est la trace de la matrice associée à la transformation de Möbius, on obtient l'expression compacte suivante qui ne dépend que de cette trace:

\begin{equation}
\boxed{ K=\frac{T-\sqrt{T^2-4}}{T+\sqrt{T^2-4}} }
\end{equation}

Maintenant s'il n'y a qu'un seul point fixe, c'est à dire si $T=\pm2$ alors $K=1$ et $\xi_1=\xi_2=\xi=\frac{(a-d)}{2c}$ (\ref{eq_mobius_pts_fixes}), par conséquent la forme normale s'écrit{\proof}:

\[
\boxed{\frac{1}{w-\xi}=\frac{1}{z-\xi}\pm c}
\]

\subsection{Itération de la transformation}
Si on itère $n$ fois une telle transformation, on obtient que:

\begin{equation}
\boxed{\frac{w-\xi_1}{w-\xi_2}=K^n\frac{z-\xi_1}{z-\xi_2}}
\end{equation}

Et si on résout pour $w$, on obtient:

\begin{align*}
w-\xi_1 &= (w-\xi_2)K^n\frac{z-\xi_1}{z-\xi_2}\\
\xi_2 K^n\frac{z-\xi_1}{z-\xi_2}-\xi_1 &= w \left(K^n\frac{z-\xi_1}{z-\xi_2}-1\right)\\
\end{align*}

Finalement on obtient:
\begin{equation}
\boxed{w= \frac{\xi_2 K^n\frac{z-\xi_1}{z-\xi_2}-\xi_1}{K^n\frac{z-\xi_1}{z-\xi_2}-1}}
\end{equation}

Lorsque $n\to\infty$:
\begin{enumerate}
\item Si $|K|<1$ , alors $w\to\xi_1$ , Le point $\xi_1$ est alors attracteur
\item Si $|K|>1$ , alors $w\to\xi_2$ , Le point $\xi_2$ est alors attracteur
\item Si $|K|=1$ , alors il n'y a pas de point d'attraction
\end{enumerate}


\subsection{Résumé et classification \workon}
\label{eq_mobius_classification}
Soit la transformation de Möbius :

\[
w=\frac{a\cdot z+b}{c\cdot z+d}\quad\quad avec \quad\quad a,b,c,d \in\mathbb{S}\quad\quad et\quad\quad ad-bc=1
\]

On peut établir le tableau suivant pour résumer les différents cas:

\[
\renewcommand{\arraystretch}{1.8}
\displaystyle{
\begin{array}{|c|c|c|c|c|c|c|}
\hline
Cas&Points\ fixes & \xi_1 & \xi_2 & Forme\ normale &K \\
\hline
c\ne0,\ T\ne\pm2&\xi_1\ne\xi_2=(fini) & \xi_1=\frac{(a-d)+\sqrt{T^2-4}}{2c} & \xi_2=\frac{(a-d)-\sqrt{T^2-4}}{2c} & \frac{w-\xi_1}{w-\xi_2}=K\frac{z-\xi_1}{z-\xi_2}&\frac{T-\sqrt{T^2-4}}{T+\sqrt{T^2-4}}\\
\hline
c\ne0,\ T=\pm2&\xi_1=\xi_2=\xi=(fini)& \multicolumn{2}{c|}{\xi=\frac{(a-d)}{2c}} & \frac{1}{w-\xi}=\frac{1}{z-\xi}\pm c &1\\
\hline
c=0&\xi_1=(fini),\ a\ne d,\ \xi_2=\infty& \multicolumn{2}{c|}{\xi_1=\frac{ab}{1-a^2}} &(w-\xi_1)=K(z-\xi_1) &\frac{a}{d}\\
\hline
c=0,\ a=d=\pm 1&\xi_1=\xi_2=\infty&& & w=z\pm b &  \\
\hline
\end{array}}
\]

\[
\renewcommand{\arraystretch}{1.8}
\begin{array}{|l|l|l|}
\hline
Condition& Cons\acute{e}quences& Type\ de\ transformation\\
\hline
T\in \mathbb{R},\ -2<T<2& |K|=1,\ K\ne 1 &Elliptique\\
T=\pm 2& K=1 &Parabolique\\
T\in \mathbb{R},\ |T|>2& K\in\mathbb{R},\ K>1 &Hyperbolique\\
T\in \mathbb{C},\ T\not{\in}[-2,2]& K\in\mathbb{C}&Loxodromique\\
\hline
\end{array}
\]

\subsection{Exemples}

\subsubsection{Similitude : $w=k e^{i\theta}z+t$ \verif}

Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}$=$\begin{pmatrix} \sqrt{k} e^{i\frac{\theta}{2}} & \frac{t}{\sqrt{k}}e^{-i\frac{\theta}{2}} \\0 & \frac{1}{\sqrt{k}}e^{-i\frac{\theta}{2}} \end{pmatrix}$ on obtient bien un déterminant unitaire, en effet :

\[
ad-bc=\frac{\sqrt{k}}{\sqrt{k}}e^{i\frac{\theta}{2}}e^{-i\frac{\theta}{2}}=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{\sqrt{k}e^{i\frac{\theta}{2}}z+\frac{t}{\sqrt{k}}e^{-i\frac{\theta}{2}}}{\frac{1}{\sqrt{k}}e^{-i\frac{\theta}{2}}}=\frac{k e^{i\frac{\theta}{2}}z+te^{-i\frac{\theta}{2}}}{e^{-i\frac{\theta}{2}}}
\]
On obtient une similitude, cet à dire la composition d'une rotation d'angle $\theta$ d'une translation de valeur $t$ et d'une homothétie de rapport $k$:
\[
w=k e^{i\theta}z+t
\]

Comme $c=0$, K s'exprime par:
\[
\boxed{K=\frac{a}{d}=a^2=k e^{i\theta}}
\]

Comme on est dans le cas général $c=0$, il existe deux points fixes qui sont :

\[
\xi_1=\frac{ab}{1-a^2} \quad \quad et\quad\quad \xi_2=\infty
\]

\[
\boxed{\xi_1=\frac{t}{1-k e^{i\theta}} \quad \quad et\quad\quad \xi_2=\infty}
\]

Par conséquent la forme normale de la transformation s'exprime par:

\[
(w-\xi_1)=K(z-\xi_1)
\]

\[
w=K(z-\xi_1)+\xi_1
\]

\[
\boxed{w=ke^{i\theta}\left(z-\frac{t}{1-ke^{i\theta}}\right)+\frac{t}{1-ke^{i\theta}}}
\]

Dénomination des cas élémentaires possibles:
\begin{itemize}
\item Si $t=0$ et $k=1$ on obtient une \textbf{rotation} pure d'angle $\theta$.
\item Si $t=0$ et $\theta=0$ alors on obtient une \textbf{homothétie} pure de rapport $k$.
\item Si $\theta=0$ et $k=1$ alors on obtient une \textbf{translation} pure de valeur $t$.
\end{itemize}

\subsubsection{Transformation inverse (Elliptique): $w=\frac{1}{z}$ }

Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}$=$\begin{pmatrix} 0 & i \\i & 0 \end{pmatrix}$ on obtient bien un déterminant unitaire, en effet :
\[
ad-bc=0-i^2=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{i}{iz}=\frac{1}{z}
\]
Comme la trace $T=(a+d)=0$ est nulle, on est dans le cas d'une \textbf{transformation elliptique} (voir \ref{eq_mobius_classification}), de plus le multiplicateur $K$ vaut:

\[
\boxed{K=\frac{T-\sqrt{T^2-4}}{T+\sqrt{T^2-4}}=\frac{-\sqrt{-4}}{\sqrt{-4}}=-1}
\]

Comme on est dans le cas général $c\ne0,\ T\ne\pm2$, il existe deux points fixes qui sont :

\[
\xi_1=\frac{(a-d)+\sqrt{T^2-4}}{2c} \quad\quad et\quad\quad \xi_2=\frac{(a-d)-\sqrt{T^2-4}}{2c}
\]
Soit si on remplace
\[
\xi_1=\frac{2i}{2i} \quad\quad et\quad\quad \xi_2=\frac{-2i}{2i}
\]
\[
\boxed{\xi_1=1 \quad\quad et\quad\quad \xi_2=-1}
\]
Qui sont bien des points fixes de la transformation $w=\frac{1}{z}$, la forme normale s'écrit donc :

\[
\frac{w-\xi_1}{w-\xi_2}=K\frac{z-\xi_1}{z-\xi_2}
\]

\[
\boxed{\frac{w-1}{w+1}=-\frac{z-1}{z+1}}
\]

\subsubsection{Transformation réflexion (Elliptique): $w=1-z$ }

Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}$=$\begin{pmatrix} -i & i \\0 & i \end{pmatrix}$ on obtient bien un déterminant unitaire, en effet :
\[
ad-bc=-i^2-0=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{-iz+i}{i}=1-z
\]
Comme la trace $T=(-i+i)=0$ est nulle, on est dans le cas d'une \textbf{transformation elliptique} (voir \ref{eq_mobius_classification}), de plus le multiplicateur $K$ vaut:
\[
\boxed{K=\frac{T-\sqrt{T^2-4}}{T+\sqrt{T^2-4}}=\frac{-\sqrt{-4}}{\sqrt{-4}}=-1 \quad\quad ou\quad\quad K=\frac{a}{d}=\frac{-i}{i}=-1}
\]

Comme on est dans le cas $c=0$, il existe deux points-fixes l'un est le point à l'infini $\xi_2=\infty$ qui reste inchangé, l'autre est finit et il vaut :

\[
\boxed{\xi_1=\frac{ab}{1-a^2}=\frac{-i^2}{1-i^2}=\frac{1}{2}}
\]

Qui sont bien des points fixes de la transformation $w=1-z$, la forme normale s'écrit donc :

\[
(w-\xi_1)=K(z-\xi_1)
\]
\[
\boxed{\left(w-\frac{1}{2}\right)=\left(\frac{1}{2}-z\right)}
\]

\subsubsection{Transformation inversion + réflexion (Elliptique): $w=\frac{z-1}{z}$}

Cette transformation s'obtient par composition d'une inversion puis d'une réflexion. Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}=\begin{pmatrix} -i & i \\0 & i \end{pmatrix}\begin{pmatrix} 0 & i \\i & 0 \end{pmatrix}=\begin{pmatrix} -1 & 1 \\-1 & 0 \end{pmatrix}$, on obtient bien un déterminant unitaire, en effet vérifions:
\[
ad-bc=0+1=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{z-1}{z}
\]
Comme la trace $T=(-1+0)=-1$, nous somme toujours dans le cas d'une \textbf{transformation elliptique} (voir \ref{eq_mobius_classification}), de plus le multiplicateur $K$ vaut:
\[
\boxed{K=\frac{-1-\sqrt{1-4}}{-1+\sqrt{1-4}}=\frac{1+i\sqrt{3}}{1-i\sqrt{3}}=\frac{-1+i\sqrt{3}}{2}}
\]
Comme on est dans le cas général $c\ne0,\ T\ne\pm2$, il existe deux points fixes qui sont :
\[
\xi_1=\frac{(a-d)+\sqrt{T^2-4}}{2c} \quad\quad et\quad\quad \xi_2=\frac{(a-d)-\sqrt{T^2-4}}{2c}
\]
\[
\boxed{\xi_1=\frac{1-i\sqrt{3}}{2} \quad\quad et\quad\quad \xi_2=\frac{1+i\sqrt{3}}{2}}
\]
On vérifie aisément qu'il s'agit bien de deux points fixes, de la transformation $w=\frac{z-1}{z}$, la forme normale s'écrit donc :

\[
\frac{w-\xi_1}{w-\xi_2}=K\frac{z-\xi_1}{z-\xi_2}
\]

\[
\frac{w-\left(\frac{1-i\sqrt{3}}{2}\right)}{w-\left(\frac{1+i\sqrt{3}}{2}\right)}=\left(\frac{-1+i\sqrt{3}}{2}\right) \frac{z-\left(\frac{1-i\sqrt{3}}{2}\right)}{z-\left(\frac{1+i\sqrt{3}}{2}\right)}
\]
Ce qui se simplifie un légèrement:
\[
\boxed{\frac{2w-\left(1-i\sqrt{3}\right)}{2w-\left(1+i\sqrt{3}\right)}=\left(\frac{-1+i\sqrt{3}}{2}\right) \frac{2z-\left(1-i\sqrt{3}\right)}{2z-\left(1+i\sqrt{3}\right)}}
\]

\subsubsection{Transformation réflexion + inversion (Elliptique): $w=\frac{1}{1-z}$}

Cette transformation s'obtient par composition d'une réflexion puis d'une inversion. Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}=\begin{pmatrix} 0 & i \\i & 0 \end{pmatrix}\begin{pmatrix} -i & i \\0 & i \end{pmatrix}=\begin{pmatrix} 0  & -1  \\ 1 & -1  \end{pmatrix}$, on obtient bien un déterminant unitaire, en effet vérifions:

\[
ad-bc=0+1=1
\]

Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{1}{1-z}
\]

Comme la trace $T=(0-1)=-1$, nous somme toujours dans le cas d'une \textbf{transformation elliptique} (voir \ref{eq_mobius_classification}), de plus le multiplicateur $K$ vaut:
\[
\boxed{K=\frac{-1-\sqrt{1-4}}{-1+\sqrt{1-4}}=\frac{1+i\sqrt{3}}{1-i\sqrt{3}}=\frac{-1+i\sqrt{3}}{2}}
\]
Comme on est dans le cas général $c\ne0,\ T\ne\pm2$, il existe deux points fixes qui sont :
\[
\xi_1=\frac{(a-d)+\sqrt{T^2-4}}{2c} \quad\quad et\quad\quad \xi_2=\frac{(a-d)-\sqrt{T^2-4}}{2c}
\]
\[
\boxed{\xi_1=\frac{1+i\sqrt{3}}{2} \quad\quad et\quad\quad \xi_2=\frac{1-i\sqrt{3}}{2}}
\]

On vérifie aisément qu'il s'agit bien de deux points fixes, de la transformation $w=\frac{1}{1-z}$, la forme normale s'écrit donc :

\[
\frac{w-\xi_1}{w-\xi_2}=K\frac{z-\xi_1}{z-\xi_2}
\]

Ce qui donne une expression très similaire à celle obtenue précédemment sauf que les deux points fixes ont été inversés:
\[
\boxed{\frac{2w-\left(1+i\sqrt{3}\right)}{2w-\left(1-i\sqrt{3}\right)}=\left(\frac{-1+i\sqrt{3}}{2}\right) \frac{2z-\left(1+i\sqrt{3}\right)}{2z-\left(1-i\sqrt{3}\right)}}
\]

\subsubsection{Transformation quelconque (Parabolique)}

Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}$=$\begin{pmatrix} 1 & 0 \\1 & 1 \end{pmatrix}$ on obtient bien un déterminant unitaire, en effet :
\[
ad-bc=1-0=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{z}{z+1}
\]
Comme la trace $T=(1+1)=2$ est égale à $2$, on est dans le cas d'une \textbf{transformation parabolique} (voir \ref{eq_mobius_classification}), de plus le multiplicateur $K$ vaut:

\[
\boxed{K=1}
\]
Comme on est dans le cas $T=2$, il existe un seul point fixe qui est :

\[
\boxed{\xi=\frac{a-d}{2c}=0}
\]

La forme normale s'écrit alors trivialement :

\[
\frac{1}{w-\xi}=\frac{1}{z-\xi}\pm c
\]
Soit comme $T$ est positif :
\[
\boxed{\frac{1}{w}=\frac{1}{z} + 1}
\]

\subsubsection{Transformation quelconque (Hyperbolique)}

Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}$=$\begin{pmatrix} 1 & 1 \\2 & 3 \end{pmatrix}$ on obtient bien un déterminant unitaire, en effet :
\[
ad-bc=3-2=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{az+b}{cz+d}=\frac{z+1}{2z+3}
\]
Comme la trace $T=(1+3)=4$ est réelle et supérieur à $2$, on est dans le cas d'une \textbf{transformation hyperbolique} (voir \ref{eq_mobius_classification}), de plus le multiplicateur $K$ vaut:

\[
\boxed{K=\frac{T-\sqrt{T^2-4}}{T+\sqrt{T^2-4}}=\frac{4-\sqrt{12}}{4+\sqrt{12}}=7-4\sqrt{3}}
\]

Comme on est dans le cas général $c\ne0,\ T\ne\pm2$, il existe deux points fixes qui sont :

\[
\xi_1=\frac{(a-d)+\sqrt{T^2-4}}{2c} \quad\quad et\quad\quad \xi_2=\frac{(a-d)-\sqrt{T^2-4}}{2c}
\]

\[
\xi_1=\frac{-2+\sqrt{12}}{4} \quad\quad et\quad\quad \xi_2=\frac{-2-\sqrt{12}}{4}
\]
Ce qui ce simplifie un peu :
\[
\boxed{\xi_1=\frac{-1+\sqrt{3}}{2} \quad\quad et\quad\quad \xi_2=\frac{-1-\sqrt{3}}{2}}
\]
La forme normale s'écrit donc :
\[
\frac{w-\xi_1}{w-\xi_2}=K\frac{z-\xi_1}{z-\xi_2}
\]

\subsubsection{Transformation rotation de mobius \workon}

Si on prend $\begin{pmatrix} a & b \\c & d \end{pmatrix}$=$\begin{pmatrix} cos(\alpha) & sin(\alpha) \\-sin(\alpha) & cos(\alpha) \end{pmatrix}$ on obtient bien un déterminant unitaire, en effet :
\[
cos(\alpha)^2+sin(\alpha)^2=1
\]
Si on écrit la transformation de Möbius correspondante, on obtient:
\[
w=\frac{cos(\alpha)z+sin(\alpha)}{-sin(\alpha)z+cos(\alpha)}
\]
Comme la trace $T=2 cos(\alpha)$ est réelle et inférieure ou égale à $2$, on est dans le cas d'une \textbf{transformation elliptique ou parabolique} (voir \ref{eq_mobius_classification}). Si on prend $\alpha=\frac{\pi}{2}$, on obtient:
\[
w=-\frac{1}{z}
\]

\newpage
\section{Rayon de convergence d'une série \workon}

Soit une série du type :
\[
S=\sum_{n=0}^\infty A_n z^n
\]

\subsection{Théorème de Cauchy-Hadamard\proof}

Le rayon de de convergence $R$ d'une telle série s'écrit:
\[
\boxed{\frac{1}{R}=\limsup\limits_{n\to \infty}\ |A_n|^\frac{1}{n}}
\]
\notes{Cette formule donne à coup sur le rayon de convergence de la série mais il n'est pas toujours facile de manipuler des limites supérieures. C'est pourquoi dans la pratique on utilise plutôt des critères plus simples. Ces critères, s'ils sont vérifiés garantissent la convergence, mais s'il ne le sont pas, on ne peut rien affirmer.}

\subsection{Critère de Cauchy \proof}

Alors \textbf{si la limite existe}, le rayon de convergence $R$ s'exprime par: 
\[
\boxed{\frac{1}{R}= \lim\limits_{n\to \infty} |A_n|^\frac{1}{n}}
\]

\subsection{Critère de d'Alembert\proof}

Alors \textbf{si la limite existe}, le rayon de convergence $R$ s'exprime par:
\[
	\boxed{\frac{1}{R}=\lim\limits_{n\to \infty} \left| \frac{A_{n+1}}{A_n} \right|}
\]

Le critère de d'Alembert implique le critère de Cauchy, mais la réciproque est fausse. Dans la pratique il est plus facile de manipuler le critère de d'Alembert.

\subsection{Par la distance au pôle plus proche\proof}

\noindent
Pour un développement de Taylor-Laurent, les rayons de convergences sont en lien étroit avec les pôles de la fonction. Le rayon de convergence peut alors être définit par la distance entre le point de calcul et le pôle le plus proche.

\[
\boxed{R=\left|a-p\right| \quad avec \quad p:pole\ le\ plus\ proche }
\]


\newpage
\section{Series de Taylor-Laurent}
\subsection{Définitions}

Une fonction \textbf{holomorphe} est une fonction \textbf{analytique} c'est à dire que elle peut être approximée au voisinage $a$ localement par sont développement en \textbf{séries de Taylor}:

\label{eq_taylor}
\begin{equation}
\boxed{f(z)=\sum_0^\infty C_n (z-a)^n \quad avec \quad C_n=\frac{1}{2i\pi}\oint_C \frac{f(z)}{(z-a)^{n+1}}dz\quad ou \quad C_n=\frac{f^{(n)}(a)}{n!}\quad\quad z\in D(a,R)}
\end{equation}

La série converge \textbf{dans un disque} $D(a,R)$ centré en a dont le rayon $R$ sera à déterminer.\\

Si on considère les exposants négatifs on obtient les \textbf{séries de Laurent}:

\label{eq_laurent}
\begin{equation}
\boxed{f(z)=-\sum_{-\infty}^{-1} C_n (z-a)^n \quad avec \quad C_n=\frac{1}{2i\pi}\oint_C \frac{f(z)}{(z-a)^{n+1}}dz\quad\quad z\notin D(a,r)}
\end{equation}

La série converge dans une région privée d'un disque $D(a,R)$ dont le rayon $R$ est à déterminer.\\

La réunion des exposants positifs et négatifs donne les séries de \textbf{Taylor-Laurent} (parfois simplement appelées séries de Laurent) 

\label{eq_taylor_laurent}
\begin{equation}
\boxed{f(z)=\sum_{-\infty}^{\infty} C_n (z-a)^n \quad avec \quad C_n=\frac{1}{2i\pi}\oint_C \frac{f(z)}{(z-a)^{n+1}}dz\quad\quad z \in A(a,r,R)}
\end{equation}

La série converge dans \textbf{un anneau} $A(a,r,R)$ dont les rayons $r$ et $R$ seront à déterminer.

\subsection{Démonstration : séries de Taylor}
\label{eq_taylor_dem}
Soit $f(z)$ une fonction \textbf{holomorphe}, partons de la formule intégrale de Cauchy ( voir \ref{eq_cauchy_int} ) :
\[
f(z)= \frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{u-z} du
\]
Quelque-soit, le complexe $a$, on peut écrire:
\[
\frac{f(u)}{u-z}=\frac{f(u)}{u-z-a+a}=\frac{f(u)}{u-a+a-z}
\]
\[
\frac{f(u)}{u-z}=\frac{f(u)}{u-a}\frac{1}{1+\frac{a-z}{u-a}}=\frac{f(u)}{u-a}\frac{1}{1-\frac{z-a}{u-a}}
\]

Si $\left|\frac{z-a}{u-a}\right|<1$, alors on peut utiliser la série géométrique ($q<1 \quad \rightarrow\quad \frac{1}{1-q}=\sum_0^\infty q^n$):
\[
\frac{f(u)}{u-z}=\frac{f(u)}{u-a} \sum_{n=0}^\infty \left(\frac{z-a}{u-a}\right)^n
\]

Si pour le chemin $\gamma$, on prend le cercle centré en $a$ et de rayon $R$ en posant $u=a+Re^{it}$, alors pour tout $z$ appartenant à $D(a,R)$ le disque de centre a et de rayon $R$ on à bien:

\[
z\in D(a,R) \quad\Rightarrow\quad \left|\frac{z-a}{u-a}\right|=\frac{|z-a|}{R}<1
\]

On peut remplacer dans l'intégrale:
\[
f(z)= \frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{u-a} \sum_{n=0}^\infty \left(\frac{z-a}{u-a}\right)^n du
\]
L'inversion somme-intégrale se justifie par la convergence uniforme de la série géométrique:
\[
f(z)=  \sum_{n=0}^\infty \frac{1}{2i\pi} \oint_\gamma \frac{f(u)}{u-a} \left(\frac{z-a}{u-a}\right)^n du
\]
On constate que $(z-a)^n$ ne dépend pas de $u$ donc on peut simplifier l'expression pour obtenir:
\[
\boxed{f(z)=  \sum_{n=0}^\infty \left(\frac{1}{2i\pi} \oint_\gamma \frac{f(u)}{(u-a)^{n+1}}du\right) (z-a)^n \quad\quad z\in D(a,r)}
\]
Ce qui démontre la décomposition en série de Taylor. Aussi ,de la relation obtenue sur le lien entre les dérivées n-ième et l'intégrale de Cauchy (voir \ref{eq_cauchy_deriv}) on déduit immédiatement que :

\[
f^{(n)}(a)= \frac{n!}{2i\pi}\oint_\gamma \frac{f(z)}{(z-a)^{n+1}} dz
\]
\[
\boxed{\frac{f^{(n)}(a)}{n!}= C_n}
\]

Donc $f(z)$ est une fonction \textbf{analytique}.

\subsection{Démonstration : séries de Laurent}
\label{eq_laurent_dem}
Partons de la formule intégrale de Cauchy ( voir \ref{eq_cauchy_int} ) :
\[
f(z)= \frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{u-z} du
\]
Quelque-soit, le complexe $a$, on peut écrire:
\[
\frac{f(u)}{u-z}=\frac{f(u)}{u-z-a+a}=\frac{f(u)}{u-a+a-z}
\]
\[
\frac{f(u)}{u-z}=\frac{f(u)}{u-a}\frac{1}{1+\frac{a-z}{u-a}}=\frac{f(u)}{u-a}\frac{1}{1-\frac{z-a}{u-a}}
\]
Si $\left|\frac{z-a}{u-a}\right|>1$, alors on utilise la série géométrique ($q>1 \quad \rightarrow\quad \frac{1}{1-q}=-\frac{1}{q}\sum_0^\infty\frac{1}{q^n}$):
\[
\frac{f(u)}{u-z}=-\frac{f(u)}{u-a} \frac{u-a}{z-a}\sum_{n=0}^\infty \left(\frac{u-a}{z-a}\right)^n
\]
\[
\frac{f(u)}{u-z}=-\frac{f(u)}{z-a}\sum_{n=0}^\infty \left(\frac{u-a}{z-a}\right)^n
\]
Si pour le chemin $\gamma$, on prend le cercle centré en $a$ et de rayon $r$ en posant $u=a+re^{it}$, alors pour tout $z$ \textbf{n'appartenant pas} à $D(a,r)$ le disque de centre a et de rayon $r$ on à bien:
\[
z\notin D(a,r) \quad\Rightarrow\quad \left|\frac{z-a}{u-a}\right|=\frac{|z-a|}{r}>1
\]
Donc on peut remplacer dans l'intégrale, cela donne :
\[
f(z)= \frac{1}{2i\pi}\oint_\gamma -\frac{f(u)}{z-a}\sum_{n=0}^\infty \left(\frac{u-a}{z-a}\right)^n du
\]
\[
f(z)= -\frac{1}{2i\pi}\sum_{n=0}^\infty \frac{1}{z-a}\oint_\gamma f(u)\left(\frac{u-a}{z-a}\right)^n du
\]

\[
f(z)= -\frac{1}{2i\pi}\sum_{n=0}^\infty \frac{1}{(z-a)^{n+1}}\oint_\gamma f(u)(u-a)^n du
\]

\[
f(z)= \sum_{n=0}^\infty \left(-\frac{1}{2i\pi}\oint_\gamma f(u)(u-a)^n du \right) \frac{1}{(z-a)^{n+1}}
\]

\[
f(z)= \sum_{n=0}^\infty \left(-\frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{(u-a)^{-n}} du \right) (z-a)^{-n-1}
\]
Si on change la borne inférieure:
\[
f(z)= \sum_{n=1}^\infty \left(-\frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{(u-a)^{-(n-1)}} du \right) (z-a)^{-(n-1)-1}
\]
Simplifions:
\[
f(z)= \sum_{n=1}^\infty \left(-\frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{(u-a)^{-n+1}} du \right) (z-a)^{-n}
\]
On peut finalement remarquer que si on change les bornes:
\[
f(z)= -\sum_{n=-\infty}^{-1} \left(\frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{(u-a)^{n+1}} du \right) (z-a)^{n}
\]
\[
\boxed{f(z)= -\sum_{n=-\infty}^{-1} C_n (z-a)^{n}}
\]

\subsection{Démonstration : séries de Taylor-Laurent}
\label{eq_taylor_laurent_dem}
Partons de la formule intégrale de Cauchy ( voir \ref{eq_cauchy_int} ) :
\[
f(z)= \frac{1}{2i\pi}\oint_\gamma \frac{f(u)}{u-z} du
\]

Pour le chemin on peut choisir de parcourir:
\[
f(z)= \frac{1}{2i\pi}\left(\oint_{CR} \frac{f(u)}{u-z} du-\oint_{Cr} \frac{f(u)}{u-z} du +\int_A^B \frac{f(u)}{u-z} du +\int_B^A \frac{f(u)}{u-z} du \right)
\]
Où $CR$ est un cercle de rayon $R$ centré en $a$ et $Cr$ un cercle de rayon $r$ centré en $a$. Les deux intégrales suivant le segment $AB$ s'annulent, et on obtient:
\[
f(z)= \frac{1}{2i\pi}\oint_{CR} \frac{f(u)}{u-z} du-\frac{1}{2i\pi}\oint_{Cr} \frac{f(u)}{u-z} du 
\]
Et d'après les résultats obtenus précédemment on peut calculer les intégrale avec Taylor pour la première et avec Laurent pour la seconde:
\[
f(z)= \sum_{n=0}^{\infty} C_n (z-a)^{n}+\sum_{n=-\infty}^{-1} C_n (z-a)^{n}
\]
Ce qui peut se fusionner en une seule somme
\[
\boxed{f(z)= \sum_{n=-\infty}^{\infty} C_n (z-a)^{n}}
\]
Comme les deux sommes doivent converger cette représentation converge dans un anneau $A(a,r,R)$.

\subsection{Unicité}
\label{unicite_dl}
Si posons deux représentation d'une fonction $f(z)$ holomophe sur $D$, un anneau $r< |z-a| <R$ :
\[
f(z)=\sum_{-\infty}^\infty A_n (z-a)^n=\sum_{-\infty}^\infty B_n (z-a)^n
\]
On multiplie par $(z-a)^{-k-1}$ pour obtenir:
\[
\sum_{-\infty}^\infty A_n (z-a)^n(z-a)^{-k-1}=\sum_{-\infty}^\infty B_n (z-a)^n(z-a)^{-k-1}
\]
\[
\sum_{-\infty}^\infty A_n (z-a)^{n-k-1}=\sum_{-\infty}^\infty B_n (z-a)^{n-k-1}
\]
Choisissons maintenant un Lacet $\gamma\in D$ alors:
\[
\oint_\gamma\sum_{-\infty}^\infty A_n (z-a)^{n-k-1}dz=\oint_\gamma\sum_{-\infty}^\infty B_n (z-a)^{n-k-1}dz
\]
Si on permute la somme et l'intégrale:
\[
\sum_{-\infty}^\infty A_n \oint_\gamma(z-a)^{n-k-1}dz=\sum_{-\infty}^\infty B_n \oint_\gamma(z-a)^{n-k-1}dz
\]
Et l'intégrale (voir \ref{eq_cauchy_contour}) :
\[
 \oint_\gamma(z-a)^{n-k-1}dz = 2i\pi \delta_{nk} 
\]
Avec $\delta_{nk}$ le symbole de Kronecker : 
\[
\sum_{-\infty}^\infty A_n 2i\pi \delta_{nk} =\sum_{-\infty}^\infty B_n 2i\pi \delta_{nk} 
\]
Donc quelque soit $k$:
\[
\boxed{A_k=B_k}
\]
Les coefficients sont donc nécessairement égaux et le développement en série de Taylor-Laurent est unique.

\subsection{Exemples : Séries de Taylor}

\subsubsection{Fonction exponentielle : $exp(z)$}
\label{dl_exponentiel}
Comme la fonction exponentielle est sa propre dérivée c'est le cas le plus facile à traiter, la formule donne directement:
\[
e^z=e^0+\frac{e^0}{1!}z+\frac{e^0}{2!}z^2+\frac{e^0}{3!}z^3+\frac{e^0}{4!}z^4+...
\]
\[
e^z=1+\frac{z}{1!}+\frac{z^2}{2!}+\frac{z^3}{3!}+\frac{z^4}{4!}+...
\]
Finalement on obtient:

\begin{equation}
\boxed{e^z=\sum_{n=0}^{\infty}\frac{z^{n}}{n!}=1+\frac{z}{1}+\frac{z^2}{2}+\frac{z^3}{6}+\frac{z^4}{24}+...+O(z^5)}
\end{equation}

\graphpartiallow{exp}{$e^z$}{T}

\rinf

\subsubsection{Fonction puissance complexe : $a^z$ }
\label{dl_power_a}
Plus généralement pour toute base $a$, on peut donner un sens à l'expression $f(z)=a^z$ même si $z\in\mathbb{C}$, en effet, on peut écrire que:
\[
	a^z=e^{ln(a^z)}
\]
Et de part les propriétés du logarithme et de l'exponentielle , on obtient:
\[
	a^z=e^{z ln(a)}
\]
On en déduit directement du développement de l'exponentielle que :
\begin{equation}
\boxed{a^z=\sum_{0}^\infty \frac{ln(a)^n}{n!} z^n}
\end{equation}

\rinf

\subsubsection{Fonction sinus hyperbolique: $sinh(z)$}
\label{dl_sinh}
La fonction sinus hyperbolique s'exprime par la notation d'Euler (voir \ref{eq_exp_trig})\\

\[
	sinh(z)=\frac{e^{z}-e^{-z}}{2}
\]

\[
	sinh(z)=\frac{\sum_{n=0}^{\infty}\frac{z^{n}}{n!}-\sum_{n=0}^{\infty}\frac{(-z)^{n}}{n!}}{2}
\]

\[
	sinh(z)=\frac{\sum_{n=0}^{\infty}\frac{z^{n}}{n!}-\frac{(-z)^{n}}{n!}}{2}
\]
On constate alors que lorsque $n$ est pair, le terme est nul et lorsque $n$ est impaire, le terme est doublé, on obtient:

\begin{equation}
\boxed{sinh(z)=\sum_{n=0}^{\infty}\frac{z^{1+2 n}}{(1+2 n)!}=z+\frac{z^3}{6}+\frac{z^5}{120}+...+O(z^7)}
\end{equation}

\graphpartiallow{sinh}{$sinh(z)$}{T}

\rinf

\subsubsection{Fonction sinus : $sin(z)$}
\label{dl_sin}

\[
sin(z)=sin(0)+\frac{sin'(0)}{1!}z+\frac{sin''(0)}{2!}z^2+\frac{sin'''(0)}{3!}z^3+\frac{sin''''(0)}{4!}z^4+...
\]
\[
sin(z)=sin(0)+\frac{cos(0)}{1!}z-\frac{sin(0)}{2!}z^2-\frac{cos(0)}{3!}z^3+\frac{sin(0)}{4!}z^4+...
\]
\[
sin(z)=\frac{z}{1!}-\frac{z^3}{3!}+\frac{z^5}{5!}-\frac{z^7}{7!}+...
\]
Finalement on obtient:

\begin{equation}
\boxed{sin(z)=\sum_{n=0}^{\infty}\frac{z^{1+2 n}}{(1+2 n)!}(-1)^n=z-\frac{z^3}{6}+\frac{z^5}{120}-...+O(z^7)}
\end{equation}

\graphpartiallow{sin}{$sin(z)$}{T}

\rinf

\subsubsection{Fonction cosinus hyperbolique: $cosh(z)$}
\label{dl_cosh}

Avec la notation d'Euler (voir \ref{eq_exp_trig}), on a:
\[
	cosh(z)=\frac{e^z+e^{-z}}{2}
\]
\[
	cosh(z)=\frac{\sum_{n=0}^{\infty}\frac{z^{n}}{n!}+\frac{(-z)^{n}}{n!}}{2}
\]
On constate que les termes impairs sont nuls:
\begin{equation}
	\boxed{cosh(z)=\sum_{n=0}^{\infty}\frac{z^{2n}}{2n!}=1+\frac{z^2}{2}+\frac{z^4}{24}+...+O(z^6)}
\end{equation}

\graphpartiallow{cosh}{$cosh(z)$}{T}

\rinf

\subsubsection{Fonction cosinus : $cos(z)$}
\label{dl_cos}
\[
cos(z)=cos(0)+\frac{cos'(0)}{1!}z+\frac{cos''(0)}{2!}z^2+\frac{cos'''(0)}{3!}z^3+\frac{cos''''(0)}{4!}z^4+...
\]

\[
cos(z)=cos(0)-\frac{sin(0)}{1!}z-\frac{cos(0)}{2!}z^2+\frac{sin(0)}{3!}z^3+\frac{cos(0)}{4!}z^4+...
\]

\[
cos(z)=1-\frac{z^2}{2!}+\frac{z^4}{4!}-\frac{z^6}{6!}+...
\]
Finalement on obtient:

\begin{equation}
\boxed{cos(z)=\sum_{n=0}^{\infty}\frac{z^{2 n}}{(2 n)!}(-1)^n=1-\frac{z^2}{2}+\frac{z^4}{24}-...+O(z^6)}
\end{equation}

\graphpartiallow{cos}{$cos(z)$}{T}

\rinf

\subsubsection{Fonction génératrice des nombres de Bernoulli: $B(z)=\frac{z}{e^z-1}$}

Comme $e^z-1=\sum_{n=1}^{\infty}\frac{z^{n}}{n!}$ on peut écrire :

\[
f(z)=\frac{z}{e^z-1}=\frac{z}{\sum_{n=1}^{\infty}\frac{z^{n}}{n!}}
\]
Et si on pose $f(z)=\sum_{n=0}^{\infty} C_n z^{n}$
\[
\left(\sum_{n=0}^{\infty} C_n z^{n} \right)\left( \sum_{n=1}^{\infty}\frac{z^{n}}{n!} \right)=z
\]
Si on divise par $z$:
\[
\left(\sum_{n=0}^{\infty} C_n z^{n} \right)\left( \sum_{n=0}^{\infty}\frac{z^{n}}{(n+1)!} \right)=1
\]
Et on peut par comparaisons successives en déduire tous les coefficients. Ce que on peut mettre sous forme matricielle :
\[
\left(
\begin{array}{ccccc}
1 &0 &0 &0 &\hdots \\
\frac{1}{2!} &1 &0 &0 &\hdots \\
\frac{1}{3!} &\frac{1}{2!} &1 &0 &\hdots \\
\frac{1}{4!} &\frac{1}{3!} &\frac{1}{2!} &1 &\hdots \\
\vdots &\vdots &\vdots &\vdots &\ddots \\
\end{array}
\right)
.
\left(
\begin{array}{c}
C_0\\
C_1\\
C_2\\
C_3\\
\vdots\\
\end{array}
\right)
=
\left(
\begin{array}{c}
1\\
0\\
0\\
0\\
\vdots\\
\end{array}
\right)
\]
On peut alors en déduire l'expression de $C_n$ :

\[
\left\{
\begin{array}{l}
C_0=1\\
C_1=-\frac{1}{2!}C_0\\
C_2=-\frac{1}{2!}C_1-\frac{1}{3!}C_0\\
C_3=-\frac{1}{2!}C_2-\frac{1}{3!}C_1-\frac{1}{4!}C_0\\
...\\
\end{array}
\right.
\quad\quad\Rightarrow\quad\quad
\boxed{C_n=-\sum_{k=0}^{n-1}C_k\frac{1}{(n+1-k)!}}
\]

On obtient alors les nombres de Bernoulli avec $B_n=n!C_n$ car $C_n=\frac{B_n}{n!}=f^{(n)}(0)$. Ce qui donne :
\label{eq_bernoulli}
\begin{equation}
	\boxed{\frac{z}{e^z-1}=\sum_{n=0}^{\infty} \frac{B_n}{n!} z^n} \quad\quad\quad et \quad\quad\quad \boxed{B_n=-n!\sum_{k=0}^{n-1}\frac{B_k}{k!}\frac{1}{(n+1-k)!}}
\end{equation}

\noindent
La formule récurrente pour les nombres de Bernoulli peut aussi se réécrire avec les coefficients binomiaux (voir \ref{coef_bin}):
\begin{equation}
B_n=-\frac{1}{n+1}\sum_{k=0}^{n-1}\frac{n+1!}{(n+1-k)!k!}B_k \quad\quad\quad Soit \quad\quad\quad  \boxed{B_n=-\frac{1}{n+1}\sum_{k=0}^{n-1} C_{n+1}^{k} B_k }
\end{equation}

\graphpartialhigh{bernoulli}{$\frac{z}{e^z-1}$}{T}

\noindent
\textbf{Rayon de convergence:}  Comme la fonction possède des pôles en $z_p=2k\pi$ et que le développent de Taylor-Laurent est calculé en zéro. Le rayon de convergence est de $\boxed{r=2\pi}$ (distance au pôle le plus proche).\\

\noindent
Voici un aperçu des premiers nombres de Bernoulli de 0 à 20 (pour une liste plus grande voir \ref{table_bern}).
\[
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0&1&2&3&4&5&6&7&8&9&10&11&12&13&14&15&16&17&18&19&20\\
\hline
1&-\frac{1}{2}&\frac{1}{6}&0&-\frac{1}{30}&0&\frac{1}{42}&0&-\frac{1}{30}&0&\frac{5}{66}&0&-\frac{691}{2730}&0&\frac{7}{6}&0&-\frac{3617}{510}&0&\frac{43867}{798}&0&-\frac{174611}{330}\\
\hline
\end{array}
\]
\noindent
Les nombres de Bernoulli diverges très rapidement.

\subsubsection{Fonction cotangente hyperbolique: $coth(z)$}
\label{dl_coth}
La cotangente hyperbolique s'écrit avec la formule d'Euler:
\[
coth(z)=\frac{e^{z}+e^{-z}}{e^{z}-e^{-z}}=\frac{e^{2z}+1}{e^{2z}-1}
\]
Et la fonction génératrice des nombres de Bernoulli (voir \ref{eq_bernoulli}) s'écrit $B(z)=\frac{z}{e^z-1}=\sum_{n=0}^{\infty} \frac{B_n}{n!} z^n$, mais si on pose:
\[
B'(z)=B(z)+\frac{z}{2}=\frac{2z}{2e^z-1}+\frac{z(e^z-1)}{2e^z-1}=\frac{z}{2}\frac{e^z+1}{e^z-1}
\]
\[
B'(z)=\sum_{n=0}^{\infty} \frac{B_n}{n!} z^n+\frac{z}{2}
\]
Comme $B_1=-\frac{1}{2}$ et le seul nombre de Bernoulli non nul d'indice impaire, on peut simplifier en:
\[
B'(z)=\sum_{n=0}^{\infty} \frac{B_{2n}}{2n!} z^{2n}
\]
Donc
\[
coth(z)=\frac{B'(2z)}{z}=\frac{1}{z}\sum_{n=0}^{\infty} \frac{B_{2n}}{2n!} (2z)^{2n}
\]
On obtient ici une série de Laurent car le premier exposant est négatif.
\begin{equation}
\boxed{coth(z)=\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!} z^{2n-1}=\frac{1}{z}+\frac{z}{3}-\frac{z^3}{45}+\frac{2z^5}{945}+...+O(z^6)}
\end{equation}

\graphpartialhigh{coth}{$coth(z)$}{T}

\textbf{Rayon de convergence:} Comme la fonction possède des pôles en $z_p=k\pi$ et que le développent de Taylor-Laurent est calculé en zéro. Le rayon de convergence est de $\boxed{r=\pi}$ (distance au pôle le plus proche).\\

\subsubsection{Fonction cotangente: $cot(z)$}
\label{dl_cot}
On à la relation avec la cotangente hyperbolique:
\[
cot(z)=-i\ coth(-i z)
\]
Soit 
\[
cot(z)=-i\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!} (-i)^{2n-1}z^{2n-1}
\]
Si on rentre $-i$ dans la somme :
\[
cot(z)=\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!} (-i)^{2n}z^{2n-1}
\]
Mais
\[
	(-i)^{2n}=(-1)^n
\]
Donc
\begin{equation}
\boxed{cot(z)=\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n z^{2n-1}=\frac{1}{z}-\frac{z}{3}-\frac{z^3}{45}-\frac{2z^5}{945}+...+O(z^6)}
\end{equation}

\graphpartialhigh{cot}{$cot(z)$}{T}

\textbf{Rayon de convergence:} Comme la fonction possède des pôles en $z_p=k\pi$ et que le développent de Taylor-Laurent est calculé en zéro. Le rayon de convergence est de $\boxed{r=\pi}$ (distance au pôle le plus proche).\\

\subsubsection{Fonction tangente hyperbolique : $tanh(z)$}
\label{dl_tanh}
La tangente hyperbolique s'écrit en fonction de la cotangente hyperbolique:
\[
tanh(z)=2 coth(2 z)-coth(z)
\]
Soit si on remplace par l'expression trouvée pour la cotangente:
\[
tanh(z)=2 \sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!} (2z)^{2n-1}-\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!} z^{2n-1}
\]
Si on regroupe les sommes:
\[
tanh(z)=\sum_{n=0}^{\infty} 2 \frac{2^{2n} B_{2n}}{2n!} (2z)^{2n-1}-\frac{2^{2n} B_{2n}}{2n!} z^{2n-1}
\]
On simplifie les puissances de 2 dans le premier terme:
\[
tanh(z)=\sum_{n=0}^{\infty} \frac{2^{2n}2^{2n} B_{2n}}{2n!} z^{2n-1}-\frac{2^{2n} B_{2n}}{2n!} z^{2n-1}
\]
Et si on factorise et que on constate que le terme $n=0$ est nul, on obtient:
\begin{equation}
\boxed{tanh(z)=\sum_{n=1}^{\infty} \frac{(2^{4n}- 2^{2n})B_{2n}}{2n!} z^{2n-1}=z-\frac{z^3}{3}+\frac{2z^5}{15}+...+O(z^6)}
\end{equation}

\graphpartialhigh{tanh}{$tanh(z)$}{T}


\subsubsection{Fonction tangente : $tan(z)$}
\label{dl_tan}
On a la relation avec la tangente hyperbolique:
\[
tan(z)=-i\ tanh(i z)
\]
Et d'après le résultat précédent :
\[
tan(z)=-i\ \sum_{n=0}^{\infty} \frac{(2^{4n}- 2^{2n})B_{2n}}{2n!} (i z)^{2n-1}
\]
\[
tan(z)=- \sum_{n=0}^{\infty} \frac{(2^{4n}- 2^{2n})B_{2n}}{2n!} i^{2n} z^{2n-1}
\]
Or $i^{2n}=(-1)^n$ donc
\[
tan(z)=- \sum_{n=0}^{\infty} \frac{(2^{4n}- 2^{2n})B_{2n}}{2n!} (-1)^n z^{2n-1}
\]
Finalement si on rentre le signe moins et que on constate que le terme $n=0$ est nul :
\begin{equation}
\boxed{tan(z)=\sum_{n=1}^{\infty} \frac{(2^{2n}-2^{4n})B_{2n}}{2n!} (-1)^n z^{2n-1}=z+\frac{z^3}{3}+\frac{2z^5}{15}+...+O(z^6)}
\end{equation}

\graphpartialhigh{tan}{$tan(z)$}{T}

\textbf{Rayon de convergence:} Comme la fonction possède des pôles en $z_p=k\frac{\pi}{2}$ et que le développent de Taylor-Laurent est calculé en zéro. Le rayon de convergence est de $\boxed{r=\frac{\pi}{2}}$ (distance au pôle le plus proche).\\

\subsubsection{Fonction géométrique : $\frac{1}{1-z}$}
\label{dl_geom}
Si on calculs les coefficients $C_n$ du la série de Taylor, on obtient:

\[
\begin{array}{l}
f(0)=1\\
f^{(1)}(0)=(\frac{1}{1-z})'=\frac{1}{(1-0)^2}=1\\
f^{(2)}(0)=(\frac{1}{1-z})''=-\frac{2}{(z-1)^3}=2\\
f^{(3)}(0)=(\frac{1}{1-z})'''=\frac{6}{(z-1)^4}=6\\
f^{(4)}(0)=(\frac{1}{1-z})'''=-\frac{24}{(z-1)^5}=24\\
f^{(5)}(0)=(\frac{1}{1-z})'''=\frac{120}{(z-1)^6}=120\\
f^{(n)}(0)=n!\\
\end{array}
\]
Donc 
\[
C_n=\frac{f^{(n)}(a)}{n!}=1
\]
Et le développement de Taylor s'écrit donc :
\[
\frac{1}{1-z}=1+z+z^2+z^3+z^4+...
\]

\noindent
Finalement on obtient:
\label{eq_dl_geometric}
\begin{equation}
\boxed{\frac{1}{1-z}=\sum_{n=0}^{\infty} z^n}
\end{equation}

\graphpartialhigh{geom}{$\frac{1}{1-z}$}{T}

\underline{\textbf{Rayon de convergence:}}
\begin{itemize}
\item La fonction possède un seul pôle en $p=1$ et on a calculé le développement de Taylor en $a=0$ on a donc un rayon de convergence de $\boxed{r=1}$.
\item q= $|\frac{z^{n+1}}{z^n}|=|z|$ donc le module de $z$ doit être inférieur à $1$, c'est à dire que $\boxed{r=1}$.
\end{itemize}

\subsubsection{Fonction : $\frac{1}{1+z}$}

Si on remplace $z$ par $-z$ dans l'expression précédente, on obtient:
\[
\frac{1}{1+z}=1-z+z^2-z^3+z^4-...
\]
\noindent
Soit
\begin{equation}
\boxed{\frac{1}{1+z}=\sum_{n=0}^{\infty} (-1)^n z^n}
\end{equation}

%\graphpartial{geomp}{$\frac{1}{1+z}$}{T}

\subsubsection{Fonction logarithme: $ln(1+z)$ et $ln(1-z)$}
\label{eq_dl_ln}
On peut observer que
\[
ln(1+z)=\int_0^z \frac{1}{1+z}
\]
Comme la fonction $ln(1+z)$ vaut zéro en zéro on peut sans perte de généralité intégrer termes à termes :
\[
ln(1+z)=z-\frac{z^2}{2}+\frac{z^3}{3}-\frac{z^4}{4}+...
\]
Finalement
\begin{equation}
\boxed{ln(1+z)=\sum_{n=1}^{\infty} \frac{(-1)^{n+1} z^n}{n}=z-\frac{z^2}{2}+\frac{z^3}{3}-\frac{z^4}{4}+...+O(z^5)}
\end{equation}

\graphpartialhigh{ln}{$ln(z)$}{T}

\underline{\textbf{Rayon de convergence:}}
\begin{itemize}
\item La fonction possède un seul pôle en $p=-1$ et on a calculé le développement de Taylor en $a=0$ on a donc un rayon de convergence de $\boxed{r=1}$.
\item q= $\lim\limits_{n\to\infty}|\frac{z^{n+1}/(n+1)}{z^n/n}|=\lim\limits_{n\to\infty}|\frac{nz^{n+1}}{(n+1)z^n}|=|z|$ donc le module de $z$ doit être inférieur à $1$, c'est à dire que $\boxed{r=1}$.
\end{itemize}
\vspace{0.5cm}

Maintenant si on remplace $z$ par $-z$ on trouve trivialement:

\begin{equation}
\boxed{ln(1-z)=-\sum_{n=1}^{\infty} \frac{z^n}{n}=-z-\frac{z^2}{2}-\frac{z^3}{3}-\frac{z^4}{4}-...+O(z^5)}
\end{equation}

\subsubsection{Fonction binôme: $(b+z)^a$}
\label{dl_bin}
Par la relation sur les dérivées composées de puissances (voir \ref{derive_puissance}):
\[
	(f(z)^a)'=a f(z)'f(z)^{a-1}
\]
La dérivée première s'exprime alors par :
\[
	((b+z)^a)^{(1)}=a (b+z)^{a-1}
\]
La dérivée seconde s'exprime par :
\[
	((b+z)^a)^{(2)}=\left(a-1\right)a (b+z)^{a-2}
\]
La dérivée troisième s'exprime par :
\[
	((b+z)^a)^{(3)}=\left(a-2\right)\left(a-1\right)a (b+z)^{a-3}
\]
La dérivée n-ième s'exprime par :
\[
	((b+z)^a)^{(n)}= (b+z)^{a-n} \prod_{k=0}^{n-1} \left(a-k\right)
\]
Les coefficients de la série s'écrivent alors :
\[
C_n= b^{a-n}\frac{\prod_{k=0}^{n-1} \left(a-k\right)}{n!}=b^{a-n} \frac{\prod_{k=1}^{n} \left(a-k+1\right)}{n!}=b^{a-n} \frac{\prod_{k=1}^{n} k\left(\frac{a+1}{k}-1\right)}{n!}=b^{a-n} \frac{n! \prod_{k=1}^{n}\left(\frac{a+1}{k}-1\right)}{n!}
\]
Donc on peut simplifier la factorielle et remarquer que on obtient les coefficients binomiaux généralisés ($a \in \mathbb{C}$) (voir \ref{coef_bin}):

\[
C_n= b^{a-n}\prod_{k=1}^{n} \left(\frac{a+1}{k}-1\right) = b^{a-n}C_a^n
\]
Et 

\begin{equation}
\boxed{(b+z)^a=\sum_{n=0}^\infty b^{a-n}C_a^n z^n}
\end{equation}
Et le cas particulier $b=1$:

\begin{equation}
\boxed{(1+z)^a=\sum_{n=0}^\infty C_a^n z^n}
\end{equation}


\subsubsection{Fonction racine:$\sqrt{1+z}$}
\label{dl_sqrtroot}
La fonction racine carrée est le cas particulier $b=1$ et $a=\frac{1}{2}$ de la relation précédente (voir \ref{dl_bin}):
\begin{equation}
\boxed{\sqrt{1+z}=\sum_{n=0}^\infty C_\frac{1}{2}^n z^n=1+\frac{z}{2}-\frac{z^2}{8}+\frac{z^3}{16}-\frac{5z^4}{128}+...+O(z^5)}
\end{equation}

\graphpartialhigh{sqrt}{$\sqrt{1+z}$}{T}

\subsubsection{Fonction inverse racine: $\frac{1}{\sqrt{1+z}}$}
\label{dl_invsqrtroot}
La fonction inverse racine carrée est le cas particulier $b=1$ et $a=-\frac{1}{2}$ de la relation précédente (voir \ref{dl_bin}):
\begin{equation}
\boxed{\frac{1}{\sqrt{1+z}}=\sum_{n=0}^\infty C_{-\frac{1}{2}}^n z^n=1-\frac{z}{2}+\frac{3z^2}{8}-\frac{5z^3}{16}+\frac{35z^4}{128}+...+O(z^5)}
\end{equation}

\subsubsection{Fonction arc tangente : $arctan(z)$}
\label{dl_arctan}
Pour trouver le développement de la fonction $arctan(z)$ on peut partir de la formule:
\[
	arctan(z)=\int_0^z \frac{1}{1+z^2}
\]
Or le développement de $\frac{1}{1+z^2}$, s'obtient facilement à partir de celui de $\frac{1}{1+z}=\sum_{n=0}^{\infty} (-1)^n z^n$, en remplaçant $z$ par $z^2$, on obtient :
\[
	arctan(z)=\int_0^z \sum_{n=0}^{\infty} (-1)^n z^{2n}
\]
Comme $arctan(0)=0$, on peut sans perte de généralité intégrer termes à termes, finalement :
\begin{equation}
	\boxed{arctan(z)=\sum_{n=0}^{\infty} (-1)^n \frac{z^{2n+1}}{2n+1}=z-\frac{z^3}{3}+\frac{z^5}{5}-\frac{z^7}{7}+...+O(z^9)}
\end{equation}
 


\subsubsection{Fonction arc-sinus : $arcsin(z)$}
\label{dl_arcsin}
Pour trouver le développement de la fonction $arcsin(z)$ on peut partir de la formule:
\[
	arcsin(z)=\int_0^z \frac{1}{\sqrt{1-z^2}}
\]
Or le développement de $ $, s'obtient à partir de $\sqrt{1+z}=\sum_{n=0}^\infty C_{-\frac{1}{2}}^n z^n$, en remplaçant $z$ par $-z^2$, on obtient:
\[
	arcsin(z)=\int_0^z \sum_{n=0}^\infty C_{-\frac{1}{2}}^n (-1)^n z^{2n}
\]
Comme $arcsin(0)=0$, on peut sans perte de généralité intégrer termes à termes, finalement :
\begin{equation}
	\boxed{arcsin(z)=\sum_{n=0}^\infty C_{-\frac{1}{2}}^n (-1)^n \frac{z^{2n+1}}{2n+1}}
\end{equation}

\subsubsection{Fonction arc-cosinus : $arccos(z)$}
\label{dl_arccos}
On déduit facilement à partir de l'arc-sinus :
\[
arccos(z)=\frac{\pi}{2}-\arcsin(z)
\]
Finalement
\begin{equation}
	\boxed{arccos(z)=\frac{\pi}{2}-\sum_{n=0}^\infty C_{-\frac{1}{2}}^n (-1)^n \frac{z^{2n+1}}{2n+1}}
\end{equation}

\subsubsection{Fonction d'erreur : $erf(z)$\workon}
\label{dl_erf}
On défini la fonction d'erreur par l'intégrale:

\[
erf(z)=\frac{1}{\sqrt{\pi}}\int_{-z}^{z} e^{-t^2} dt
\]

Soit si on utilise le développement de la fonction exponentielle \ref{dl_exponentiel}:


\begin{eqnarray}
erf(z)&=&\frac{1}{\sqrt{\pi}}\int_{-z}^{z} \sum_{n=1}^{\infty} \frac{(-1)^n t^{2n} }{n!} dt\\
&=&\frac{1}{\sqrt{\pi}} \sum_{n=1}^{\infty}  \frac{(-1)^n}{n!} \int_{-z}^{z} t^{2n}  dt\\
&=&\frac{1}{\sqrt{\pi}} \sum_{n=1}^{\infty}  \frac{(-1)^n}{n!} \left[ \frac{t^{2n+1}}{2n+1} \right]_{-z}^{z}\\
\end{eqnarray}

Finalement 

\begin{equation}
\boxed{
erf(z)=\frac{2}{\sqrt{\pi}} \sum_{n=1}^{\infty}  \frac{(-1)^n z^{2n+1}}{n!(2n+1)}}
\end{equation}

La fonction n'a pas de pôles le rayon de convergence est donc infini.

\subsubsection{Intégrales de Fresnel : C(z), S(z)}

Les intégrales de Fresnel sont les parties réelles et imaginaires de l'intégrale:

\[
I(z)=\int_0^{z} e^{iu^2}du \quad et \quad \left\{\begin{array}{c}C(z)=\Re(I(z))=\int_0^{z}cos(u^2)du\\S(z)=\Im(I(z))=\int_0^{z}sin(u^2)du\\ \end{array} \right.
\]

Si on applique le même raisonnement que pour la fonction erf(z):

\begin{eqnarray}
I(z)&=&\int_0^{z} \sum_{n=1}^{\infty} \frac{(iu^2)^n}{n!} du\\
&=& \sum_{n=1}^{\infty} \frac{ i^n }{n!} \int_0^{z} u^{2n} du\\
&=& \sum_{n=1}^{\infty} \frac{ i^n z^{2n+1}}{n!(2n+1)}\\
\end{eqnarray}

Du coup on obtient directement en prenant partie réelle et partie imaginaire:

\begin{equation}
\boxed{
C(z)= \sum_{n=1}^{\infty} \frac{ (-1)^n z^{4n+1}}{(2n)!(4n+1)} \quad\quad et \quad\quad S(z)= \sum_{n=1}^{\infty} \frac{ (-1)^n z^{4n+3}}{(2n+1)!(4n+3)}}
\end{equation}



\subsection{Exemples : Séries de Laurent}
\subsubsection{Fonction géométrique : $f(z)=\frac{1}{1-z}$}

On peut remarquer que 
\[
f(z)=\frac{1}{1-z}=-\frac{1}{z}\frac{1}{1-\frac{1}{z}}=-\frac{1}{z}f\left(\frac{1}{z}\right)
\]
Si on se rappelle de la série de Taylor associée à $f(z)$ qui converge pour $|z|<1$:
\[
f(z)=\sum_{n=0}^{\infty}z^n
\]
Par conséquent la série suivante converge si $|z|>1$
\[
f(z)=-\frac{1}{z}f\left(\frac{1}{z}\right)=-\frac{1}{z}\sum_{n=0}^{\infty}\frac{1}{z^n}
\]
Soit
\[
f(z)=-\sum_{n=0}^{\infty}\frac{1}{z^{n+1}}
\]
En changeant la borne
\[
\boxed{\frac{1}{1-z}=-\sum_{n=1}^{\infty}\frac{1}{z^n} \quad pour \quad |z|>1}
\]
Si on regarde la convergence des sommes partielles:
\graphpartialhigh{geom}{$\frac{1}{1-z}$}{L}

\subsection{Exemple complet : Séries de Taylor-Laurent}

Considérons la fonction :
\[
f(z)=\frac{1}{(1-z)(2-z)}
\]
Si on décompose en éléments simples :
\[
f(z)=\frac{A}{(1-z)}+\frac{B}{(2-z)} \quad avec \quad A=\lim_{z\to 1}\frac{1}{2-z}=1 \quad et \quad B=\lim_{z\to 2}\frac{1}{1-z}=-1
\]
Donc 
\[
f(z)=\frac{1}{(1-z)(2-z)}=\frac{1}{1-z}-\frac{1}{2-z}
\]
Soit
\[
f(z)=\frac{1}{(1-z)(2-z)}=\frac{1}{1-z}-\frac{1}{2}\frac{1}{1-\frac{z}{2}}
\]

\begin{itemize}

\item Pour \textbf{$\boxed{|z|<1}$} on a : Si on prend le développement de Taylor sur mes deux termes, on obtient:
\[
	f(z)=\sum_{n=0}^{\infty}z^n - \frac{1}{2} \sum_{n=0}^{\infty} \left(\frac{z}{2}\right) ^n
\]
Le premier est valide pour $|z|<1$ et le second pour $|\frac{z}{2}|<1$ soit $|z|<2$ donc la somme est valide pour $|z|<1$ et si on rassemble les termes , on obtient:

\[
	\boxed{f_1(z)=\sum_{n=0}^{\infty}\left(1-\frac{1}{2^{n+1}} \right) z^n \quad pour \quad |z|<1}
\]

\item Pour \textbf{$\boxed{|z|>2}$} on a : Si on prend le développement de Laurent sur mes deux termes, on obtient:
\[
f(z)=-\sum_{n=1}^{\infty}\frac{1}{z^n}+\frac{1}{2}\sum_{n=1}^{\infty}\frac{1}{\left(\frac{z}{2}\right)^n}
\]
Le premier est valide pour $|z|>1$ et le second pour $|\frac{z}{2}|>1$ soit $|z|>2$ donc la somme est valide pour $|z|>2$ et si on rassemble les termes , on obtient:

\[
f(z)=\sum_{n=1}^{\infty}\frac{2^{n-1}}{z^n}-\sum_{n=1}^{\infty}\frac{1}{z^n}
\]
\[
\boxed{f_2(z)=\sum_{n=1}^{\infty} \left( 2^{n-1}-1 \right) \frac{1}{z^n} \quad pour\quad |z|>2 }
\]

\item Pour \textbf{$\boxed{1<|z|<2}$} on a : Si on prend le développement de Laurent pour le premier terme et le développement de Taylor pour le second, on obtient:

\[
f(z)=-\sum_{n=1}^{\infty}\frac{1}{z^n}- \frac{1}{2} \sum_{n=0}^{\infty} \left(\frac{z}{2}\right)^n
\]
Si on change les bornes du premier terme:
\[
f(z)=-\sum_{n=-\infty}^{-1}z^n- \sum_{n=0}^{\infty} 2^{-n-1} z^n
\]

\[
\boxed{f_3(z)=\sum_{n=-\infty}^{\infty} A_n z^n \quad A_n=\left\{\begin{array}{lcc}
-1 & si & n<0\\
-2^{-(n+1)} & si & n\ge 0\\
\end{array}
\right.
\quad avec \quad 1<|z|<2
}
\]

\graphtriplefunction{T_poles_1_2_64}{$f_1$}{L_poles_1_2_64}{$f_2$}{TL_poles_1_2_64}{$f_3$}

\end{itemize}

\newpage
\newpage
\section{Propriétés des fonctions Analytiques}
\subsection{Prolongement Analytique}
\label{prolongement_analytique}
\subsubsection{Énoncé}

Le \textbf{prolongement analytique} d'une fonction est une notion très importante et puissante de l'analyse complexe. 

Énonçons tout d'abord le théorème :
\begin{center}
\fbox{\begin{minipage}{0.9\textwidth}
Soit $f(s)$ une fonction analytique sur $U$, et $g(s)$ une fonction analytique sur $V$ tel que $U \subset V$. Si $g(s)=f(s)$ sur $U$. Alors $g(s)$ constitue l'unique prolongement analytique de $f(s)$ sur $V$.
\end{minipage}}
\end{center}

En d'autre mots, il est possible d'étendre le domaine de définition d'une fonction analytique de manière unique.

\subsubsection{Démonstration}
Soit $f(s)$ définie sur $U$ et $g(s)$ et $h(s)$ définies sur $V$ tel que $U\in V$ et si $f(s)=g(s)=h(s)$ sur $U$ alors la fonction définie par $e(s)=g(s)-h(s)$ est holomorphe et $e(s)=0$ sur $U$. Il s'en suit nécessairement que $e(s)=0$ sur $V$,(\textbf{sinon il y aurait discontinuité et la fonction ne serait plus dérivable}) autrement dit que $g(s)=h(s)$ sur V. Autrement dit si on trouve deux fonctions $g$ et $h$ qui prolongent une fonction $f$ alors nécessairement, $g=h$. Le prolongement d'une fonction holomorphe est unique.

\subsubsection{Exemple : La fonction géométrique}

Considérons le développement en série de la fonction géométrique (voir \ref{eq_dl_geometric}):

\[
Soit\quad\quad g(s)=\frac{1}{1-s} \quad\quad\quad et\quad\quad\quad f(s)=\sum_{n=0}^{\infty} s^n \quad si \quad |s|<1
\]

La série $f(s)$ est définie sur le domaine $U$, le disque centré de rayon $1$. La fonction génératrice $g(s)$ est définie sur le domaine $V$, le plan complexe privé de $1$. Mais $U \subset V$ de plus $g(s)=f(s)$ sur $U$. Donc $g(s)$ constitue l'unique prolongement analytique de $f(s)$ sur $V$.\\

Il est souvent difficile voir impossible de trouver une forme clause, mais cela ne n'empêche pas de pouvoir prolonger la fonction. Bien que le développement soit local, au sens ou il ne converge que si $|s|<1$, \textbf{il décrit en fait la fonction dans son ensemble}. En fait on peut calculer le développement en série de Taylor en un autre point, à partir de $f(s)$. et ainsi obtenir un nouveau développement $f_1(s)$ valide sur un autre domaine. \textbf{On peut ainsi prolonger la fonction de proche en proche et choisir une trajectoire de manière à contourner le pôle en 1 et construire une suite de fonctions $f_n(s)$ toutes égales sur le recouvrement de leurs domaines différents}.\\

\begin{multicols}{2}

\begin{figure}[H]
\centering
\begin{center}
\begin{tikzpicture}[scale=1]
\fill[fill=gray!20] (0,0) circle (1) ;
\fill[fill=gray!40] (0.5,0.5) circle (0.7071) ;
\TikzGraphC{-2}{-2}{2}{2}
\draw (0,0) circle (1) ;
\draw (0.5,0.5) circle (0.7071) ;
\draw (0,0) node {$\bullet$} ; 
\draw (0,0) node [above left] {$a_0$} ;
\draw (0.5,0.5) node {$\bullet$} ; 
\draw (0.5,0.5) node [above right] {$a_1$} ;
\draw[color=red] (1,0) node {$\bullet$} ; 
\draw[color=red] (1,0) node [below right] {$1$} ; 
\end{tikzpicture}
\end{center}
\caption{Schéma : Prolongement analytique de $f(s)$}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.20]{./img/cpp/geom.png}
\includegraphics[scale=0.20]{./img/cpp/geomT_64.png}
\includegraphics[scale=0.20]{./img/cpp/TT_geomertic_ac_40.png}
\caption{Graphique de $g(s)$, $f(s)$ puis $f_1(s)$}
\end{figure}

\columnbreak
Par exemple, on peut calculer le développement en série de $f(s)$ en $a_1=\frac{1}{2}+i\frac{1}{2}$ ceci est possible car $|a_1|=\frac{1}{\sqrt{2}}$ donc $a_1\in U$. Notons ce nouveau développement en $a_1$, $f_1(s)$, on peut le calculer en fonction de $f(s)$ par :
\[
f_1(s)=f(a_1)+\frac{f'(a_1)}{1!}(s-a_1)+\frac{f^{(2)}(a_1)}{2!}(s-a1)^2+...
\]
Les dérivées s'expriment par :
\[
f^{(m)}(s)=\sum_{n=m}^{\infty}\left(\prod_{k=0}^{m-1}(n-k)\right) s^{n-m}
\]

Nous avons ainsi "déplacer" le domaine de convergence de la suite on peut désormais \textbf{atteindre de nouvelles valeurs}. Mais nous n'avons utilisé que la connaissance que nous avions sur $f(s)$. Ceci signifie que l'information sur la fonction sous-jacente est entièrement codé dans la série.

\end{multicols}

\subsection{Symétrie par rapport à la conjugaison : $f(\bar{z})=\overline{f(z)}$\verif}
\label{eq_analytique_sym}

Soit $f$ une fonction holomorphe, donc $f$ est analytique (voir \ref{eq_taylor_dem}). Si les coefficients $C_n$ de son expansion en série de Taylor sont réels, c'est à dire que on a $\boxed{ C_n=\overline{C_n} }$, alors $f(z)$ est symétrique par rapport à la conjugaison complexe, c'est à dire que on a :

\begin{equation}
\boxed{f(\bar{z})=\overline{f(z)}}\quad\quad ssi\quad\quad C_n\in\mathbb{R}
\end{equation}

En effet si $f(z)$ est analytique, alors on a :

\[
f(z)=\sum_{n=0}^\infty C_n z^n
\]

Donc trivialement, par propriété sur la conjugaison d'un produit et d'une somme il s'en suit que :

\[
f(\bar{z})=\sum_{n=0}^\infty C_n \bar{z}^n=\sum_{n=0}^\infty C_n \overline{z^n}=\sum_{n=0}^\infty \overline{C_n} \overline{z^n}=\overline{\sum_{n=0}^\infty C_n z^n}=\overline{f(z)}
\]

Ce qui démontre la proposition.

\newpage
\section{Théorème des résidus }
\label{eq_residus}
\subsection{Énoncé}

\begin{tabular}{cc}
\begin{minipage}{12cm}
Soit $f(z)$, une fonction complexe analytique sur $U$, et $C=\delta U$ un contour fermé orienté dans le sens trigonométrique, englobant $P$ pôles isolés $\rho_i$ de $f(z)$, alors l'intégrale selon ce contour vaut:
\begin{equation}
\boxed{\oint_{C} f(z)dz=(2i\pi) \sum_{i=0}^{P} Res(f,\rho_i)}
\end{equation}

Le théorème des résidus généralise la formule intégrale de Cauchy \ref{eq_cauchy_int}, et fournit un nouveau point de vue, en introduisant la notion de \textbf{résidus} que nous allons développer ici.

\end{minipage}
&
\begin{minipage}{5cm}
\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1]
\TikzGraphC{-2}{-2}{2}{2}
\draw (-0.3,-0.4) node {$\bullet$} node [above left] {$\rho_0$};
\draw (0.6,0.4) node {$\bullet$} node [above left] {$\rho_1$};
\draw (-0.2,0.4) node {$\bullet$} node [above left] {$\rho_2$};
\draw (0.4,-1) node {$\bullet$} node [above left] {$\rho_3$};
\draw [color=red](-0.9,-0.4) node [below left] {$C$};
\draw [color=red,thick,rotate around={45:(0,0)}] (0,0) ellipse (1cm and 2cm);
\draw [->,thick] (-0.71,-0.69) -- (-0.7,-0.7);
\draw [->,thick] (0.71,0.69) -- (0.7,0.7);
\end{tikzpicture}
\caption{Schéma : Théorème des résidus}
\end{figure}
\end{minipage}
\\
\end{tabular}



\subsection{Résidus}
\label{eq_residus_def}
Les résidus d'une fonction sont en lien étroit avec son développement en série de Laurent (voir \ref{eq_taylor_laurent}) autour du pôle considéré. En effet, le résidu \textbf{pour un pôle simple} s'exprime par:

\begin{equation}
\boxed{Res(f,\rho_i)=\lim_{z\to \rho_i}(z-\rho_i)f(z)}
\end{equation}

Comme les pôles sont isolés, $f(z)$ admet un développement en série de Taylor-Laurent (voir \ref{eq_taylor_laurent}) centré en chacun d'eux:

\[
f(z)=\sum_{-\infty}^{\infty} C_n (z-\rho_i)^n
\]
Par conséquent le résidu en $\rho_i$ est simplement le terme $C_{-1}$, car:
\[
Res(f,\rho_i)=\lim_{z\to \rho_i}(z-\rho_i)f(z)=\lim_{z\to \rho_i}\sum_{-\infty}^{\infty} C_n (z-\rho_i)^{n+1}
\]
\[
\boxed{Res(f,\rho_i)=C_{-1}}
\]

Le résidu \textbf{pour un pôle d'ordre $k$} à une expression plus complexe qui nécessite de dériver $k-1$ fois la fonction:
\begin{equation}
\boxed{Res(f,\rho_i)=\frac{1}{(k-1)!}\lim_{z\to \rho_i} ((z-\rho_i)^k f(z))^{(k-1)}}
\end{equation}
Généralement on s'évite d'avoir à calculer se genre de résidu, en déformant le contour d'intégration.

\[
Res(f,\rho_i)=\frac{1}{(k-1)!}\lim_{z\to \rho_i} \left( \sum_{-\infty}^{\infty} C_n (z-\rho_i)^{n+k}\right)^{(k-1)}=\frac{1}{(k-1)!}\lim_{z\to \rho_i} \sum_{-\infty}^{\infty} (n+k)(n+k-1)...(n+2) C_n (z-\rho_i)^{n+1} 
\]
Le seul terme non nul apparait alors pour $n=-1$ cela donne bien :
\[
\boxed{Res(f,\rho_i)=C_{-1}}
\]

\subsection{Démonstrations}

\subsubsection{Cas d'un pôle simple isolé}
Partons de la formule intégrale de Cauchy (voir \ref{eq_cauchy_int}):
\[
f(a)=\frac{1}{2i\pi}\oint_\gamma \frac{f(z)}{(z-a)} dz
\]
On rappel que $\gamma$ est un chemin contenant $a$ et inclus dans le domaine d'analycité de $f$. Considérons $g(z)=\frac{f(z)}{(z-a)}$ c'est à dire une fonction ayant un pôle en $a$ de résidu $f(a)$, on a:
\[
\oint_\gamma g(z) dz = 2i\pi f(a)
\]
\[
\oint_\gamma g(z) dz = 2i\pi \lim_{z\to a} (z-a) g(z)
\]
Finalement avec la définition du résidu d'un pôle simple (voir \ref{eq_residus_def}), on retrouve bien:
\[
\boxed{\oint_\gamma g(z) dz = 2i\pi Res(g,a)}
\]
Nous avons ici démontré le théorème pour une fonction possédant un seul pôle simple en $a$.

\subsubsection{Cas d'un pôle d'ordre $k$ isolé}
Partons de la formule intégrale généralisée de Cauchy (voir \ref{eq_cauchy_deriv}):
\[
f^{(n)}(a)= \frac{n!}{2i\pi}\oint_\gamma \frac{f(z)}{(z-a)^{n+1}} dz
\]
On rappel que $\gamma$ est un chemin contenant $a$ et inclus dans le domaine d'analycité de $f$. Considérons $g(z)=\frac{f(z)}{(z-a)^{k}}$ c'est à dire une fonction ayant un pôle d'ordre $k$ en $a$ de résidu $f(a)$:
\[
\oint_\gamma g(z) dz = \frac{2i\pi}{(k-1)!} f(a)^{(k-1)}
\]
\[
\oint_\gamma g(z) dz = \frac{2i\pi}{(k-1)!} \left(\lim_{z\to a}(z-a)^k g(z)\right)^{(k-1)}
\]
Finalement avec la définition du résidu d'un pôle d'ordre $k$ (voir \ref{eq_residus_def}), on retrouve bien:
\[
\boxed{\oint_\gamma g(z) dz = 2i\pi Res(g,a)}
\]
Nous avons ici démontré le théorème pour une fonction possédant un seul pôle d'ordre $k$ en $a$.

\subsubsection{Cas général}

Dans le cas de pôles multiples, d'après le \textbf{théorème intégral de Cauchy} (voir \ref{eq_cauchy_int} ), on peut alors découper le chemin de manière à se retrouver avec une somme d'intégrales d'englobant que des pôles isolés:

\[
\oint_{C} g(z)dz=\sum_{i=0}^{P} \int_{\gamma_i} g(z)dz
\]

Avec $\gamma_i$ des cercles de rayon $\epsilon$ aussi petit que l'on veut et centrés en $\rho_i$, schématiquement on obtient:

\begin{figure}[H]
\centering
\begin{tabular}{cc}
\begin{tikzpicture}[scale=1]
\TikzGraphC{-2}{-2}{2}{2}
\draw (-0.3,-0.4) node {$\bullet$} node [above left] {$\rho_0$};
\draw (0.6,0.4) node {$\bullet$} node [above left] {$\rho_1$};
\draw (-0.2,0.4) node {$\bullet$} node [above left] {$\rho_2$};
\draw (0.4,-1) node {$\bullet$} node [above left] {$\rho_3$};
\draw [color=red](-0.9,-0.4) node [below left] {$C$};
\draw [color=red,thick,rotate around={45:(0,0)}] (0,0) ellipse (1cm and 2cm);
\draw [->,thick] (-0.71,-0.69) -- (-0.7,-0.7);
\draw [->,thick] (0.71,0.69) -- (0.7,0.7);
\end{tikzpicture}
\begin{tikzpicture}[scale=1]
\TikzGraphC{-2}{-2}{2}{2}
\CirPole{-0.3}{-0.4}{0.25}
\CirPole{0.6}{0.4}{0.25}
\CirPole{-0.2}{0.4}{0.25}
\CirPole{0.4}{-1}{0.25}
\draw (-0.3,-0.4) node [above left] {$\rho_0$};
\draw (0.6,0.4) node [above left] {$\rho_1$};
\draw (-0.2,0.4) node [above left] {$\rho_2$};
\draw (0.4,-1) node [above left] {$\rho_3$};
\end{tikzpicture}\\
\end{tabular}
\caption{Schéma : Théorème des résidus}
\end{figure} 


Ce qui démontre alors la formule générale:

\[
\boxed{\oint_{C} g(z)dz=(2i\pi) \sum_{i=0}^{P} Res(g,\rho_i)}
\]

\subsection{Exemples de calcul d'intégrales}
\subsubsection{L'intégrale : $\int_{-\infty}^{\infty} \frac{e^{i t z}}{z^2+1}$}

On va calculer
\[
\int_{-\infty}^{\infty} f(z)dz\quad avec \quad f(z)=\frac{e^{i t z}}{z^2+1}
\]
On a :
\[
f(z)=\frac{e^{i t z}}{(z-i)(z+i)}
\]
Cette fonction a deux pôles simples $i$ et $-i$, et on peut calculer le résidu en $i$ par:

\[
Res(f,i)=\lim_{z=i}(z-i)f(z)=\frac{e^{i t z}}{(z+i)}=\frac{e^{-t}}{2i}
\]

Si pour le chemin $C$ on choisit le demi-cercle qui enveloppe le pôle $i$:

\begin{center}
\begin{tikzpicture}[scale=1]
\TikzGraphC{-2}{-1}{2}{2}
\TikzPathD{1.5}{$a$}
\draw (0,1) node {$\bullet$} node [below left] {$i$};
\draw [color=red] (-1,1) node [above left] {$C$};
\end{tikzpicture}
\end{center}

Le théorème des résidus donne alors:
\[
\oint_{C} f(z)dz=(2\pi i) Res(f,i)=(2\pi i) \frac{e^{-t}}{2i}=\pi e^{-t}
\]
Donc on peut décomposer l'intégrale en deux chemins:

\[
\oint_{C} f(z)dz=\int_{z=-a}^{a} f(z)dz+\int_{\phi=0}^{\pi} f(a e^{i\phi})d\phi=\pi e^{-t}
\]

Mais ici on peut voir que la seconde intégrale tend vers zero en module lorsque $a\to\infty$, car :

\[
|\int_{\phi=0}^{\pi} f(a e^{i\phi})d\phi|< \int_{\phi=0}^{\pi}|f(a e^{i\phi})|d\phi < \int_{\phi=0}^{\pi}\frac{|e^{i t z}|}{|z^2+1|}d\phi < \int_{\phi=0}^{\pi}\frac{1}{|a^2e^{i2\phi}+1|}d\phi<0
\]
Finalement on obtient lorsque $a\to\infty$

\[
\boxed{\int_{-\infty}^{\infty} \frac{e^{i t z}}{z^2+1} dz = \pi e^{-t}}
\]
\subsubsection{Intégrale de Fresnel : $\int_{-\infty}^{\infty} \frac{sin(b z)}{z} dz$ }

Calculons par les résidus cette intégrale de Fresnel célèbre, qui se calcul également avec les formules d'intégrales d'Euler (voir \ref{eq_gamma_fresnel_int}). Remarquons que l'intégrante est bien définie en zéro la limite existe et vaut $b$. Maintenant on peut dire que cette intégrale correspond en fait à la partie imaginaire de l'intégrale d'une exponentielle, soit :

\[
	\int_{-\infty}^{\infty} \frac{sin(b z)}{z} dz= Im \left( \VP\int_{-\infty}^{\infty} \frac{e^{i b z}}{z} dz \right)
\]

Cependant il faut faire particulièrement attention car l'intégrante de droite est maintenant singulière en zéro et l'intégrale est à prendre au sens de la valeur principale. C'est à dire en fait :

\[
	\int_{-\infty}^{\infty} \frac{sin(b z)}{z} dz= Im \left( \lim_{\epsilon\to 0}\int_{-\infty}^{-\epsilon} \frac{e^{i b z}}{z} dz+\int_{\epsilon}^{\infty} \frac{e^{i b z}}{z} dz \right)
\]

Maintenant choisissons le contour $C$ de manière à contourner cette singularité en zéro tel que :

\begin{center}
\begin{center}
\begin{tikzpicture}[scale=1]
\TikzGraphC{-2}{-1}{2}{2}
\TikzPathC{1.5}{0.3}{$0$}{$C_1$}{$C_2$}{$C_3$}{$C_4$}{$R$}{$\epsilon$}
\end{tikzpicture}
\end{center}
\end{center}

Alors, le contour se décompose en 4 chemins:
\[
\oint_C \frac{e^{i b z}}{z} dz=\oint_{C_1} \frac{e^{i b z}}{z} dz+\oint_{C_2} \frac{e^{i b z}}{z} dz+\oint_{C_3} \frac{e^{i b z}}{z} dz+\oint_{C_4} \frac{e^{i b z}}{z} dz
\]
Et de plus d'après l'intégrale de cauchy (voir \ref{eq_cauchy_int}), comme l'intégrante est analytique sur C, sont intégrale est nulle donc $\oint_C \frac{e^{i b z}}{z} dz=0$, et on obtient pour le reste:

\[
\int_{0}^{\pi} \frac{e^{i b Re^{i\theta}}}{Re^{i\theta}} iRe^{i\theta}d\theta+\int_{\pi}^{0} \frac{e^{i b \epsilon e^{i\theta}}}{\epsilon e^{i\theta}} i \epsilon e^{i\theta}d\theta+\int_{-R}^{-\epsilon} \frac{e^{i b z}}{z} dz+\int_{\epsilon}^{R} \frac{e^{i b z}}{z} dz=0
\]

Lorsque $R\to\infty$ et $\epsilon\to 0$, a droite on reconnait la valeur principale de l'intégrale : 

\[
i\int_{0}^{\pi} e^{i b Re^{i\theta}} d\theta+i\int_{\pi}^{0} e^{i b \epsilon e^{i\theta}} d\theta+ VP \int_{-\infty}^{\infty} \frac{e^{i b z}}{z} dz=0
\]

Comme $\Im(Re^{i\theta})>0$ si $b>0$ alors la première intégrale tend vers zéro quand $R\to\infty$, il reste :

\[
VP \int_{-\infty}^{\infty} \frac{e^{i b z}}{z} dz=-\lim_{\epsilon\to 0} i\int_{\pi}^{0} e^{i b \epsilon e^{i\theta}} d\theta
\]

\[
VP \int_{-\infty}^{\infty} \frac{e^{i b z}}{z} dz= i\int_{\pi}^{0} d\theta=-(-i\pi)
\]

Finalement 
\[
VP \int_{-\infty}^{\infty} \frac{e^{i b z}}{z} dz=i\pi
\]
Et si on reprend la partie imaginaire, l'intégrale redevient une intégrale au sens ordinaire et on obtient:

\[
\boxed{\int_{-\infty}^{\infty} \frac{sin(b z)}{z} dz=\pi \quad avec \quad b>0}
\]

\subsubsection{Intégrales de Fourier : $\int_{-\infty}^{\infty}f(t)e^{-i s t} dt$ \todo}

Considérons une Transformée de Fourier (voir \ref{eq_fourier_def}) d'une fonction $f(t)$:

\[
f(s)=\int_{-\infty}^{\infty}f(t)e^{-i s t} dt
\]

\subsection{Exemples de calcul de sommes}

\subsubsection{Somme : $\sum_{n=1}^{\infty} \frac{1}{n^2+a^2}$}
Calculons la somme :
\[
	S(a)=\sum_{n=1}^{\infty} \frac{1}{n^2+a^2}
\]
D'abord on peut remarquer que la fonction sommée est paire sur l'indice donc :
\[
	S(a)=\frac{1}{2}\sum_{n=-\infty \ne 0}^{\infty} \frac{1}{n^2+a^2}
\]

Considérons la fonction $g(z)=\pi cot (\pi z) = \pi \frac{cos(\pi z)}{sin(\pi z)}$, cette fonction possède la propriété intéressante d'avoir un pôle de résidu 1 pour chaque entier $n$, en effet:

\[
	Res(g,n)=\lim_{z\to n} (z-n) \pi \frac{cos(\pi z)}{sin(\pi z)} = (z-n) \pi \frac{ (-1)^n }{(z-n) \pi (-1)^n}=1
\]

D'après le théorème des résidus, on à alors 

\[
	\oint_{C_n} \frac{g(z)}{z^2+a^2} = (2i\pi) \frac{1}{n^2+a^2}
\]
On poser $f(z)=\frac{g(z)}{z^2+a^2}$, de tel sorte que :

\[
	\oint_{C_n} f(z) = (2i\pi) \frac{1}{n^2+a^2}
\]

Ou $C_n$ est un cercle de rayon $\epsilon \to 0$ centré en $n$. La somme peut alors s'écrire :

\[
S(a)=\frac{1}{2}\sum_{n=-\infty\ne 0}^{\infty} \frac{1}{2i\pi} \oint_{C_n} \frac{g(z)}{z^2+a^2}
\]

Mais en fait on peut très astucieusement transformer la somme de contours en un seul contour qui entoure lui seulement les trois pôles en $\pm ia$ et zéro. Visuellement la transformation prend la forme :

\begin{tabular}{cccc}
\begin{tikzpicture}[scale=0.5]
\TikzGraphC{-4}{-4}{4}{4}
\draw (0,2.3) node {$\bullet$} node [above right] {$+ ia$};
\draw (0,0) node {$\bullet$} node [above right] {$0$};
\draw (0,-2.3) node {$\bullet$} node [below right] {$-ia$};
\CirPole{-1}{0}{0.3}
\CirPole{-2}{0}{0.3}
\CirPole{-3}{0}{0.3}
\CirPole{ 1}{0}{0.3}
\CirPole{ 2}{0}{0.3}
\CirPole{ 3}{0}{0.3}
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.5]
\TikzGraphC{-4}{-4}{4}{4}
\draw (0,2.3) node {$\bullet$} node [above right] {$+ ia$};
\draw (0,0) node {$\bullet$} node [above right] {$0$};
\draw (0,-2.3) node {$\bullet$} node [below right] {$-ia$};
\draw (1,0) node {$\bullet$};
\draw (2,0) node {$\bullet$};
\draw (3,0) node {$\bullet$};
\draw (-1,0) node {$\bullet$};
\draw (-2,0) node {$\bullet$};
\draw (-3,0) node {$\bullet$};
\draw [red,thick,<-,>=stealth](3.5,0.3) arc (0:180:3.5);
\draw [red,thick,<-,>=stealth](-3.5,-0.3) arc (180:360:3.5);
\draw [red,thick,->,>=stealth] (3.5,0.3) -- (0.7,0.3);
\draw [red,thick,<-,>=stealth] (3.5,-0.3) -- (0.7,-0.3);
\draw [red,thick,<-,>=stealth] (-3.5,0.3) -- (-0.7,0.3);
\draw [red,thick,->,>=stealth] (-3.5,-0.3) -- (-0.7,-0.3);
\draw [red,thick,->,>=stealth](-0.7,-0.3) arc (270:450:0.3);
\draw [red,thick,->,>=stealth]( 0.7, 0.3) arc (90:270:0.3);
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.5]
\TikzGraphC{-4}{-4}{4}{4}
\draw (0,2.3) node {$\bullet$} node [above right] {$+ ia$};
\draw (0,0) node {$\bullet$} node [above right] {$0$};
\draw (0,-2.3) node {$\bullet$} node [below right] {$-ia$};
\draw (1,0) node {$\bullet$};
\draw (2,0) node {$\bullet$};
\draw (3,0) node {$\bullet$};
\draw (-1,0) node {$\bullet$};
\draw (-2,0) node {$\bullet$};
\draw (-3,0) node {$\bullet$};
\draw [red,thick,->,>=stealth] (-0.3,-3) -- (-0.3,3);
\draw [red,thick,<-,>=stealth] (0.3,-3) -- (0.3,3);
\draw [red,thick,<-,>=stealth] (-0.3,-3) arc (180:360:0.3);
\draw [red,thick,<-,>=stealth] (0.3,3) arc (0:180:0.3);
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.5]
\TikzGraphC{-4}{-4}{4}{4}
\draw (0,2.3) node {$\bullet$} node [above right] {$+ ia$};
\draw (0,0) node {$\bullet$} node [above right] {$0$};
\draw (0,-2.3) node {$\bullet$} node [below right] {$-ia$};
\draw (1,0) node {$\bullet$};
\draw (2,0) node {$\bullet$};
\draw (3,0) node {$\bullet$};
\draw (-1,0) node {$\bullet$};
\draw (-2,0) node {$\bullet$};
\draw (-3,0) node {$\bullet$};
\CirPoleB{0}{2.3}{0.3}
\CirPoleB{0}{0}{0.3}
\CirPoleB{0}{-2.3}{0.3}
\end{tikzpicture}
\\
\end{tabular}

Ainsi, on peut simplement écrire:

\[
S(a)=\frac{1}{2} \frac{-2i\pi}{2i\pi} \left( Res(f,ia)+Res(f,-ia)+Res(f,0)\right)
\]
On notera le signe moins car on tourne dans l'autre sens. Et on n'a plus qu'a calculer trois résidus, magique non !, allons y :

\[
S(a)=-\frac{1}{2}\left( \lim_{z\to ia} (z-ia)\frac{\pi cot(\pi z) }{(z-ia)(z+ia)} + \lim_{z\to -ia} (z+ia)\frac{\pi cot(\pi z) }{(z-ia)(z+ia)} +\lim_{z\to 0} (z)\frac{\pi cot(\pi z) }{z^2+a^2}\right)
\]

Le troisième résidu est triviale car $a>0$ donc le pôle en zéro est d'ordre $1$ et de résidu $\frac{1}{a^2}$ comme on l'a montré.

\[
S(a)=-\frac{1}{2}\left( \lim_{z\to ia} \frac{\pi cot(\pi z) }{z+ia} + \lim_{z\to -ia}\frac{\pi cot(\pi z) }{z-ia} + \frac{1}{a^2}\right)
\]

\[
S(a)=-\frac{1}{2}\left( \frac{\pi cot(\pi ia) }{2ia} + \frac{\pi cot(-\pi ia) }{-2ia} + \frac{1}{a^2}\right)
\]

Comme $cot(-z)=-cot(z)$

\[
S(a)=-\frac{1}{2}\left( \frac{\pi cot(\pi ia) }{ia} + \frac{1}{a^2}\right)
\]

Et Comme $coth(z)=-\frac{cot(iz)}{i}$, on peut aussi écrire

\[
\boxed{\sum_{n=1}^{\infty} \frac{1}{n^2+a^2}=\frac{\pi}{2a}\left(  coth(\pi a)  - \frac{1}{a\pi}\right)}
\]

Et si on prend la limite lorsque $a\to 0$, on retrouve bien la somme du problème de Bâle $\frac{\pi^2}{6}$.

\[
\sum_{n=1}^{\infty} \frac{1}{n^2}=\lim_{a\to 0} \frac{\pi}{2a}\left(  coth(\pi a)  - \frac{1}{a\pi}\right)
\]

Mais $coth(z)\approx \frac{1}{z}+\frac{z}{3}+...$ (voir \ref{dl_coth}) lorsque $z\to 0$, donc 

\[
\sum_{n=1}^{\infty} \frac{1}{n^2}=\lim_{a\to 0} \frac{\pi}{2a}\left(  \frac{a\pi}{3}\right)
\]

Finalement
\[
\boxed{\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}}
\]

\subsection{Exemples de calculs de fonctions génératrices}

\subsubsection{Génératrice des nombres de Fibonacci}
\label{eq_residus_fib}
On cherche une fonction génératrice de la forme :

\[
f(z)=\sum_{n=0}^{\infty}F_n z^z
\]

Avec $F_n$ les nombres de Fibonacci définis par l'équation de récurrence:

\[
F_{n+2}=F_{n+1}+F_{n} \quad avec \quad F_{0}=0 \quad et\quad F_{1}=1
\]
Ce qui donne :
\[
F_{n+2}-F_{n+1}-F_{n}=0
\]
Si on multiplie par $z^n$ et que on somme pour tout $n$ cela donne :
\[
\sum_{n=0}^{\infty}\left(F_{n+2}-F_{n+1}-F_{n}\right)z^n=0
\]
Si on développe :
\[
\sum_{n=0}^{\infty}F_{n+2}z^n-\sum_{n=0}^{\infty}F_{n+1}z^n-\sum_{n=0}^{\infty}F_{n}z^n=0
\]
Mais on peut décaler les indices des deux premières sommes:
\[
\sum_{n=2}^{\infty}F_{n}z^{n-2}-\sum_{n=1}^{\infty}F_{n}z^{n-1}-\sum_{n=0}^{\infty}F_{n}z^n=0
\]
Soit
\[
\frac{1}{z^2}\sum_{n=2}^{\infty}F_{n}z^{n}-\frac{1}{z}\sum_{n=1}^{\infty}F_{n}z^{n}-\sum_{n=0}^{\infty}F_{n}z^n=0
\]
On remarque alors que $f(z)$ apparait trois fois:
\[
\frac{1}{z^2}(f(z)-F_0-z F_1)-\frac{1}{z}(f(z)-F_0)-f(z)=0
\]
Si on multiplie par $z^2$:
\[
f(z)-F_0-z F_1-z(f(z)-F_0)-z^2 f(z)=0
\]

Si on développe et que on factorise par $f(z)$ :

\[
f(z)(1-z-z^2)=F_0+z(F_1-F_0)
\]

Soit 
\[
f(z)=\frac{F_0+z(F_1-F_0)}{1-z-z^2}
\]

Avec $F_{0}=0 \quad et\quad F_{1}=1$ cela donne :
\[
f(z)=\frac{z}{1-z-z^2}
\]
Il ne reste plus qu'a trouver le développement en série de Taylor de cette fonction et nous auront une expression de \textbf{la forme clause des nombres de Fibonacci}. D'abord remarquons que $f(z)$ peut s'écrire:

\[
f(z)=\frac{-z}{(z-\alpha)(z-\beta)}
\]

Avec $\alpha=\frac{1+\sqrt{5}}{-2}$ et $\beta=\frac{1-\sqrt{5}}{-2}$

Les d'après leurs définitions on peut exprimer les coefficients par l'intégrale (voir \ref{eq_taylor}):

\[
F_n=\frac{1}{2i\pi}\oint_C \frac{f(z)}{z^{n+1}}dz
\]
\[
F_n=\frac{1}{2i\pi}\oint_C \frac{1}{z^{n+1}}\frac{-z}{(z-\alpha)(z-\beta)}dz
\]

Avec $C$ un cercle centré autour de l'origine, n'englobant pas les singularités $\alpha$ et $\beta$. Mais le résidu en zéro est difficile à calculer car il est d'ordre $n+1$. En fait on peut déformer le contour pour englober à la place les deux singularités $\alpha$ et $\beta$ avec les chemin $C_1$ et $C_2$: 

\begin{center}
\begin{tabular}{cc}
\begin{tikzpicture}[scale=0.75]
\TikzGraphC{-2}{-2}{2}{2}
\draw (-1.61,0) node {$\bullet$} node [above right] {$\alpha$};
\draw (0,0) node [above right] {$0$};
\draw (0.61,0) node {$\bullet$} node [below right] {$\beta$};
\CirPole{0}{0}{0.3}
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.75]
\TikzGraphC{-2}{-2}{2}{2}
\draw (-1.61,0) node [above right] {$\alpha$};
\draw (0,0) node {$\bullet$} node [above right] {$0$};
\draw (0.61,0) node [below right] {$\beta$};
\CirPoleB{-1.61}{0}{0.3}
\CirPoleB{0.61}{0}{0.3}
\end{tikzpicture}
\\
\end{tabular}
\end{center}

L'intégrale devient alors:

\[
F_n=\frac{1}{2i\pi}\left(\oint_{C_1} \frac{1}{z^{n+1}}\frac{-z}{(z-\alpha)(z-\beta)}dz+\oint_{C_2} \frac{1}{z^{n+1}}\frac{-z}{(z-\alpha)(z-\beta)}dz\right)
\]

Le théorème des résidus donne alors:

\[
F_n=\frac{-2i\pi}{2i\pi}\left( Res\left(\frac{1}{z^{n+1}}\frac{-z}{(z-\alpha)(z-\beta)},\alpha\right)+Res\left(\frac{1}{z^{n+1}}\frac{-z}{(z-\alpha)(z-\beta)},\beta\right) \right)
\]

\[
F_n=-\left( \frac{1}{\alpha^{n+1}}\frac{-\alpha}{(\alpha-\beta)}+\frac{1}{\beta^{n+1}}\frac{-\beta}{(\beta-\alpha)} \right)
\]

\[
F_n=\left( \frac{1}{\alpha^{n}}\frac{1}{(\alpha-\beta)}-\frac{1}{\beta^{n}}\frac{1}{(\alpha-\beta)} \right)
\]

\[
F_n=\frac{1}{(\alpha-\beta)}\left( \frac{1}{\alpha^{n}}-\frac{1}{\beta^{n}} \right)
\]

Et si on remplace par les valeurs de $\alpha=\frac{1+\sqrt{5}}{-2}$ et $\beta=\frac{1-\sqrt{5}}{-2}$, cela donne :

\[
F_n=-\frac{1}{\left(\frac{1+\sqrt{5}}{-2}-\frac{1-\sqrt{5}}{-2}\right)}\left( \frac{1}{\left(\frac{1+\sqrt{5}}{-2}\right)^{n}}-\frac{1}{\left(\frac{1-\sqrt{5}}{-2}\right)^{n}}\right)=-\frac{1}{\sqrt{5}}\left( \left(\frac{-2}{1+\sqrt{5}}\right)^{n}-\left(\frac{-2}{1-\sqrt{5}}\right)^{n}\right)
\]

\[
F_n=-\frac{1}{\sqrt{5}}\left( \left(\frac{-2(1-\sqrt{5})}{(1-\sqrt{5})(1+\sqrt{5})}\right)^{n}-\left(\frac{-2(1+\sqrt{5})}{(1-\sqrt{5})(1+\sqrt{5})}\right)^{n}\right)=\frac{1}{\sqrt{5}}\left( \left(\frac{1+\sqrt{5}}{2}\right)^{n}-\left(\frac{1-\sqrt{5}}{2}\right)^{n}\right)
\]

Finalement pour résumer on à trouvé la fonction génératrice des nombres de Fibonacci, avec:

\begin{equation}
\boxed{\frac{z}{1-z-z^2}=\sum_{n=0}^{\infty}F_n z^z=z+z^2+2z^3+3z^4+5z^5+...+O(z^7)}
\end{equation}
et
\begin{equation}
\boxed{F_n=\frac{1}{\sqrt{5}}\left( \varphi^{n}-\varphi'^{n}\right)\quad avec \quad \varphi=\frac{1+\sqrt{5}}{2} \quad et\quad \varphi'=\frac{1-\sqrt{5}}{2}}
\end{equation}

\notes{Le nombre $\varphi$ est \textbf{le nombre d'or}. Comme $|\varphi'|<|\varphi|$, il en résulte que un équivalent lorsque $n\to\infty$ de $F_n$ est $\frac{\varphi^n}{\sqrt{5}}$.}

\newpage
\section{Principe de l'argument}
\subsection{Énoncé}

\begin{tabular}{cc}
\begin{minipage}{12cm}

Soit $f(z)$, une fonction méromorphe sur un domaine $U$, et $\delta U$ le contour fermé orienté dans le sens trigonométrique de ce domaine, englobant $P$ pôles isolés notés $p_i$ et $N$ zéros isolés notés $z_i$, alors:

\begin{equation}
\boxed{\oint_{\delta U} \frac{f'(z)}{f(z)}dz=2i\pi (N-P)}
\end{equation}

On remarque que la fonction $\frac{f'(z)}{f(z)}$ est également une fonction méromorphe sur $U$ car la dérivée et l'inverse d'une fonction méromorphe sont aussi des fonctions méromorphes.

\end{minipage}
&
\begin{minipage}{5cm}

\begin{figure}[H]
\centering
\begin{tikzpicture}[scale=1]
\TikzGraphC{-2}{-2}{2}{2}
\draw (-0.3,-0.4) node {$\bullet$} node [above left] {$\rho_1$};
\draw (0.6,0.4) node {$\bullet$} node [above left] {$\rho_2$};
\draw (0.2,-0.4) node {$\bullet$} node [above right] {$\rho_3$};
\draw (-0.2,0.4) node {$\bullet$} node [above left] {$z_1$};
\draw (0.4,-1) node {$\bullet$} node [above left] {$z_2$};
\draw [color=red](-0.9,-0.4) node [below left] {$\delta U$};
\draw [color=red,thick,rotate around={45:(0,0)}] (0,0) ellipse (1cm and 2cm);
\draw [->,thick] (-0.71,-0.69) -- (-0.7,-0.7);
\draw [->,thick] (0.71,0.69) -- (0.7,0.7);
\end{tikzpicture}
\caption{Schéma : Principe de l'argument}
\end{figure} 

\end{minipage}
\end{tabular}

\subsection{Démonstration :}

Pour toute fonction $f(z)$ méromorphe sur un domaine $U$, ayant $P$ pôles $p_i$ et $N$ zéros $z_i$, on peut poser:

\[
f(z)=\frac{\prod_{i=0}^{N}(z-z_i)}{\prod_{i=0}^{P}(z-p_i)} g(z)
\]

Ou $g(z)$ est alors une fonction entière et non nulle sur $U$. Dans ce cas la dérivée logarithmique s'exprime par:

\[
\frac{f'(z)}{f(z)}=\frac{d}{dz} ln(f(z))=\frac{d}{dz} \left( ln(g(z))+\sum_{i=0}^{N}ln(z-z_i) - \sum_{i=0}^{P}ln(z-p_i)\right)
\]

\[
\frac{f'(z)}{f(z)}=\frac{g'(z)}{g(z)}+\sum_{i=0}^{N}\frac{1}{z-z_i} - \sum_{i=0}^{P}\frac{1}{z-p_i}
\]

Donc si on intègre sur le contour du domaine, on obtient:

\[
\oint_{\delta U} \frac{f'(z)}{f(z)} dz=\oint_{\delta U} \frac{g'(z)}{g(z)} dz + \oint_{\delta U}\sum_{i=0}^{N}\frac{1}{z-z_i} dz - \oint_{\delta U} \sum_{i=0}^{P}\frac{1}{z-p_i} dz
\]


Or $\oint_{\delta U} \frac{g'(z)}{g(z)} dz=0$ car l'intégrante n'a pas de pôles dans $U$.

\[
\oint_{\delta U} \frac{f'(z)}{f(z)} dz=\oint_{\delta U}\sum_{i=0}^{N}\frac{1}{z-z_i} dz - \oint_{\delta U} \sum_{i=0}^{P}\frac{1}{z-p_i} dz
\]

Le théorème des résidus \ref{eq_residus}, donne alors : 
\[
\oint_{\delta U} \frac{f'(z)}{f(z)}dz=2i\pi \sum_{i=0}^{N} Res \left( \frac{1}{z-z_i},z_i \right)-2i\pi \sum_{i=0}^{P} Res \left( \frac{1}{z-p_i},z_i \right)
\]
Mais les résidus sont triviaux et valent 1 donc on peut écrire directement que :

\begin{equation}
\boxed{\oint_{\delta U} \frac{f'(z)}{f(z)}dz=2i\pi (N-P)}
\end{equation}

\newpage
\subsection{Théorème de Rouché}

Soit $f$ et $g$ deux fonctions méromorphes sur un domaine $U$ et $\delta U$ le bord de ce domaine. Si $\forall z\in\delta U$ on a $|f(z)-g(z)|<|g(z)|$ alors :

\begin{equation}
\boxed{Z_{f,U}-P_{f,U}=Z_{g,U}-P_{g,U}} 
\end{equation}

Ou $Z_{f,U}$ et $P_{f,U}$ dénote respectivement le nombre de zéros et de pôles de $f$ sur $U$.\\

Pour démontrer ce théorème on peut partir de l'inégalité:

\[
|f(z)-g(z)|<|g(z)| \quad\quad\quad \forall z\in\delta U
\]

Comme $g(z)$ et $f(z)$ ne s'annulent pas sur $\delta U$ (sinon l'inégalité n'est pas respectée), on peut écrire que:
\[
\frac{|f(z)-g(z)|}{|g(z)|}<1 \quad\quad\quad \forall z\in\delta U
\]

Maintenant considérons :

\begin{eqnarray*}
h(z)&=&\frac{f(z)}{g(z)}\\
h(z)-1&=&\frac{f(z)}{g(z)}-1\\
h(z)-1&=&\frac{f(z)-g(z)}{g(z)}\\
|h(z)-1|&=&\left|\frac{f(z)-g(z)}{g(z)}\right|\\
\end{eqnarray*}

L'inégalité se traduit alors par :

\[
|h(z)-1|<1\quad\quad\quad \forall z\in\delta U
\]

Donc l'image de $U$ par $h(z)$ est inclue dans $D(1,1)$ le disque centré en $1$ de rayon strictement inférieur à $1$. Par conséquent cela signifie que $h(z)$ ne possède pas de pôle ni de zéro sur $U$. Intégrons maintenant la dérivée logarithmique sur le bord domaine :

\begin{eqnarray*}
\frac{h'(z)}{h(z)}&=&\frac{f'(z)}{f(z)}-\frac{g'(z)}{g(z)}\\
\oint_{\delta U} \frac{h'(z)}{h(z)}dz&=& \oint_{\delta U} \frac{f'(z)}{f(z)}dz-\oint_{\delta U} \frac{g'(z)}{g(z)}dz\\
\end{eqnarray*}

Si on applique le théorème de l'argument on obtient :

\[
	0=Z_{f,U}-P_{f,U}-(Z_{g,U}-P_{g,U})
\]
Finalement 
\begin{equation}
\boxed{Z_{f,U}-P_{f,U}=Z_{g,U}-P_{g,U}} 
\end{equation}

\newpage
\subsection{Exemples\todo}

\newpage
\section{Transformation des Séries}
\subsection{Sommations d'Abel}

\subsubsection{Sommation par parties : Première forme }
\label{eq_abel1}

La formule sommatoire d'Abel est l'équivalent pour les sommes discrètes de l'intégration par partie. Soit deux suite $u_n$ et $v_n$, si on pose $U_k=\sum_{n=a}^k u_n$, calculons:

\[
\sum_{n=a}^{b}u_n v_n
\]
Si $n>a$ alors $u_n=U_n-U_{n-1}$, on extrait le premier terme et que on remplace :

\[
\sum_{n=a}^{b}u_n v_n=u_a v_a + \sum_{n=a+1}^{b}u_n v_n
\]
\[
\sum_{n=a}^{b}u_n v_n=u_a v_a + \sum_{n=a+1}^{b} (U_n-U_{n-1}) v_n
\]

Avec $u_a=U_a$ et si on développe la somme:

\[
\sum_{n=a}^{b}u_n v_n = U_a v_a + \sum_{n=a+1}^{b} (U_n-U_{n-1}) v_n
\]

\[
\sum_{n=a}^{b}u_n v_n = U_a v_a + \sum_{n=a+1}^{b} 
U_n v_n - \sum_{n=a+1}^{b} U_{n-1} v_n
\]

On peut rentrer le premier terme dans la première somme et on peut décaler les indice dans la seconde :

\[
\sum_{n=a}^{b}u_n v_n = \sum_{n=a}^{b} 
U_n v_n - \sum_{n=a}^{b-1} U_{n} v_{n+1}
\]

Si on extrait le dernier terme de la première somme :

\[
\sum_{n=a}^{b}u_n v_n = U_b v_b + \sum_{n=a}^{b-1} 
U_n v_n - \sum_{n=a}^{b-1} U_{n} v_{n+1}
\]

On peut alors regrouper les deux sommes:

\[
\boxed{\sum_{n=a}^{b}u_n v_n = U_b v_b - \sum_{n=a}^{b-1} 
U_n (v_{n+1} - v_{n}) \quad\quad avec \quad\quad U_k=\sum_{n=a}^k u_n}
\]

\notes{On peut remarquer l'analogie avec l'intégration par partie, en effet la suite $v_n$ est différencié alors que la suite $u_n$ est sommée (elle devient $u_n$).}

\newpage
\subsubsection{Sommation par parties : Seconde forme }
\label{eq_abel2}

Maintenant on prend $w,x,y\in\mathbb{R}$, et on note $a=\floor{x}$ et $b=\floor{y}$. Et on considère $v_n=\varphi(n)$, et $U(w)=\sum_{n=a}^{\floor{w}}u_n$, avec ces notations la formules précédente nous donne:

\[
\sum_{n=a}^{b}u_n \varphi(n) = U(b) \varphi(b) - \sum_{n=a}^{b-1} 
U(n) (\varphi(n+1) - \varphi(n)) \quad\quad avec \quad\quad U_k=\sum_{n=a}^k u_n
\]

Mais on peut écrire que $\varphi(n+1)-\varphi(n)=\int_n^{n+1}\varphi'(t)dt$, on obtient alors:
\[
	\sum_{n=a}^{b}u_n \varphi(n) = U(b) \varphi(b) - \sum_{n=a}^{b-1} 
U(n) \int_n^{n+1}\varphi'(t)dt
\]
Mais $U(w)$ est constant de $n$ à $n+1$ donc on peut le rentrer dans l'intégrale et :
\[
	\sum_{n=a}^{b}u_n \varphi(n) = U(b) \varphi(b) - \sum_{n=a}^{b-1} 
 \int_n^{n+1}U(t)\varphi'(t)dt
\]
Maintenant, on peut alors fusionner l'intégrale et la somme pour obtenir :
\[
	\sum_{n=a}^{b}u_n \varphi(n) = U(b) \varphi(b) - \int_a^{b}U(t)\varphi'(t)dt
\]
Si on passe le premier terme de la somme de gauche dans la partie droite de l'équation, on obtient:
\[
	\sum_{n=a+1}^{b} u_n \varphi(n)= U(b) \varphi(b) - u_a \varphi(a) - \int_a^{b}U(t)\varphi'(t)dt
\]
Rappelons que $U(a)=u_a$
\[
	\sum_{n=a+1}^{b} u_n \varphi(n)= U(b) \varphi(b) - U(a) \varphi(a) - \int_a^{b}U(t)\varphi'(t)dt
\]

Mais on à aussi de par la dérivée d'un produit de manière générale on a $[gf]_a^b= \int_a^b (g'f+gf')$, ce qui donne :
\[
\left\{
\begin{array}{c}
{[U(t) \varphi(t)]_a^x= \int_a^x (U'(t) \varphi(t)+U(t) \varphi'(t))\ dt} \\
{[U(t) \varphi(t)]_b^y= \int_b^y (U'(t) \varphi(t)+U(t) \varphi'(t))\ dt} \\
\end{array}
\right.
\]
Mais sur les intervalles considérés $U'(t)=0$ car $U(t)$ est constant de $a$ à $x$ ou de $b$ à $y$, donc :
\[
\left\{
\begin{array}{c}
U(a) \varphi(a) = U(x) \varphi(x) -\int_a^x U(t)\varphi'(t) dt\\
U(b) \varphi(b) = U(y) \varphi(y) -\int_b^y U(t)\varphi'(t) dt\\
\end{array}
\right.
\]
Donc si on remplace on obtient :
\[
	\sum_{n=a+1}^{b} u_n \varphi(n)= U(y) \varphi(y) -\int_b^y U(t)\varphi'(t) dt - \left(U(x) \varphi(x) -\int_a^x U(t)\varphi'(t) dt\right) - \int_a^{b}U(t)\varphi'(t)dt
\]
\[
	\sum_{n=a+1}^{b} u_n \varphi(n)= U(y) \varphi(y)-U(x) \varphi(x)  - \int_x^a U(t)\varphi'(t) dt - \int_a^{b}U(t)\varphi'(t)dt - \int_b^y U(t)\varphi'(t) dt
\]
Finalement si on fusionne les trois intégrales, $\forall x,y \in \mathbb{R}$:
\[
	\boxed{\sum_{n=\floor{x}+1}^{\floor{y}} u_n \varphi(n)= U(y) \varphi(y)-U(x) \varphi(x)  - \int_x^y U(t)\varphi'(t) dt \quad\quad avec \quad\quad U(w)=\sum_{n=a}^{\floor{w}}u_n}
\]

\newpage
\subsection{Sommation d'Euler-Maclaurin}

\subsubsection{Définition}
La formule d'Euler-Maclaurin nous enseigne qu'une somme discrète sur une fonction holomorphe $f(z)$ peut être reliée à une intégrale et une somme pondérée par les nombres de Bernoulli de ses dérivées aux extrémités. Ainsi, soit $a$ et $b$ deux entiers positifs tel que $b>a$ :
\label{eq_maclaurin}
\begin{equation}
\boxed{\sum_a^{b-1} f(z) = \int_a^b f(z) dz + \sum_{n=1}^{\infty}\frac{B_{n}}{n!}(f^{(n-1)}(b)-f^{(n-1)}(a))}
\end{equation}
Alternativement si on considère que les nombres de Bernoulli pairs:
\[
\sum_a^{b-1} f(z) = \int_a^b f(z) dz + \frac{B_{1}}{1!}(f(b)-f(a))+ \sum_{n=2}^{\infty}\frac{B_{n}}{n!}(f^{(n-1)}(b)-f^{(n-1)}(a))
\]
\[
\sum_a^{b-1} f(z) = \int_a^b f(z) dz - \frac{f(b)-f(a)}{2}+ \sum_{n=2}^{\infty}\frac{B_{n}}{n!}(f^{(n-1)}(b)-f^{(n-1)}(a))
\]
\begin{equation}
\boxed{\sum_a^{b-1} f(z) = \int_a^b f(z) dz - \frac{f(b)-f(a)}{2}+ \sum_{n=1}^{\infty}\frac{B_{2n}}{2n!}(f^{(2n-1)}(b)-f^{(2n-1)}(a))}
\end{equation}

Pour une liste des nombres de Bernoulli, voir \ref{table_bern}, et pour voir comment les calculer voir \ref{eq_bernoulli} . 

\subsubsection{Démonstration par les opérateurs}
Si on part du développement de Taylor en $a$ d'ordre $n$ (voir \ref{eq_taylor}):
\[
f(z)=f(a)+\frac{f'(a)}{1!}(z-a)+...+\frac{f^{(n)}(a)}{n!}(z-a)^n
\]
On peut également écrire :
\[
f(z+a)=f(a)+\frac{f'(a)}{1!}z+...+\frac{f^{(n)}(a)}{n!}z^n
\]
Et aussi si on inverse z et a ce qui est parfaitement licite:
\[
f(z+a)=f(z)+\frac{f'(z)}{1!}a+...+\frac{f^{(n)}(z)}{n!}a^n
\]
Introduisons l'opérateur $\op{D}$ de dérivation c'est à dire tel que $\applyop{D}{f(z)}(z)=f'(z)$ et par extension si on applique $n$ fois l'opérateur on a $\applyopp{D}{n}{f(z)}(z)=f^{(n)}(z)$, on obtient avec cette notation:
\[
f(z+a)=f(z)+\frac{\op{D}f(z)}{1!}a+...+\frac{\opp{D}{n}f(z)}{n!}a^n
\]
Soit si on factorise
\[
f(z+a)=\left(I+\frac{\op{D}a}{1!}+...+\frac{(\op{D} a)^n}{n!}\right)f(z)
\]
On reconnait alors la série de Taylor d'une exponentielle, et on obtient une manière élégante d'écrire un développement de Taylor:
\[
\boxed{f(z+a)=e^{\op{D}a} f(z)}
\]
Maintenant on peut écrire aussi :
\[
f(z+1)=e^{\op{D}} f(z)
\]
Et on peut introduire l'opérateur décalage $\op{E}=e^\op{D}$ tel que $\applyop{E}{f(z)}(z)=f(z+1)$, maintenant on peut ecrire la somme suivante en fonction de $\op{E}$:
\[
\sum_a^b f(z) = f(a)+f(a+1)+f(a+2)+f(a+3)+...+f(a+(b-a))
\]
Soit
\[
\sum_a^b f(z) = \op{I}f(a)+\op{E}f(a)+\op{E}^2f(a)+\op{E}^3f(a)+...+\op{E}^{(b-a)}f(a)
\]
\[
\sum_a^b f(z) = (\op{I}+\op{E}+\op{E}^2+\op{E}^3+...+\op{E}^{(b-a)})f(a)
\]
On reconnait alors une suite géométrique de raison $E$:
\[
\sum_a^b f(z) = \frac{\op{E}^{b-a+1}-I}{\op{E}-\op{I}}f(a)
\]
\[
\sum_a^b f(z) = (\op{E}^{b-a+1}-\op{I})\frac{\op{I}}{\op{E}-\op{I}}f(a)
\]
Si on pose $k=b-a+1$ et que on se rappelle que $\op{E}=e^\op{D}$ et Que on introduit $\op{D}^{-1}\op{D}=\op{I}$ :
\[
\sum_a^b f(z) = (\op{E}^{k}-\op{I})\op{D}^{-1}\frac{\op{D}}{e^\op{D}-\op{I}}f(a)
\]
Mais on reconnait la fonction génératrice des nombres de Bernoulli (voir \ref{eq_bernoulli}):$\frac{z}{e^z-1}=\sum_{n=0}^{\infty} \frac{B_n}{n!} z^n$

\[
\sum_a^b f(z) = (\op{E}^{k}-\op{I})\op{D}^{-1}\sum_{n=0}^{\infty} \frac{B_n}{n!} \op{D}^nf(a)
\]

\[
\sum_a^b f(z) = (\op{E}^{k}-\op{I})\sum_{n=0}^{\infty} \frac{B_n}{n!} \op{D}^{n-1}f(a)
\]

\[
\sum_a^b f(z) = (\op{E}^{k}-\op{I})\left(\op{D}^{-1}+\sum_{n=1}^{\infty} \frac{B_n}{n!} \op{D}^{n-1}\right)f(a)
\]

\[
\sum_a^b f(z) = \left.\int_0^z f(x) dx \right|_a^{a+k} + (\op{E}^{k}-\op{I})\left(\sum_{n=1}^{\infty} \frac{B_n}{n!} \op{D}^{n-1}\right)f(a)
\]

\[
\sum_a^b f(z) = \left.\int_0^z f(x) dx \right|_a^{a+k} + \left.\sum_{n=1}^{\infty} \frac{B_n}{n!} f^{n-1}(z)\right|_a^{a+k}
\]

\[
\sum_a^b f(z) = \int_a^{a+k} f(z) dz + \sum_{n=1}^{\infty} \frac{B_n}{n!} \left(f^{(n-1)}(a+k)-f^{(n-1)}(a)\right)
\]

\[
\sum_a^b f(z) = \int_a^{b+1} f(z) dz + \sum_{n=1}^{\infty} \frac{B_n}{n!} \left(f^{(n-1)}(b+1)-f^{(n-1)}(a)\right)
\]
Finalement si on remplace les bornes :
\begin{equation}
\boxed{\sum_a^{b-1} f(z) = \int_a^{b} f(z) dz + \sum_{n=1}^{\infty} \frac{B_n}{n!} \left(f^{(n-1)}(b)-f^{(n-1)}(a)\right)}
\end{equation}

\subsubsection{Somme d'entiers: $f(z)=z$}
On a $f^{(1)}(z)=1$ et $f^{(2)}(z)=0$. La formule nous donne :
\[
\sum_a^{b-1} z = \int_a^b z dz + \frac{B_{1}}{1!}(f(b)-f(a)) + \frac{B_{2}}{2!}(f^{(1)}(b)-f^{(1)}(a))
\]
Le second terme est nul:
\[
\sum_a^{b-1} z = \int_a^b z dz + \frac{B_{1}}{1!}(f(b)-f(a))
\]
Finalement
\[
\boxed{\sum_a^{b-1} z = \frac{1}{2}(b^2-a^2) - \frac{1}{2}(b-a)}
\]

Vérifions avec $a=1$ et $b=5$:

\[
\sum_a^{b-1} z^2=\sum_1^{4} z^2 = 1+2+3+4=10
\]
\[
\frac{1}{2}(5^2-1^2) - \frac{1}{2}(5-1)=12-2=10
\]

	\subsubsection{Somme des carrés: $f(z)=z^2$}

On a $f^{(1)}(z)=2z$ et $f^{(2)}(z)=2$. La formule nous donne :
\[
\sum_a^{b-1} z^2 = \int_a^b z^2 dz + \frac{B_{1}}{1!}(f(b)-f(a)) + \frac{B_{2}}{2!}(f^{(1)}(b)-f^{(1)}(a))
\]
De plus $B_1=\frac{-1}{2}$, $B_2=\frac{1}{6}$:
\[
\boxed{\sum_a^{b-1} z^2 = \frac{1}{3}(b^3-a^3) - \frac{1}{2}(b^2-a^2) + \frac{1}{6}(b-a)}
\]
Vérifions avec $a=1$ et $b=5$:

\[
\sum_a^{b-1} z^2=\sum_1^{4} z^2 = 1^2+2^2+3^2+4^2=1+4+9+16=30
\]
\[
\frac{5^3-1^3}{3} - \frac{5^2-1^2}{3} + \frac{1}{6}(5-1)= \frac{124}{3}+\frac{4}{6} - 12 = \frac{744+12}{18} - 12=30
\]

	\subsubsection{Somme des cubes: $f(z)=z^3$}

On a $f^{(1)}(z)=3z^2$ et $f^{(2)}(z)=6z$ et $f^{(3)}(z)=6$. La formule nous donne :
\[
\sum_a^{b-1} z^3 = \int_a^b z^3 dz + \frac{B_{1}}{1!}(f(b)-f(a)) + \frac{B_{2}}{2!}(f^{(1)}(b)-f^{(1)}(a))+ \frac{B_{3}}{3!}(f^{(2)}(b)-f^{(2)}(a))
\]
De plus $B_1=\frac{-1}{2}$, $B_2=\frac{1}{6}$, $B_3=0$:

\[
\boxed{\sum_a^{b-1} z^3 = \frac{1}{4}(b^4-a^4) - \frac{1}{2}(b^3-a^3) + \frac{1}{4}(b^2-a^2)}
\]
Vérifions avec $a=1$ et $b=5$:
\[
\sum_a^{b-1} z^3=\sum_1^{4} z^3 = 1^3+2^3+3^3+4^3=1+8+27+64=100
\]

\[
\frac{1}{4}(5^4-1^4) - \frac{1}{2}(5^3-1^3) + \frac{1}{4}(5^2-1^2)=156-62+6 = 100
\]

\newpage
\subsection{Produit de Cauchy}
\label{eq_produit_cauchy}
Soit deux séries $A$ et $B$ absolument convergentes :
\[
	A=\sum_{j=0}^\infty a_j \quad et \quad B=\sum_{k=0}^\infty b_k
\]

Calculons leur produit, on obtient:
\[
	A B= \sum_{j=0}^\infty a_j\sum_{k=0}^\infty b_k =  \sum_{j=0}^\infty \sum_{k=0}^\infty a_j b_k
\]

On peut écrire tous les termes que l'on doit sommer sous forme d'une matrice:

\[
	\left(
	\begin{array}{ccccc}
	a_0b_0 & a_0b_1 & a_0b_2 & a_0b_3 & \hdots\\
	a_1b_0 & a_1b_1 & a_1b_2 & a_1b_3 & \hdots\\
	a_2b_0 & a_2b_1 & a_2b_2 & a_2b_3 & \hdots\\
	a_3b_0 & a_3b_1 & a_3b_2 & a_3b_3 & \hdots\\
	\vdots & \vdots & \vdots & \vdots & \ddots\\	
	\end{array}
	\right)
\]

On peut alors effectuer la somme à la manière de Cauchy c'est à dire en sommant sur les diagonales et on obtient:

\begin{equation}
	\boxed{\sum_{j=0}^\infty a_j \sum_{k=0}^\infty b_k= \sum_{n=0}^\infty c_n \quad avec \quad c_n=\sum_{j+k=n}a_j b_k=\sum_{k=0}^n a_{k} b_{n-k}}
\end{equation}


\subsection{Produit de Dirichlet}
\label{eq_produit_dirichlet}
En fait il existe un lien important avec \textbf{la convolution de Dirichlet} (voir \ref{eq_dirichlet_conv}) car un autre moyen de sommer l'ensemble des termes est d'écrire que le produit des deux indices $j$ et $k$ est égale à $n$, soit :
\begin{equation}
	\boxed{\sum_{j=0}^\infty a_j \sum_{k=0}^\infty b_k= \sum_{n=0}^\infty c_n \quad avec \quad c_n=\sum_{jk=n}a_j b_k=\sum_{d|n}a_d b_{\frac{n}{d}}=(a \star b)(n)}
\end{equation}

Si les indices des sommes ne démarrent pas à zéro, on à alors la formule plus générale:

\begin{equation}
	\boxed{\sum_{j=j_0}^\infty a_j \sum_{k=k_0}^\infty b_k= \sum_{n=j_0 k_0}^\infty c_n \quad avec \quad c_n=\sum_{jk=n}a_j b_k=\sum_{d|n}a_d b_{\frac{n}{d}}=(a \star b)(n)}
\end{equation}

Une conséquence \textbf{très importante} est l'application au produit de deux \textbf{séries de Dirichlet}, car:

\[
\left(\sum_{n=1}^{\infty}\frac{f(n)}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{g(n)}{n^s}\right)=\sum_{n=1}^{\infty}\sum_{jk=n}\frac{f(k)}{k^s}\frac{g(j)}{j^s}=\sum_{n=1}^{\infty}\sum_{jk=n}\frac{f(k)g(j)}{(jk)^s}=\sum_{n=1}^{\infty}\frac{1}{n^s}\sum_{jk=n}f(k)g(j)
\]
Donc finalement:

\begin{equation}
	\boxed{\left(\sum_{n=1}^{\infty}\frac{f(n)}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{g(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{(f\star g)(n)}{n^s}}
\end{equation}

\newpage
\section{Convolution de Dirichlet}
\label{eq_dirichlet_conv}
\subsection{Définition}
Soit $f(s)$ et $g(s)$ deux fonctions complexes, et $a,b,n\in\mathbb{N}^*$, on pose :
\[
\boxed{(g \star f)(n)=\sum_{ab=n}g(a)f(b)}
\]
La somme porte donc sur tous les couples d'entiers $a$ et $b$ tel que $ab=n$. Ce qui implique que $a$ et $b$ sont des diviseurs de $n$. La somme s'écrit également sur l'ensemble des diviseurs de $n$, car $a=d$ implique que $b=n/d$ et on obtient: 
\[
\boxed{(g \star f)(n)=\sum_{d|n}g(d)f\left(\frac{n}{d}\right)}
\]
Par exemple:
\begin{itemize}
\item Si $n=p$ avec $p$ un nombre premier, alors la somme comporte deux termes car $n$ possède deux diviseurs $d=\{1,p\}$, on peut écrire :
\[
(g \star f)(p)= g(1)f(p) + g(p)f(1)
\]
\item Si $n=p^k$ avec $p$ un nombre premier, alors la somme comporte $k+1$ termes car $n$ possède $k+1$ diviseurs $d=\{1,p,p^2,...,p^k\}$, on peut écrire :
\[
(g \star f)(p^k)= \sum_{i=0}^{k} g(p^i)f(p^{k-i})
\]
\item Si $n=p_1 p_2$ avec $p_1$ et $p_2$ deux nombres premiers distincts, alors la somme comporte trois termes car $n$ possède trois diviseurs $d=\{1,p_1,p_2\}$, on peut écrire :

\[
(g \star f)(p)= g(1)f(n) + g(p_1)f\left(\frac{n}{p_1}\right)+ g(p_2)f\left(\frac{n}{p_2}\right)
\]
Soit $n=p_1 p_2$ :
\[
(g \star f)(p)= g(1)f(p_1 p_2) + g(p_1)f(p_2)+ g(p_2)f(p_1)
\]

\end{itemize}

\subsection{Commutativité : $(f\star g)=(g\star f)$}

Partons de :
\[
(g \star f)(n)=\sum_{ab=n}g(a)f(b)
\]
Si on pose $a=d$, avec $d$ un diviseur de $n$, on à vu que la convolution s'écrit aussi :
\[
(g \star f)(n)=\sum_{d|n}g(d)f\left(\frac{n}{d}\right)
\]
Et si on inverse $f$ et $g$ alors:
\[
(f \star g)(n)=\sum_{d|n}f(d)g\left(\frac{n}{d}\right)
\]
Mais on peut aussi poser $b=d$ et dans ce cas $a=b/d$ :
\[
(g \star f)(n)=\sum_{d|n}g\left(\frac{n}{d}\right)f(d)
\]
Donc 
\[
(f \star g)(n)=(g \star f)(n)
\]
Finalement on à bien :
\[
\boxed{(f\star g)=(g\star f)}
\]

\subsection{Associativité : $f\star (g\star h) = (f\star g) \star h$ }

\[
f\star (g\star h) = \sum_{ab=n} f(a) (g\star h)(b)= \sum_{ab=n} f(a) \sum_{cd=b} g(c) h(d) = \sum_{acd=n} f(a)g(c)h(d)
\]
Par symétrie, si on échange $h$ avec $f$ dans la formule précédente on obtient
\[
h\star (g\star f) = \sum_{acd=n} h(a)g(c)f(d)
\]
Comme $a,c,d$ sont interchangeable on obtient bien l'associativité:
\[
\boxed{f\star (g\star h) = (f\star g) \star h}
\]

\subsection{Élément neutre: $(f\star \delta_1)=f$}
\label{eq_diriclet_delta}
Posons la fonction :
\[
\delta_1(n)=\left\{\begin{array}{ll}
1 & Si\ n=1\\
0 & Sinon\\
\end{array}
\right.
\]
Calculons:
\[
	(f\star \delta_1)(n)=\sum_{d|n}f(d)\delta_1\left(\frac{n}{d}\right)
\]
Le seul terme non nul de la somme s'obtient pour $d=n$ et donc :
\[
	(f\star \delta_1)(n)=f(n)
\]
On obtient bien que $\delta_1$ et l'élément neutre de l'application de convolution:
\[
	\boxed{(f\star \delta_1)=f}
\]

\subsection{Convolution par la fonction constante : $(f\star \mathbbm{1})(n)=\sum_{d|n}f(d)$ }

On définit la fonction constante par :
\[
\mathbbm{1}(n)=1 \quad \forall n\in\mathbb{N}^*
\]
Par la définition de la convolution de Dirichlet, on a:
\[
(f\star \mathbbm{1})(n)=\sum_{d|n} f(d) \mathbbm{1}\left(\frac{n}{d}\right)
\]
Ce qui donne trivialement:
\[
\boxed{(f\star \mathbbm{1})(n)=\sum_{d|n} f(d)}
\]

\subsection{Inverse de la fonction constante : La fonction de Möbius $\mu(n)$}
\label{eq_mobius}
\label{eq_dirichlet_mu}
\begin{tabular}{cc}
\begin{minipage}{11cm}
On définit la fonction de Möbius, pour $n\in\mathbb{N}^\star$, par:
\[
\boxed{\mu(n)=\left\{
\begin{array}{ll}
1 & Si\ n=1\\
0 & Si\ n\ est\ divisible\ par\ un\ carre\ parfait\ et\ n\ne 1.\\
(-1)^k & Si\ n\ est\ le\ produit\ de\ k\ nombres\ premiers\ distincts.\\
\end{array}
\right.}
\]
Il peut sembler étrange de définir une telle fonction, mais elle joue un rôle élémentaire par rapport à la \textbf{convolution de Dirichlet} qui se traduit dans la \textbf{formule d'inversion de Möbius}, ensuite elle s'exprime pleinement dans \textbf{l'inverse de la fonction $\zeta(s)$}.\\
\end{minipage}
&
\begin{minipage}{8cm}
Voyons quelques exemples, pour se faire une idée:\\
\[
\begin{array}{|c|c|c|}
\hline
n&Decomposition&\mu(n)\\
\hline
1&1&1\\
2&2&-1\\
3&3&-1\\
4&2^2&0\\
5&5&-1\\
6&2 \cdot 3&1\\
7&7&-1\\
8&2^3&0\\
9&3^2&0\\
10&2\cdot 5&1\\
\hline
\end{array}
\]
\end{minipage}\\
\end{tabular}

Montrons que \textbf{cette fonction constitue l'inverse pour la convolution de Dirichlet de la fonction constante} définie précédemment. Il s'agit ici de montrer que la convolution de $\mu$ par $\mathbbm{1}$ donne l'élément neutre $\delta_1$. Le résultat obtenu précédemment de la convolution par la fonction constante, on peut partir de :
\[
(\mu \star \mathbbm{1})(n)=\sum_{d|n}\mu(d)
\]
Posons $h(n)=\sum_{d|n}\mu(d)$, alors pour $n=1$:
\[
	h(1)=\mu(1)=1
\]
Et pour $n>1$, considérons $P$, l'ensemble des facteurs premiers de $n$ notons son cardinal $N=card(P)$, considérons également $D$ l'ensemble des diviseurs de $n$ qui ne contiennent pas un carré, et notons $d_k$ un diviseur de $n$ formé de $k$ facteurs premiers parmi les $N$. on peut alors écrire :
\[
	h(n)=\sum_{d_k\in D} \mu(d_k)
\]
Comme $d_k$ est formé de $k$ facteurs premier, on obtient par propriété de la fonction $\mu$ que $\mu(d_k)=(-1)^k$. De plus l'ensemble $D$ contient $C^k_n$ diviseur $d_k$ composé de $k$ éléments, par conséquent:
\[
	h(n)=\sum_{k=0}^{N}C^k_n (-1)^k
\]
Notons qu'il ne faut pas oublier le cas $k=0$ qui correspond à $d_k=1$, on reconnait alors le binôme de newton (voir \ref{eq_bin_func}).
\[
	h(n)=(1-1)^N=0
\]
Donc en fait $h(n)=\delta_1(n)$, et finalement avec la commutativité :
\begin{equation}
\boxed{\mu \star \mathbbm{1} = \mathbbm{1} \star \mu =\delta_1}
\end{equation}

\subsection{Inversion de Möbius}

Soit $f$ et $g$ deux fonctions arithmétiques, si:
\[
f\star\mathbbm{1}=g
\]
Alors si on convolue par $\mu$ des deux cotés:
\[
(f\star\mathbbm{1})\star\mu=g\star\mu
\]
Par associativité :
\[
f\star(\mathbbm{1}\star\mu)=g\star\mu
\]
On retrouve l'élément neutre dans le membre de gauche
\[
f\star \delta_1=g\star\mu
\]
\[
f=g\star\mu
\]
Alors finalement, l'inversion de Möbius donne alors :
\begin{equation}
	\boxed{g=f\star\mathbbm{1} \quad \leftrightarrow \quad f=g\star\mu }
\end{equation}
Soit en d'autres termes:
\begin{equation}
	\boxed{g(n)=\sum_{d|n} f(d) \quad \leftrightarrow \quad f(n)=\sum_{d|n} g(d) \mu\left(\frac{d}{n}\right)}
\end{equation}

\subsection{Inversion d'une fonction multiplicative}

Soit $h$ une fonction totalement multiplicative c'est à dire tel que $\forall m,n \in\mathbb{N}$:
\[
\boxed{h(mn)=h(m) h(n) \quad et \quad h(1)=1 }
\]
Alors pour une telle fonction si on pose :
\[
(hf\star hg)(n)= \sum_{ij=n}h(i)f(i)h(j)g(j)
\]
Et si on utilise le fait que $h$ est multiplicative:
\[
(hf\star hg)(n)= \sum_{ij=n}h(ij)f(i)g(j)
\]
\[
(hf\star hg)(n)= h(n) \sum_{ij=n}f(i)g(j)
\]
Finalement si $h$ est multiplicative alors on peut écrire
\[
\boxed{(hf)\star (hg)= h ( g\star h)}
\]

Une conséquence importante est que toute fonction multiplicative $h$ possède un inverse trivial $h\mu$, en effet :

\[
h\star (h\mu)= (h\mathbbm{1})\star (h\mu)
\]
Comme $h$ est multiplicative la propriété précédente s'applique, on obtient:
\[
h\star (h\mu)= h(\mathbbm{1}\star \mu)
\]
\[
h\star (h\mu)= h\delta_1
\]
\[
h\star (h\mu)= h(1)
\]
Et comme $h(1)=1$, finalement on peut écrire:
\[
\boxed{h\star (h\mu)= 1}
\]
\subsection{Inversion de Möbius généralisée \todo}
\label{eq_mobius_gen}

Finalement :
\begin{equation}
\boxed{g(t)=\sum_{n=1}^{\floor{t}} h_n f\left(\frac{t}{n}\right) \quad \leftrightarrow \quad f(t)=\sum_{n=1}^{\floor{t}} h_n \mu(n) g\left(\frac{t}{n}\right) }
\end{equation}

\subsection{Exemples}


\subsubsection{Fonction nombre de diviseurs : $d=\mathbbm{1}\star\mathbbm{1}$ et $\mathbbm{1}=d\star\mu$}
\label{eq_dirichlet_d}
Partons de :
\[
(\mathbbm{1}\star\mathbbm{1})(n)=\sum_{d|n} \mathbbm{1}(d)\mathbbm{1}(n/d)=\sum_{d|n} 1
\]
Soit par définition de la fonction nombre de diviseurs notée $\boxed{d(n)=\sum_{d|n} 1}$,donc :
\[
\boxed{d=\mathbbm{1}\star\mathbbm{1}}
\]

Si on convolue de chaque coté par la fonction de Möbius, on obtient trivialement l'autre expression:
\[
d\star\mu=\mathbbm{1}\star\mathbbm{1}\star\mu
\]
\[
\boxed{\mathbbm{1}=d\star\mu}
\]

\subsubsection{Fonction somme des diviseurs : $\sigma=I\star\mathbbm{1}$ et $I=\sigma\star\mu$}
\label{eq_dirichlet_sigma}
Partons de :
\[
(I\star\mathbbm{1})(n)=\sum_{d|n} I(d)\mathbbm{1}(n/d)
\]
On rappel que $I(n)=n$ et $\mathbbm{1}(n)=1$, donc
\[
(I\star\mathbbm{1})(n)=\sum_{d|n} d
\]
Ce qui par définition est égale à la fonction \textbf{somme des diviseurs d'un entier} notée $\boxed{\sigma(n)=\sum_{d|n} d}$:
\[
\boxed{\sigma=I\star\mathbbm{1}}
\]
Si on convolue de chaque coté par la fonction de Möbius, on obtient trivialement l'autre expression:
\[
\sigma\star\mu=I\star\mathbbm{1}\star\mu
\]
\[
\boxed{\sigma\star\mu=I}
\]

\subsubsection{Fonction indicatrice d'Euler : $I=\varphi\star\mathbbm{1}$ et $\varphi=I\star\mu$}
\label{eq_dirichlet_phi}

Partons de :
\[
(\varphi\star\mathbbm{1})(n)=\sum_{d|n} \varphi(d)\mathbbm{1}(n/d)
\]
On rappel que $\mathbbm{1}(n)=1$, donc
\[
(\varphi\star\mathbbm{1})(n)=\sum_{d|n} \varphi(d)
\]
Or 
\[
(\varphi\star\mathbbm{1})(n)=\sum_{d|n} \varphi(d)=n
\]
Donc
\[
\boxed{\varphi\star\mathbbm{1}=I}
\]
Si on convolue de chaque coté par la fonction de Möbius, on obtient trivialement l'autre expression:
\[
\varphi\star\mathbbm{1}\star\mu=I\star\mu
\]
Finalement
\[
\boxed{\varphi=I\star\mu}
\]

\newpage
\section{Formule de Jensen}
\label{eq_jensen}
La formule de Jensen nous enseigne que toute fonction analytique $f$ peut être décrite localement sur un disque $D(0,R)$ par l'ensemble des modules de ces zéros $|z_i|$. Ainsi si $f(0)\ne 0$ alors on peut écrire:

\begin{equation}
\boxed{ln(|f(0)|)=\sum_{i=1}^{N}\ln\left(\frac{|z_i|}{r}\right)+\frac{1}{2\pi}\int_0^{2\pi} ln(|f(re^{i\theta})|) d\theta}
\end{equation}

Une seconde forme équivalente de cette formule établit que:

\begin{equation}
\boxed{
ln(|f(0)|)=-\int_{0}^{r}\frac{N(t)}{t}dt+\frac{1}{2\pi}\int_{0}^{2\pi}ln(|f(r e^{i\theta})|)d\theta}
\end{equation}

Ou $N(t)$ désigne le nombre de zéros de $f(z)$ de module inférieur à $t$.

\subsection{Démonstrations}
\subsubsection{Première forme}
Soit $f(z)$ une fonction holomorphe sur le disque $D(a,R)$. Si $f(z)$ \textbf{ne s'annule pas sur le disque} $D$ alors on peut écrire :

\[
f(z)=e^{g(z)}
\]

Ou $g(z)$ est une fonction holomorphe sur $D$. Si on prend le logarithme du module ,on obtient :

\[
ln(|f(z)|)=ln(|e^{g(z)}|)=\Re(g(z))
\]

La fonction $ln(|f(z)|)$ se comporte donc comme la partie réelle d'une fonction holomorphe, c'est donc une fonction harmonique, par la propriété de moyenne (voir \ref{eq_cauchy_moy}), on peut donc écrire :

\[
ln(|f(a)|)=\frac{1}{2\pi}\int_0^{2\pi} ln(|f(a+re^{i\theta})|) d\theta
\]

Et en particulier pour $a=0$ :

\[
\boxed{ln(|f(0)|)=\frac{1}{2\pi}\int_0^{2\pi} ln(|f(re^{i\theta})|) d\theta}
\]

Maintenant si $f(z)$, \textbf{s'annule sur le disque} pour $z=z_1,z_2,...,z_N$, alors si on pose le produit de Blaschke:

\[
F(z)=f(z)\prod_{i=1}^{N}\frac{r^2-z\bar{z_i}}{r(z-z_i)}
\]

Mais sur le cercle de rayon $r$ c'est à dire si on pose $z=re^{i\theta}$, on a:

\[
\left|\frac{r^2-re^{i\theta}\bar{z_i}}{r(re^{i\theta}-z_i)}\right|=\left|\frac{r-e^{i\theta}\bar{z_i}}{re^{i\theta}-z_i}\right|=\frac{|r-e^{-i\theta}z_i|}{|re^{i\theta}-z_i|}=\frac{|r-e^{-i\theta}z_i||e^{i\theta}|}{|re^{i\theta}-z_i|}=\frac{|re^{i\theta}-z_i|}{|re^{i\theta}-z_i|}=1
\]

Donc
\[
|F(r e^{i\theta})|=|f(r e^{i\theta})|
\]

$F(z)$ est alors une fonction holomorphe qui ne s'annule pas sur le disque $D$. On peut alors appliquer le raisonnement précédent c'est à dire poser $F(z)=e^{G(z)}$, et écrire :

\[
ln(|F(0)|)=\frac{1}{2\pi}\int_0^{2\pi} ln(|F(re^{i\theta})|) d\theta
\]

Mais comme $|F(r e^{i\theta})|=|f(r e^{i\theta})|$, alors 

\[
ln(|F(0)|)=\frac{1}{2\pi}\int_0^{2\pi} ln(|f(re^{i\theta})|) d\theta
\]

De plus $F(0)=f(0)\prod_{i=1}^{N}\frac{r}{z_i}$, ce qui donne :

\[
ln\left(\left|f(0)\prod_{i=1}^{N}\frac{r}{z_i}\right|\right)=\frac{1}{2\pi}\int_0^{2\pi} ln(|f(re^{i\theta})|) d\theta
\]
\[
ln(|f(0)|)+\sum_{i=1}^{N}\ln\left(\left|\frac{r}{z_i}\right|\right)=\frac{1}{2\pi}\int_0^{2\pi} ln(|f(re^{i\theta})|) d\theta
\]

On obtient finalement la formule de Jensen:
\begin{equation}
\boxed{ln(|f(0)|)=\sum_{i=1}^{N}\ln\left(\frac{|z_i|}{r}\right)+\frac{1}{2\pi}\int_0^{2\pi} ln(|f(re^{i\theta})|) d\theta}
\end{equation}


\subsubsection{Seconde forme}

Considérons la somme:
\[
S=\sum_{i=1}^{N}\ln\left(\frac{|z_i|}{r}\right)=-\sum_{i=1}^{N}\ln\left(\frac{r}{|z_i|}\right)=-\sum_{i=1}^{N}\ln(r)+\sum_{i=1}^{N}\ln(|z_i|)
\]
Soit
\[
\begin{array}{ll}
S&=-N\ln(r)+\sum_{i=1}^{N}\ln(|z_i|)\\
 &=-N\ln(r)+\sum_{i=1}^{N}\ln(|z_i|) (i-(i-1))\\
 &=-N\ln(r)+\sum_{i=1}^{N}i\ln(|z_i|)-(i-1)\ln(|z_i|)\\
 &=-N\ln(r)+\sum_{i=1}^{N}i\ln(|z_i|)- \sum_{i=1}^{N} (i-1)\ln(|z_i|)\\
 &=-N\ln(r)+\sum_{i=1}^{N}i\ln(|z_i|)- \sum_{i=0}^{N-1} i\ln(|z_{i+1}|)\\
 &=-N\ln(r)+Nln(|z_N|)+\sum_{i=1}^{N-1}i\ln(|z_i|)- \sum_{i=1}^{N-1} i\ln(|z_{i+1}|)\\
 &=N (ln(|z_N|)-ln(r))+\sum_{i=1}^{N-1}i \left(\ln(|z_i|)-\ln(|z_{i+1}|)\right)\\
 &=N \int_{r}^{|z_N|}\frac{1}{t} dt +\sum_{i=1}^{N-1}i \int_{|z_{i+1}|}^{|z_i|}\frac{1}{t}dt\\
\end{array}
\]

Maintenant si on introduit $N(t)$, le nombre de zéros inférieur à $t$, alors $N=N(r)$ et $i=N(|z_i|)$ si les $|z_i|$ sont indexés par les modules croissant, cela donne:

\[
S=N(r) \int_{r}^{|z_N|}\frac{1}{t} dt +\sum_{i=1}^{N-1} N(|z_i|) \int_{|z_{i+1}|}^{|z_i|}\frac{1}{t}dt
\]

Mais pour $t\in[|z_N|,r]$ on a $N(t)=N(r)$ donc on peut entrer $N(t)$ dans la première intégrale. On peut faire le même raisonnement avec la seconde intégrale:

\[
S=\int_{r}^{|z_N|}\frac{N(t)}{t} dt +\sum_{i=1}^{N-1} \int_{|z_{i+1}|}^{|z_i|}\frac{N(t)}{t}dt
\]
Soit
\[
S=\int_{r}^{|z_N|}\frac{N(t)}{t} dt + \int_{|z_N|}^{|z_1|}\frac{N(t)}{t}dt
\]
\[
S=-\int_{|z_1|}^{r}\frac{N(t)}{t}dt
\]

Finalement comme $|z_1|>0$ et que $N(t)=0$ pour $t\in[0,|z_1|]$:

\begin{equation}
\boxed{\sum_{i=1}^{N}\ln\left(\frac{|z_i|}{r}\right)=-\int_{0}^{r}\frac{N(t)}{t}dt}
\end{equation}

Cela démontre donc la seconde forme.

\subsection{Le théorème fondamental de l'algèbre}

Soit un polynôme à coefficients complexes de degré $k$ et supposons $a_0>0$:

\[
P_k(z)= a_0+a_1 z + ... + a_k z^k
\]

Rappelons la seconde formule de Jensen obtenue (voir \ref{eq_jensen}):

\[
ln(|f(0)|)+\int_{0}^{r}\frac{N(t)}{t}dt=\frac{1}{2\pi}\int_{0}^{2\pi}ln(|f(r e^{i\theta})|)d\theta
\]

La formule donne alors:
\[
ln(a_0)+\int_{0}^{r}\frac{N(t)}{t}dt = \frac{1}{2\pi}\int_{0}^{2\pi}ln(|P_k(r e^{i\theta})|)d\theta
\]
Mais lorsque $r\to\infty$ seule le monome de rang $k$ est prépondérant et :
\[
ln(a_0)+\int_{0}^{r}\frac{N(t)}{t}dt = \frac{1}{2\pi}\int_{0}^{2\pi}ln(a_k r^k)d\theta
\]
On peut décomposer à droite
\[
ln(a_0)+\int_{0}^{r}\frac{N(t)}{t}dt = \frac{k}{2\pi}\int_{0}^{2\pi}ln(r)d\theta + \frac{1}{2\pi}\int_{0}^{2\pi}ln(a_k)d\theta
\]
Les intégrales sont triviales
\[
ln(a_0)+\int_{0}^{r}\frac{N(t)}{t}dt = k ln(r) + ln(a_k)
\]
On a donc 
\[
\int_{0}^{r}\frac{N(t)}{t}dt = k ln(r)+ C_1 \quad avec \quad C_1=ln(a_k)-ln(a_0)
\]
Mais si $|a_0| > 0$ alors le polynôme ne s'annule pas en zéro et il existe donc un rayon minimal $|z_0|$ en dessous duquel le polynôme ne s'annule pas. De plus il existe un rayon $|z_n|$ au dessus duquel $N(t)=n$, ou $n$ est le nombre de zéros du polynôme, on peut donc décomposer l'intégrale comme :
\[
\int_{|z_0|}^{|z_n|}\frac{N(t)}{t}dt + \int_{|z_n|}^{r}\frac{n}{t}dt  = k ln(r)+ C_1
\]
La première intégrale est une constante. Et la seconde peut donc écrire :
\[
n\int_{|z_n|}^{r}\frac{1}{t}dt  = k ln(r)+ C_2 \quad avec \quad C_2=C_1+\int_{|z_0|}^{|z_n|}\frac{N(t)}{t}dt
\]
Soit 
\[
n\left[ln(t)\right]_{|z_0|}^{r}=k ln(r)+ C_2
\]
\[
n ln(r)- n ln(|z_0|)=k ln(r)+ C_2
\]
\[
n ln(r)=k ln(r)+ C_3 \quad avec\quad C_3=C_2+ n ln(|z_0|)
\]
Maintenant 
\[
n=k +\frac{C_3}{ln(r)}
\]
Mais si on prend la limite lorsque $r\to\infty$ alors on obtient bien que le nombre de zéros $n$ est égal au degré $k$ du polynôme. Ce qui démontre \textbf{le théorème fondamental de l'algèbre}.
\[
\boxed{n=k}
\]
 

\newpage
\section{Factorisation de Hadamard-Weierstrass}
\subsection{Définition}
\label{eq_hadamard_weierstrass}
Toute \textbf{fonction entière} peut se décomposer comme un produit ou intervint l'ensemble de ces zéros :

\begin{equation}
	\boxed{f(z)= z^m e^{P_{\rho}(z)} \prod_{n=1}^N E_{\rho}\left(\frac{z}{Z_n}\right) \quad\quad\quad avec \quad\quad\quad E_\rho(z)=\left\{
	\begin{array}{ll}
	(1-z) &si\ \rho=0\\
	(1-z)e^{(\frac{z}{1}+\frac{z}{2}+...+\frac{z}{\rho})} &sinon\\ 
	\end{array}
	\right.}
\end{equation}

\noindent
$Z_n$, sont les zéros de la fonction sauf en zéro. $m$ est le degré du zéro en zéro (si la fonction en possède un). $P_{\rho}(z)$ est un polynôme de degré $\rho$ et $\rho$ est défini comme la plus petite valeur entière tel que la série $ \sum \frac{r}{|Z_n|}^{1+\rho} $ converge.

\subsection{Démonstration\proof}

\subsection{Exemples}
\subsubsection{Sinus}
Comme la fonction sinus est une fonction entière (voir \ref{entire-sinus}), elle peut donc être factorisée comme un produit de H.W. Calculons d'abord l'ensemble de ses zéros $Z_n$. De part la représentation d'Euler, on doit résoudre :

\[
	\frac{e^{iz}-e^{-iz}}{2i}=0 
\]
On peut s'affranchir du dénominateur qui n'est jamais nul :
\[
	e^{iz}-e^{-iz}=0 
\]
On peut multiplier par $e^{iz}$ qui n'est jamais nul, on obtient:
\[
	e^{2iz}=1 \quad\rightarrow\quad 2z=2k\pi 
\]
Ce qui implique que les zéros sont:
\[
	Z_k=k\pi \quad pour \quad k\in\mathbb{N^*}
\]
On peut réorganiser les zéros et plutôt les exprimer comme 

\[
	Z_n=[1\pi,-1\pi,2\pi,-2\pi,3\pi,-3\pi,....]
\]
Maintenant on va déterminer le degré $\rho$ du polynôme, selon la définition, $\rho$ est la plus petite valeur entière tel que la série $ \sum \frac{r}{|Z_n|}^{1+\rho} $ converge:

\[
\sum_{n=1}^{n=\infty} \frac{r}{|Z_n|}^{1+\rho}=\sum_{n=1}^{n=\infty} \frac{r}{|n\pi|}^{1+\rho}=\frac{2r}{\pi}\sum_{n=1}^{n=\infty} \frac{1}{n}^{1+\rho}
\]
Pour $\rho=0$, la série diverge car on obtient la série harmonique (voir \ref{eq_zeta1}), et avec $\rho=1$ on obtient $\zeta(2)$ (voir \ref{eq_zeta2}) qui vaut $\frac{\pi^2}{6}$ et la série converge. Donc $\rho=1$, Donc $P(z)=Az+B$ est un polynome d'ordre 1 donc les constantes $A $ et $B$ sont à déterminer. Si on reprend la définition:

\[
sin(z)= z e^{Az+B} \prod_{n=1}^\infty E_{1}(\frac{z}{Z_n})
\]
\[
sin(z)= z e^{Az+B} \prod_{n=1}^\infty (1-\frac{z}{Z_n})e^{\frac{z}{Z_n}}
\]
Mais ici comme les zéros sont symétriques, l'astuce consiste à exprimer le produit comme un double produit, en effet: 

\[
\prod_{n=1}^\infty (1-\frac{z}{Z_n})e^{\frac{z}{Z_n}}=\prod_{n=1}^\infty (1-\frac{z}{n\pi})e^{\frac{z}{n\pi}}\prod_{n=1}^\infty (1-\frac{z}{-n\pi})e^{\frac{z}{-n\pi}}
\]
Lorsque on regroupe les produits les exponentielles s'annulent :
\[
\prod_{n=1}^\infty (1-\frac{z}{Z_n})e^{\frac{z}{Z_n}}=\prod_{n=1}^\infty (1-\frac{z}{n\pi}) (1-\frac{z}{-n\pi})=\prod_{n=1}^\infty (1-\frac{z^2}{(n\pi)^2})
\]
Donc 
\[
sin(z)= z e^{Az+B} \prod_{n=1}^\infty (1-\frac{z^2}{(n\pi)^2})
\]
Pour déterminer les constantes $A$ et $B$,on peut utiliser le fait que:
\[
\lim_{z\to 0}\frac{sin(z)}{z}=\lim_{z\to 0} e^{Az+B} \prod_{n=1}^\infty (1-\frac{z^2}{(n\pi)^2})
\]
\[
1=e^{B} \quad donc\quad B=0
\]
Et on peut utiliser le fait que $sin(-z)=-sin(z)$ :

\[
-z e^{-Az} \prod_{n=1}^\infty (1-\frac{z^2}{(n\pi)^2})=-z e^{Az} \prod_{n=1}^\infty (1-\frac{z^2}{(n\pi)^2})
\]
\[
-z e^{Az}=-z e^{-Az}
\]
\[
e^{2Az}=1 \quad donc\quad A=0
\]
Par conséquent on obtient la formule :
\label{eq_sinus_product}
\[
\boxed{sin(z)= z \prod_{n=1}^\infty \left(1-\frac{z^2}{(n\pi)^2}\right)}
\]

\subsubsection{Cosinus}
Avec la même méthode, on part de l'équation d'Euler:
\[
	cos(z)=\frac{e^{iz}+e^{-iz}}{2}=0 
\]
\[
	e^{iz}+e^{-iz}=0 
\]
\[
	e^{2iz}=-1 
\]
\[
	2z=(1+2k)\pi
\]
Ce qui implique que les zéros sont:
\[
	Z_k=\frac{1+2k}{2}\pi \quad pour \quad k\in\mathbb{N^*}
\]
On peut réorganiser les zéros et plutôt les exprimer comme 

\[
	Z_n=\left[\frac{\pi}{2},-\frac{\pi}{2},\frac{3\pi}{2},-\frac{3\pi}{2},\frac{5\pi}{2},-\frac{5\pi}{2},...\right]
\]
Maintenant on va déterminer le degré $\rho$ du polynome, selon la définition, $\rho$ est la plus petite valeur entière tel que la série $ \sum \frac{r}{|Z_n|}^{1+\rho} $ converge:

\[
\sum_{n=1}^{n=\infty} \frac{r}{|Z_n|}^{1+\rho}= \frac{2}{\pi}\left( \sum_{n=0}^{n=\infty} \frac{2r}{|1+2n|}^{1+\rho}+\sum_{n=1}^{n=\infty} \frac{2r}{|1-2n|}^{1+\rho}\right)
\]
\[
=\frac{2}{\pi}\left(\sum_{n=0}^{n=\infty} \frac{2r}{|1+2n|}^{1+\rho}+\sum_{n=0}^{n=\infty} \frac{2r}{|1-2(n+1)|}^{1+\rho}\right)
\]
\[
=\frac{2}{\pi}\left(\sum_{n=0}^{n=\infty} \frac{2r}{|1+2n|}^{1+\rho}+\sum_{n=0}^{n=\infty} \frac{2r}{|1+2n|}^{1+\rho}\right)
\]
Finalement 
\[
\sum_{n=1}^{n=\infty} \frac{r}{|Z_n|}^{1+\rho}=\frac{8r}{\pi}\left(\sum_{n=0}^{n=\infty} \frac{1}{1+2n}^{1+\rho}\right)
\]
Pour $\rho=0$, la série diverge car la somme est supérieure à l'intégrale correspondante :
\[
\sum_{n=0}^{n=\infty} \frac{1}{1+2n}> \int_{n=0}^{n=\infty} \frac{1}{1+2x} dx
\]
\[
\sum_{n=0}^{n=\infty} \frac{1}{1+2n}> \int_{n=0}^{n=\infty} \frac{1}{1+2x} dx
\]

\[
\sum_{n=0}^{n=\infty} \frac{1}{1+2n}> \lim_{x\to\infty}ln(1+2x)>\infty
\]
Par contre si $\rho=1$ la suite converge
\[
\frac{8r}{\pi}\left(\sum_{n=0}^{n=\infty} \frac{1}{(1+2n)^2}\right)=...=\frac{8r}{\pi} \frac{\pi^2}{8}=r\pi
\]
Donc $cos(z)$ peut s'écrire sous la forme:
\[
cos(z)= e^{Az+B} \prod_{n=1}^\infty E_{1}\left(\frac{z}{Z_n}\right)
\]
Comme on a des zéros en négatif et en positif on double le produit comme avec le sinus:
\[
cos(z)= e^{Az+B} \prod_{n=0}^\infty E_{1}\left(\frac{2z}{(1+2n)\pi}\right)\prod_{n=0}^\infty E_{1}\left(-\frac{2z}{(1+2n)\pi}\right)
\]
\[
cos(z)= e^{Az+B} \prod_{n=0}^\infty (1-\frac{2z}{(1+2n)\pi})e^{\frac{2z}{(1+2n)\pi}}\prod_{n=0}^\infty (1+\frac{2z}{(1+2n)\pi})e^{-\frac{2z}{(1+2n)\pi}}
\]
Si on regroupe les deux produits, les exponentielles disparaissent et on obtient:
\[
cos(z)= e^{Az+B} \prod_{n=0}^\infty (1-\frac{2z}{(1+2n)\pi}) (1+\frac{2z}{(1+2n)\pi})
\]
Ce qui se simplifie en :
\[
cos(z)= e^{Az+B} \prod_{n=0}^\infty \left(1-\frac{4z^2}{(1+2n)^2\pi^2}\right)
\]
Déterminons maintenant A et B,utilisons d'abord:
\[
	cos(0)=1
\]
\[
	e^B=1 \quad donc \quad B=0
\]
Puis 
\[
cos(-z)=cos(z)
\]
\[
e^{-Az}=e^{Az} 
\]
\[
e^{2Az}=1 \quad donc \quad A=0
\]
Finalement
\label{eq_cosinus_product}
\[
\boxed{cos(z)= \prod_{n=0}^\infty \left(1-\frac{4z^2}{(1+2n)^2\pi^2}\right)}
\]

\newpage
\section{Séries de Fourier}
\subsection{Définition}

Toute fonction \textbf{périodique} peut se décomposer en une somme finie ou infinie de fonctions sinusoïdales:

\label{eq_serie_fourier}
\begin{equation}
\boxed{f(z)=\sum_{n=-\infty}^{\infty}C_n e^{-i2\pi n z}}
\end{equation}

\noindent
Les coefficients de Fourier $C_n$ (complexes ou réels) s'expriment par :
\begin{equation}
\boxed{
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)e^{-i \frac{2\pi}{T}n z} dz\\
}
\end{equation}

\subsection{Théorème de Parseval\proof}

\label{eq_parseval}
\begin{equation}
\boxed{
\sum_{n=-\infty}^{\infty}|C_n|^2=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} |f(z)|^2 dz}
\end{equation}

\noindent
\notes{ C'est une méthode puissante pour obtenir la valeur d'une somme infinie. Le théorème de Parseval énonce la conservation de l'énergie sur les deux représentations.}

\subsection{Formule sommatoire de Poisson}
\label{eq_poisson_formula}

Prenons $f(z)$ une fonction $T$-périodique, définit comme suit : 
\[
f(z)=\sum_{k=-\infty}^{\infty}g(z+kT)
\]
Calculons les coefficients de Fourier $C_n$ de cette fonction $f(z)$:
\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)e^{-i \frac{2\pi}{T}n z} dz
\]
\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} \sum_{k=-\infty}^{\infty}g(z+kT)e^{-i \frac{2\pi}{T}n z} dz
\]
Mais 
\[
e^{-i \frac{2\pi}{T}n (z+kT)}=e^{-i (\frac{2\pi}{T}n z + 2nk\pi)}=e^{-i \frac{2\pi}{T}n z}
\]
Donc on peut remplacer par :
\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} \sum_{k=-\infty}^{\infty}g(z+kT)e^{-i \frac{2\pi}{T}n (z+kT)} dz
\]
Et on peut aussi changer les bornes entre $0$ et $T$ cela ne  change pas la valeur de l'intégrale:
\[
C_n=\frac{1}{T}\int_{0}^{T} \sum_{k=-\infty}^{\infty}g(z+kT)e^{-i \frac{2\pi}{T}n (z+kT)} dz
\]
Maintenant si on pose $s=z+kT$, on obtient:
\[
C_n=\frac{1}{T}\int_{kT}^{T+kT} \sum_{k=-\infty}^{\infty}g(s)e^{-i \frac{2\pi}{T}n s} ds
\]
Si on sort la somme de l'intégrale
\[
C_n=\frac{1}{T}\sum_{k=-\infty}^{\infty}\int_{kT}^{(k+1)T} g(s)e^{-i \frac{2\pi}{T}n s} ds
\]
Finalement on peut fusionner la somme et l'intégrale:
\[
C_n=\frac{1}{T}\int_{-\infty}^{\infty} g(s)e^{-i \frac{2\pi}{T}n s} ds
\]
Maintenant
\[
f(z)=\sum_{n=-\infty}^{\infty}C_n e^{-i2\pi n z}
\]
\[
\sum_{k=-\infty}^{\infty}g(z+kT)=\sum_{n=-\infty}^{\infty}\left(\frac{1}{T}\int_{-\infty}^{\infty} g(s)e^{-i \frac{2\pi}{T}n s} ds\right) e^{-i2\pi n z}
\]
Finalement on obtient la forme générale de la formule de sommation de poisson:
\begin{equation}
\boxed{\sum_{k=-\infty}^{\infty}g(z+kT)=\frac{1}{T}\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} g(s)e^{-i \frac{2\pi}{T}n s} ds\right)e^{-i2\pi n z}}
\end{equation}
Si on prend $T=1$ et $z=0$, on obtient une forme plus simple:
\begin{equation}
\boxed{\sum_{k=-\infty}^{\infty}g(k)=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} g(s)e^{-i 2\pi n s} ds\right)}
\end{equation}

\subsection{Exemples}

\subsubsection{Vaguelette}
Soit 
$f(z)=\left\{\begin{array}{c}-1\quad si\quad z\in[-\pi,0]\\1\quad si\quad z\in]0,\pi]\end{array}\right.$  de période $2\pi$

\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)e^{-i \frac{2\pi}{T}n z} dz \quad\quad\quad et \quad\quad\quad
C_0=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)dz
\]
\[
C_n=\frac{1}{2\pi}\int_{-\pi}^{\pi} f(z)e^{-i n z} dz \quad\quad\quad et \quad\quad\quad
C_0=\frac{1}{2\pi}\int_{-\pi}^{\pi} f(z)dz=0
\]
On a alors :
\[
	C_n=\frac{1}{2\pi}\left(\int_{0}^{\pi} e^{-i n z} dz - \int_{-\pi}^{0} e^{-i n z} dz\right)
\]

\[
	C_n=\frac{1}{2\pi}\left(\left[\frac{e^{-i n z}}{-i n}\right]_{0}^{\pi} - \left[ \frac{e^{-i n z}}{-i n} \right]_{-\pi}^{0} \right)
\]

\[
	C_n=\frac{i}{2n\pi}\left( ((-1)^n-1) - (1-(-1)^n) \right)
\]

\[
	C_n=\frac{i((-1)^n-1)}{n\pi}
\]
Le théorème de Parseval donne alors:
\[
\sum_{n=-\infty}^{\infty}|C_n|^2=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} |f(z)|^2 dz \quad\quad Soit \quad\quad \sum_{n=-\infty}^{\infty}|\frac{i((-1)^n-1)}{n\pi}|^2=\frac{1}{2\pi}\int_{-\pi}^{\pi} 1 dz
\]
\[
\frac{1}{\pi^2}\sum_{n=-\infty}^{\infty}\frac{|((-1)^n-1)|^2}{n^2}=\frac{1}{\pi^2}\sum_{n=-\infty}^{\infty}\frac{((-1)^2n+1-2(-1)^n)}{n^2}=1
\]
\[
\frac{1}{\pi^2}\sum_{n=-\infty}^{\infty}\frac{2(1-(-1)^n)}{n^2}=\frac{2}{\pi^2}\sum_{n=-\infty}^{\infty}\frac{1-(-1)^n}{n^2}=1
\]
\[
\sum_{n=-\infty}^{\infty}\frac{1}{n^2}-\sum_{n=-\infty}^{\infty}\frac{(-1)^n}{n^2}=\frac{\pi^2}{2}
\]
\[
\sum_{n=1}^{\infty}\frac{1}{n^2}-\sum_{n=1}^{\infty}\frac{(-1)^n}{n^2}=\frac{\pi^2}{4}
\]
Et comme $\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6}$ voir \ref{eq_zeta2}, on déduit que :

\begin{equation}
\boxed{\sum_{n=1}^{\infty}\frac{(-1)^n}{n^2}=-\frac{\pi^2}{12}}
\end{equation}

\subsubsection{Puissances Généralisation}

Soit $f(z)=z^k$ sur $[-\pi,\pi]$ de période $2\pi$

\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)e^{-i \frac{2\pi}{T}n z} dz \quad\quad\quad et \quad\quad\quad
C_0=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)dz
\]
\[
C_n=\frac{1}{2\pi}\int_{-\pi}^{\pi} z^ke^{-i n z} dz \quad\quad\quad et \quad\quad\quad
C_0=\frac{1}{2\pi}\int_{-\pi}^{\pi} z^kdz
\]
On va calculer d'abord $C_0$
\[
C_0=\frac{1}{2\pi}\left[ \frac{z^{k+1}}{k+1} \right]_{-\pi}^{\pi}=\frac{1}{2\pi}(\frac{\pi^{k+1}}{k+1}-\frac{(-\pi)^{k+1}}{k+1})
\]
\[
C_0=\frac{\pi^{k+1}}{2\pi(k+1)}(1-(-1)^{k+1})
\]
\[
\boxed{C_0=\frac{\pi^{k}}{2(k+1)}(1+(-1)^{k}) \quad c.a.d \quad
C_0=\left\{\begin{array}{lll}
0&Si\ k\ est\ impair\\
\frac{\pi^{k}}{k+1}&Sinon\\
\end{array}
\right.}
\]
Maintenant on va essayer de calculer les $C_n$ :
\[
C_n=\frac{1}{2\pi}\int_{-\pi}^{\pi} z^ke^{-i n z} dz
\]
Avec une intégration par partie, on a :
\[
C_n=\frac{1}{2\pi}\left(\left[ z^k \frac{e^{-i n z}}{-i n}\right]_{-\pi}^{\pi} - k\int_{-\pi}^{\pi} z^{k-1} \frac{e^{-i n z}}{-i n} dz\right)
\]
On peut alors continuer en intégrant par partie : 
\[
C_n=\frac{1}{2\pi}\left(\left[ z^k \frac{e^{-i n z}}{-i n}\right]_{-\pi}^{\pi} - k\left(\left[ z^{k-1} \frac{e^{-i n z}}{(-i n)^2}\right]_{-\pi}^{\pi}-(k-1)\int_{-\pi}^{\pi} z^{k-2}\frac{e^{-i n z}}{(-i n)^2} dz  \right)\right)
\]
On peut alors continuer en intégrant par partie... : 
\[
C_n=\frac{1}{2\pi}\left(\left[ z^k \frac{e^{-i n z}}{-i n}\right]_{-\pi}^{\pi} - k\left(\left[ z^{k-1} \frac{e^{-i n z}}{(-i n)^2}\right]_{-\pi}^{\pi}-(k-1) \left( \left[ z^{k-2} \frac{e^{-i n z}}{(-i n)^3} \right]_{-\pi}^{\pi}-(k-2)\int_{-\pi}^{\pi}z^{k-3} \frac{e^{-i n z}}{(-i n)^3}  dz  \right)\right)\right)
\]
Si on développe et que on remplace $e^{in\pi}=e^{-in\pi}=(-1)^n$ :
\[
C_n=\frac{1}{2\pi}\left(\frac{(-1)^n}{-in}\left[ z^k \right]_{-\pi}^{\pi} - \frac{k(-1)^n}{(-i n)^2}\left[ z^{k-1} \right]_{-\pi}^{\pi}+\frac{k(k-1)(-1)^n}{(-in)^3}  \left[ z^{k-2} \right]_{-\pi}^{\pi}-k(k-1)(k-2)\int_{-\pi}^{\pi}z^{k-3} \frac{e^{-i n z}}{(-i n)^3}  dz \right)
\]
On pose $K_j= \frac{k!}{(k-j)!}$ et que on factorise par $(-1)^n $
\[
C_n=\frac{(-1)^n}{2\pi}\left(\frac{K_0}{-in}\left[ z^k \right]_{-\pi}^{\pi} - \frac{K_1}{(-i n)^2}\left[ z^{k-1} \right]_{-\pi}^{\pi}+\frac{K_2}{(-in)^3}  \left[ z^{k-2} \right]_{-\pi}^{\pi}-K_3\int_{-\pi}^{\pi}z^{k-3} \frac{e^{-i n z}}{(-i n)^3}  dz \right)
\]
On pourrait continuer jusque avoir :
\[
C_n=\frac{(-1)^n}{2\pi}\left(\frac{K_0}{-in}\left[ z^k \right]_{-\pi}^{\pi} - \frac{K_1}{(-i n)^2}\left[ z^{k-1} \right]_{-\pi}^{\pi}+\frac{K_2}{(-in)^3}  \left[ z^{k-2} \right]_{-\pi}^{\pi}-\frac{K_3}{(-in)^4}\left[z^{k-3}\right]_{-\pi}^{\pi} +...+(-1)^k K_k \int_{-\pi}^{\pi} \frac{e^{-i n z}}{(-i n)^k}  dz \right)
\]
On pose $K'_j= (-1)^j\frac{k!}{(k-j)!}$ et que on intègre le dernier terme :

\[
C_n=\frac{(-1)^n}{2\pi}\left(\frac{K'_0}{-in}\left[ z^k \right]_{-\pi}^{\pi} + \frac{K'_1}{(-i n)^2}\left[ z^{k-1} \right]_{-\pi}^{\pi}+\frac{K'_2}{(-in)^3}  \left[ z^{k-2} \right]_{-\pi}^{\pi}+\frac{K'_3}{(-in)^4}\left[z^{k-3}\right]_{-\pi}^{\pi} +...+K'_k \left[ \frac{e^{-i n z}}{(-i n)^{k+1}} \right]_{-\pi}^{\pi} \right)
\]
Le dernier terme est donc nul donc on peut écrire:
\[
C_n=\frac{(-1)^n}{2\pi}\left(\frac{K'_0}{-in}\left[ z^k \right]_{-\pi}^{\pi} + \frac{K'_1}{(-i n)^2}\left[ z^{k-1} \right]_{-\pi}^{\pi}+\frac{K'_2}{(-in)^3}  \left[ z^{k-2} \right]_{-\pi}^{\pi}+\frac{K'_3}{(-in)^4}\left[z^{k-3}\right]_{-\pi}^{\pi} +...+\frac{K'_{k-1}}{(-in)^k}\left[z\right]_{-\pi}^{\pi}\right)
\]
On peut alors écrire $\left[ z^k \right]_{-\pi}^{\pi}=\pi^k(1-(-1)^k)$, ainsi :

\[
C_n=\frac{(-1)^n}{2\pi}\left(\frac{K'_0}{-in}\pi^k(1-(-1)^k) + \frac{K'_1}{(-i n)^2}\pi^{k-1}(1-(-1)^{k-1})+...+\frac{K'_{k-1}}{(-in)^k}\pi(1-(-1))\right)
\]
Maintenant si on pose :
\[
A_j=\frac{\pi^{k-j}}{(-in)^{j+1}}K'_j=\frac{\pi^{k-j}(-1)^j}{(-in)^{j+1}}\frac{k!}{(k-j)!}=\frac{\pi^{k-j}(-1)^j}{(in)^{j+1}(-1)^{j+1}}\frac{k!}{(k-j)!}=-\frac{\pi^{k-j}}{(in)^{j+1}}\frac{k!}{(k-j)!}
\]
On obtient:
\[
C_n=\frac{(-1)^n}{2\pi}\left(A_0(1-(-1)^k) + A_1(1-(-1)^{k-1})+...+A_{k-1}(1-(-1))\right)
\]
Et 
\[
C_n=\frac{(-1)^n}{2\pi}\sum_{j=0}^{k-1} A_j (1-(-1)^{k-j})
\]
Donc 
\[
\boxed{C_n=\frac{(-1)^n}{2\pi}\sum_{j=0}^{k-1} \frac{\pi^{k-j}}{(in)^{j+1}}\frac{k!}{(k-j)!} ((-1)^{k-j}-1)}
\]
Vérification pour $k=2$
\[
C_n=\frac{(-1)^n}{2\pi}\sum_{j=0}^{1} \frac{\pi^{2-j}}{(in)^{j+1}}\frac{2!}{(2-j)!} ((-1)^{2-j}-1)
\]
\[
C_n=\frac{(-1)^n}{2\pi} \left( \frac{\pi^{2}}{(in)^{1}}\frac{2!}{2!} ((-1)^{2}-1)+\frac{\pi^{1}}{(in)^{2}}\frac{2!}{(1)!} ((-1)^{1}-1) \right)
\]
\[
C_n=\frac{(-1)^n}{2\pi} \left( -4\frac{\pi}{(in)^{2}}  \right)=\frac{(-1)^n}{2\pi} \left( 4\frac{\pi}{n^{2}}  \right)=\frac{2(-1)^n}{n^{2}}
\]
On obtient bien le même résultat que lors du calcul de $\zeta(4)$ ( voir \ref{eq_zeta4} ).\\

\noindent
On va maintenant essayer de simplifier l'expression dans la somme :
\[
C_n=\frac{(-1)^n}{2\pi}\sum_{j=0}^{k-1} \frac{\pi^{k-j}}{(in)^{j+1}}\frac{k!}{(k-j)!} ((-1)^{k-j}-1)
\]
en posant $m=k-j$:
\[
C_n=\frac{(-1)^n}{2\pi}\sum_{m=k}^{1} \frac{\pi^{m}}{(in)^{k-m+1}}\frac{k!}{m!} ((-1)^{m}-1)
\]
Et si on sort $\frac{k!}{(in)^{k}}$
\[
C_n=\frac{(-1)^n}{2\pi}\frac{k!}{(in)^{k}}\sum_{m=k}^{1} \frac{\pi^{m}}{(in)^{1-m}}\frac{1}{m!} ((-1)^{m}-1)
\]
On peut retourner les bornes :
\[
C_n=\frac{(-1)^n}{2\pi}\frac{k!}{(in)^{k}}\sum_{m=1}^{k} (in)^{m-1}\frac{\pi^{m}}{m!} ((-1)^{m}-1)
\]
Si on sort un $\frac{1}{in}$ de plus :
\[
C_n=\frac{k!}{2\pi}\frac{(-1)^n}{(in)^{k+1}}\sum_{m=1}^{k}\frac{(in\pi)^{m}}{m!} ((-1)^{m}-1)
\]
Vérification pour $k=2$
\[
C_n=\frac{2!}{2\pi}\frac{(-1)^n}{(in)^{2+1}}\sum_{m=1}^{2}\frac{(in\pi)^{m}}{m!} ((-1)^{m}-1)
\]
\[
C_n=\frac{1}{\pi}\frac{(-1)^n}{(in)^{3}}\sum_{m=1}^{2}\frac{(in\pi)^{m}}{m!} ((-1)^{m}-1)
\]
\[
C_n=\frac{1}{\pi}\frac{(-1)^n}{(in)^{3}} \frac{(in\pi)}{1!} (-2)=-2\frac{(-1)^n}{(in)^{2}}=\frac{2(-1)^n}{n^{2}}
\]
On obtient bien le bon résultat, mais on peut encore simplifier car tout les termes ou $m$ est pair sont nuls, et les termes impairs sont doublés négativement:

\[
C_n=-\frac{k!}{\pi}\frac{(-1)^n}{(in)^{k+1}}\sum_{m=0}^{\floor{\frac{1}{2}(k-1)}}\frac{(in\pi)^{2m+1}}{(2m+1)!}
\]
On peut aussi sortir un $in\pi$
\[
C_n=-k!\frac{(-1)^n}{(in)^{k}}\sum_{m=0}^{\floor{\frac{1}{2}(k-1)}}\frac{(in\pi)^{2m}}{(2m+1)!}
\]
Et comme $i^{2m}=-1$
\[
\boxed{C_n=k!\frac{(-1)^n}{(in)^{k}}\sum_{m=0}^{\floor{\frac{1}{2}(k-1)}}\frac{(n\pi)^{2m}}{(2m+1)!}}
\]

\noindent
On peut alors construire la table des $C_0$ et $C_n$ en fonction de $k$ :

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
$k$&$C_0=\left\{\begin{array}{lll}
0&Si\ k\ est\ impair\\
\frac{\pi^{k}}{k+1}&Sinon\\
\end{array}
\right.$&$C_n=k!\frac{(-1)^n}{(in)^{k}}\sum_{m=0}^{\floor{\frac{1}{2}(k-1)}}\frac{(n\pi)^{2m}}{(2m+1)!}$&$\sum_{n=-\infty}^{\infty}|C_n|^2$\\
\hline
$1$&$0$&$ 1!\frac{(-1)^n}{(in)^{1}} \frac{1}{1!}=\boxed{\frac{(-1)^n}{in}}$&$2\sum_{n=1}^{\infty}\frac{1}{n^2}$\\
\hline
$2$&$\frac{\pi^{2}}{3}$&$ 2!\frac{(-1)^n}{(in)^{2}} \frac{1}{1!}=\boxed{2\frac{(-1)^n}{(in)^2}}$&$\frac{\pi^2}{9}+8\sum_{n=1}^{\infty}\frac{1}{n^4}$\\
\hline
$3$&$0$&$ 3!\frac{(-1)^n}{(in)^{3}} \left(\frac{1}{1!}+\frac{(n\pi)^2}{3!}\right)=\boxed{\frac{(-1)^n}{(in)^3}\left(6+(n\pi)^2\right)}$&$ 2\sum_{n=1}^{\infty}\frac{36+12(n\pi)^2+(n\pi)^4}{n^9} $\\
\hline
$4$&$\frac{\pi^{4}}{5}$&$ 4!\frac{(-1)^n}{(in)^{4}} \left(\frac{1}{1!}+\frac{(n\pi)^2}{3!}\right)=\boxed{\frac{(-1)^n}{(in)^4}\left(6+(n\pi)^2\right)6}$&$ 72\sum_{n=1}^{\infty}\frac{36+12(n\pi)^2+(n\pi)^4}{n^{16}}$\\
\hline
\end{tabular}
\end{center}

\noindent
Les premiers termes permettront de calculer $\zeta(2)$ et $\zeta(4)$ grâce au théorème de Parseval, sur la conservation de l'énergie. Les termes suivant ont des expressions plus complexes.

\newpage
\section{Transformations intégrales}
\label{eq_transform_int}
Considérons l'espace $\Omega$ ainsi que l'ensemble des fonctions de cet espace, et $I=[a,b]$ un intervalle de cet espace. Les transformations intégrales sont toutes les transformations du type:

\begin{equation}
\boxed{\hat{f}(s)=\int_{a}^{b}K(s,t) f(t) dt}
\end{equation}

$K$ est appelé le noyau de la transformation (Kernel). 
Pour comprendre une telle transformation il est utile de faire l'analogie avec l'algèbre linéaire. Imaginons que les deux fonctions $f$ et $g$ soient des vecteurs, alors leur produit scalaire sur $I$ s'écrit :
\begin{equation}
\boxed{\braket{f|g}=\int_a^b f(t)g(t)^* dt}
\end{equation}
Le produit scalaire mesure la projection de $g(t)$ sur $f(t)$. Et si $<f,g>=0$ alors les fonctions sont dites orthogonales sur $I$. On comprend alors que le noyau représente une nouvelle base, dans laquelle, on va exprimer $f(t)$ (En dimension finie le noyau serait une matrice). $K(s)$ sont alors les vecteurs de la base, et la transformation peut encore s'écrire:
\begin{equation}
\boxed{\hat{f}(s)=\braket{K(s)|f}}
\end{equation}

On comprend alors qu'il peut exister beaucoup de transformations de ce type. (ex : Transformations de Fourier, Laplace, Mellin,...). Chacune ayant son propre noyau.\\

\textbf{Mais pour qu'une telle transformation fonctionne il faut montrer que le noyau constitue bien une base. C'est à dire qu'il puisse engendrer tout l'espace $\Omega$ par combinaison linéaire de ses vecteurs de base $K(s)$}. Une condition suffisante serait de montrer que aucun vecteur de base n'est colinéaire à un autre, mais on choisit généralement des bases \textbf{orthogonales} voir même \textbf{orthonormales} qui satisfont les contraintes:

\begin{equation}
\boxed{\left\{
\begin{array}{lll}
Orthogonalite: & \forall s_1\ne s_2 & \braket{K(s_1)|K(s_2)}=0\\
Normalite: &\forall s &  \braket{K(s)|K(s)}=1\\
\end{array}
\right.}
\end{equation}

En effet, \textbf{si le noyau $K(s,t)$ est une base orthonormée de l'espace} $\Omega$ alors il existe un inverse noté $K^+(s,t)$ et :
\begin{equation}
\boxed{K^+(s,t)=K(t,s)^*}
\end{equation}
Autrement dit comme dans le cas matricielle l'inverse est donné par \textbf{la matrice adjointe} (complexe conjuguée et transposée). l'opération inverse donne alors :
\[
f(s)=\int_{a}^{b}K(t,s)^* \hat{f}(t) dt
\]
Mais on re-inverse généralement le nom des variables pour avoir une notation plus cohérente:
\begin{equation}
\boxed{f(t)=\int_{a}^{b}K(s,t)^* \hat{f}(s) ds}
\end{equation}

On pourra aussi introduire l'opérateur transformation intégrale $\op{K}$, tel que:

\[
	\hat{f}(s) = \applyop{K}{f(t)}(s)
\]

\newpage
\section{Transformée de Fourier}

\subsection{Définitions}
\label{eq_fourier_def}
La transformée de Fourier généralise la notion de série de Fourier pour les fonctions non-périodiques \textbf{à énergie finie} (Cela revient en effet à considérée une fonction de période $T$ et à faire tendre cette période vers l'infini).\\

La transformée de Fourier est une transformation intégrale (voir \ref{eq_transform_int}), sur l'espace des fonctions de carré sommable $L^2$,sur l'intervalle $I=[-\infty,\infty]$, et de noyau $\boxed{K(s,t)=e^{-i t s}}$. Une fonction $f$ est de carrée sommable, si et seulement si son énergie est finie, c'est à dire:
\[
\braket{f|f}=\int_{-\infty}^{\infty}|f(t)|^2 dt<\infty
\]
La transformée de Fourier $\hat{f}(s)$ d'une fonction $f(t)$ avec $s,t \in\mathbb{R}$, s'écrit :
\begin{equation}
\boxed{\hat{f}(s)=\int_{-\infty}^{\infty}f(t)e^{-i s t} dt}
\end{equation} 
La transformée inverse s'obtient en inversant le signe dans l'intégrale :
\begin{equation}
\boxed{f(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}\hat{f}(s)e^{i s t} ds}
\end{equation} 
On introduira l'opérateur transformée de Fourier $\op{F}$ tel que:
\[
\begin{array}{l}
\hat{f}(s)=\applyop{F}{f(t)}(s)\\
f(t)=\applyopp{F}{-1}{\hat{f}(s)}(t)\\
\end{array}
\]

\notes{Il existe beaucoup de conventions différentes. On rencontre fréquemment celle issue de la physique $s=2\pi\nu$ pour avoir une analogie avec des fréquences exprimées en $Hz$ si $f(t)$ est un signal temporel ou $t$ est le temps en secondes.}

\subsection{Extension aux distributions}

\subsubsection{L'impulsion de Dirac : $\delta$ }
\label{eq_fourier_dirac}
L'impulsion de Dirac notée $\delta(t)$, n'est pas une fonction au sens usuel du terme, c'est une distribution, mais nous ne développerons pas la théorie des distributions ici. La fonction $\delta$ peut être vue, comme une limite d'une fonction gaussienne (Ce n'est pas la seule représentation possible), tel que :

\[
\delta(t)=\lim_{\sigma\to 0} \frac{e^{\frac{t^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}
\]

L'intégrale de cette fonction sur un intervalle quelconque contenant zéro vaux un, car l'aire sous la courbe d'une gaussienne normée vaut 1 :

\[
	\int_{-\infty}^{\infty} \delta(t) dt = 1
\]

Cette distribution permet d'extraire la valeur d'une fonction en un point particulier, car :

\[
	\int_{-\infty}^{\infty} \delta(t-a) f(t) dt = f(a)
\]

Et si on calcul sa transformée de Fourier et la transformée de Fourier inverse, on obtient trivialement avec la propriété précédente que :

\[
	\int_{-\infty}^{\infty} \delta(t-a) e^{-ist} dt = e^{-isa} \quad\quad et \quad\quad \frac{1}{2\pi}\int_{-\infty}^{\infty} \delta(s-a) e^{ist} ds = \frac{e^{iat}}{2\pi}
\]

Finalement :
\begin{equation}
\boxed{\applyop{F}{\delta(t-a)}{(s)} =e^{-isa} \quad\quad et \quad\quad \applyopp{F}{-1}{\delta(s-a)}{(t)}=\frac{e^{iat}}{2\pi}}
\end{equation}
Et particulièrement pour $a=0$ :
\begin{equation}
\boxed{\applyop{F}{\delta(t)}{(s)} =\mathbbm{1}(s) \quad\quad et \quad\quad \applyopp{F}{-1}{\delta(s)}{(t)}=\frac{\mathbbm{1}(t)}{2\pi} }
\end{equation}

\subsubsection{La fonction constante (L'intégrale de Fourier): méthode 1}
\label{eq_fourier_int1}
Si on prend $f(t)=\mathbbm{1}(t)$, avec $\mathbbm{1}(t)=1$ la fonction constante, la transformée de Fourier s'écrit :
\[
\applyop{F}{\mathbbm{1}(t)}{(s)}=\int_{-\infty}^{\infty} \mathbbm{1}(t) e^{-i s t} dt
\]
Cette transformée de fourrier \textbf{n'existe pas au sens des fonctions} car pour que cette intégrale existe au sens usuel, il faut que $f(t)$ soit de carré sommable. Mais d'après le résultat précédent (voir \ref{eq_fourier_dirac}), nous avons obtenu que :

\[
\applyopp{F}{-1}{\delta(s)}{(t)}=\frac{\mathbbm{1}(t)}{2\pi}
\]
Soit
\[
\frac{1}{2\pi}\applyop{F}{\mathbbm{1}(t)}{(s)} =\delta(s)
\]

Finalement:
\begin{equation}
\boxed{\applyop{F}{\mathbbm{1}(t)}{(s)}=\int_{-\infty}^\infty e^{-i s t} dt=2\pi \delta(s)}
\end{equation}

\notes{Ce résultat peut paraitre surprenant au premier abord, car l'intégrale ne converge pas au sens usuel du terme, mais au sens des distributions.}

\subsubsection{La fonction constante (L'intégrale de Fourier): méthode 2}
\label{eq_fourier_int2}
Une seconde méthode pour démontrer le résultat précédent est de considérer la représentation en sinus cardinal de l'impulsion de Dirac:

\[
\delta(t)=\lim_{R\to\infty} \frac{sin(Rt)}{t\pi}
\]

Or si on exprime la transformée de Fourier de la fonction constante comme une limite, on obtient:

\[
\applyop{F}{\mathbbm{1}(t)}{(s)}=\lim_{R\to\infty} \int_{-R}^R e^{-i s t} dt
\]

Or l'intégrale est triviale à calculer:
\[
\applyop{F}{\mathbbm{1}(t)}{(s)}=\lim_{R\to\infty} \left[ \frac{e^{-ist}}{is} \right]_{-R}^{R}=\lim_{R\to\infty} \frac{e^{-isR}-e^{isR}}{is}
\]

Si on fait apparaitre les termes manquants pour obtenir cardinal, en multipliant et divisant par $2\pi$, on obtient:
\[
\applyop{F}{\mathbbm{1}(t)}{(s)}=\lim_{R\to\infty} 2\pi\frac{e^{-isR}-e^{isR}}{2i\pi s}=2\pi \lim_{R\to\infty} \frac{sin(Rs)}{s \pi}
\]

Finalement:
\begin{equation}
\boxed{\applyop{F}{\mathbbm{1}(t)}{(s)}=\int_{-\infty}^\infty e^{-i s t} dt=2\pi \delta(s)}
\end{equation}

\subsubsection{La fonction cosinus}
\label{eq_fourier_cos}
Partons de la définition :
\[
\applyop{F}{cos(at)}{(s)} = \int_{-\infty}^{\infty} cos(at)e^{-ist}dt
\]
Si on prend la représentation d'Euler du cosinus :
\[
\applyop{F}{cos(at)}{(s)} = \int_{-\infty}^{\infty} \frac{e^{iat}+e^{-iat}}{2}e^{-ist}dt
\]
Soit
\[
\applyop{F}{cos(at)}{(s)} = \int_{-\infty}^{\infty} \frac{e^{it(a-s)}+e^{-it(a+s)}}{2}dt
\]
Si on décompose en deux intégrales :
\[
\applyop{F}{cos(at)}{(s)} = \frac{1}{2}\int_{-\infty}^{\infty} e^{-it(s-a)}dt+\frac{1}{2}\int_{-\infty}^{\infty} e^{-it(a+s)}dt
\]
Or d'après les résultats précédent $\int_{-\infty}^\infty e^{-i s t} dt=2\pi \delta(s)$ (voir \ref{eq_fourier_int1} ou \ref{eq_fourier_int2}), donc:
\[
\applyop{F}{cos(at)}{(s)} = \frac{1}{2}2\pi \delta(s-a)+\frac{1}{2}2\pi \delta(s+a)
\]
Finalement :
\begin{equation}
\boxed{\applyop{F}{cos(at)}{(s)} =\pi ( \delta(s-a)+\delta(s+a) )}
\end{equation}

\subsubsection{La fonction sinus}
\label{eq_fourier_sin}
Partons de la définition :
\[
\applyop{F}{sin(at)}{(s)} = \int_{-\infty}^{\infty} sin(at)e^{-ist}dt
\]
Si on prend la représentation d'Euler du sinus :
\[
\applyop{F}{sin(at)}{(s)} = \int_{-\infty}^{\infty} \frac{e^{iat}-e^{-iat}}{2i}e^{-ist}dt
\]
Soit
\[
\applyop{F}{sin(at)}{(s)} = \int_{-\infty}^{\infty} \frac{e^{it(a-s)}-e^{-it(a+s)}}{2i}dt
\]
Si on décompose en deux intégrales :
\[
\applyop{F}{sin(at)}{(s)} = \frac{1}{2i}\int_{-\infty}^{\infty} e^{-it(s-a)}dt-\frac{1}{2i}\int_{-\infty}^{\infty} e^{-it(a+s)}dt
\]
Or d'après les résultats précédent $\int_{-\infty}^\infty e^{-i s t} dt=2\pi \delta(s)$ (voir \ref{eq_fourier_int1} ou \ref{eq_fourier_int2}), donc:
\[
\applyop{F}{sin(at)}{(s)} = \frac{1}{2i} 2\pi\delta(s-a) -\frac{1}{2i}2\pi\delta(s+a)
\]
\[
\applyop{F}{sin(at)}{(s)} = -i \pi\delta(s-a) +i\pi\delta(s+a)
\]
Finalement :
\begin{equation}
\boxed{\applyop{F}{sin(at)}{(s)} =-i\pi ( \delta(s-a)-\delta(s+a) )}
\end{equation}


\subsection{Preuves des définitions}

\subsubsection{Application double}

Que ce passe t'il si on applique deux fois l'opérateur transformation de  Fourier, calculons:

\[
\applyopp{F}{2}{f(t)}{(r)}=\applyop{F}{ \applyop{F}{f(t)}{(s)} }{(r)}
\]
Soit
\[
\applyopp{F}{2}{f(t)}{(r)}=\int_{-\infty}^{\infty} \left(\int_{-\infty}^{\infty}f(t)e^{-i s t} dt\right)e^{-isr} ds
\]
\[
\applyopp{F}{2}{f(t)}{(r)}=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}f(t)e^{-i s (t+r) }  dt ds
\]
\[
\applyopp{F}{2}{f(t)}{(r)}=\int_{-\infty}^{\infty} f(t) \int_{-\infty}^{\infty}e^{-i s (t+r) }  ds dt
\]
Mais en fait on montre (voir \ref{eq_fourier_int1} et \ref{eq_fourier_int2}) que $\int_{-\infty}^\infty e^{-i s t} dt=2\pi \delta(s)$, donc :

\[
\applyopp{F}{2}{f(t)}{(r)}=2\pi \int_{-\infty}^{\infty} f(t) \delta(t+r)   dt
\]

\[
\applyopp{F}{2}{f(t)}{(r)}=2\pi f(-r)
\]

Ainsi si on introduit l'opérateur $\op{R}$ de réflexion tel que :

\[
\applyop{R}{f(t)}{t}=f(-t)
\]

Alors

\begin{equation}
\boxed{\op{F}^2=2\pi \op{R}}
\end{equation}

Il s'en suit également que 
\begin{equation}
\boxed{\op{F}^4= (2\pi)^2 \op{I}}
\end{equation}

\subsubsection{La formule d'inversion}

D'après le résultat sur l'application double de l'opérateur $\op{F}$, on peut établir l'identité suivante:
\[
\op{F}^2=2\pi \op{R}
\]
Donc en multipliant à droite par $\op{F^{-1}}$, on obtient:
\[
	\op{F}=2\pi \op{R} \op{F^{-1}}
\]
Donc on en déduit que l'opérateur inverse s'exprime par : 
\[
	\op{F^{-1}}=\frac{1}{2\pi}\op{R^{-1}}\op{F}
\]
Mais l'opérateur de réflexion est son propre inverse :
\begin{equation}
	\boxed{\op{F^{-1}}=\frac{1}{2\pi}\op{R}\op{F}}
\end{equation}
Si on applique cet opérateur à $\hat{f}(s)$, on obtient :
\[
\applyopp{F}{-1}{\hat{f}(s)}(t)=\frac{1}{2\pi}\applyop{\op{R}\op{F}}{\hat{f}(s)}{(t)}
\]
Appliquons l'opérateur de Fourier sur la variable $s$, on obtient une fonction de $t$ :
\[
\applyopp{F}{-1}{\hat{f}(s)}(t)=\frac{1}{2\pi}\applyop{\op{R}}{ \int_{-\infty}^{\infty}\hat{f}(s)e^{-i s t} ds }{(t)}
\]
Appliquons l'opérateur de réflexion sur $t$, on obtient une fonction de $t$, ce qui démontre la formule d'inversion:
\begin{equation}
\boxed{\applyopp{F}{-1}{\hat{f}(s)}(t)=\frac{1}{2\pi} \int_{-\infty}^{\infty}\hat{f}(s)e^{i s t} ds}
\end{equation}

\subsubsection{Cas d'un argument complexe}
\label{eq_fourier_cplx}
Si on considère que $s$ est un argument complexe tel que $s=a+bi$ avec $a$ et $b$ des réels, alors la transformée de Fourier s'écrit:

\[
\applyop{F}{f(t)}(a+ib)=\int_{-\infty}^{\infty}f(t)e^{-i (a+ib) t} dt
\]
\[
\applyop{F}{f(t)}(a+ib)=\int_{-\infty}^{\infty}f(t)e^{-iat+bt} dt
\]
\[
\applyop{F}{f(t)}(a+ib)=\int_{-\infty}^{\infty}f(t)e^{bt} e^{-iat} dt
\]
Finalement on obtient que pour un argument complexe:
\begin{equation}
\boxed{\applyop{F}{f(t)}(a+ib)=\applyop{F}{f(t)e^{bt}}(a)}
\end{equation} 

\subsubsection{Orthogonalité du noyau}
\label{eq_fourier_kernel}
Montrons que le noyau est orthogonal soit $s_1\ne  s_2$, calculons :
\[
\braket{K(s_1)|K(s_2)}=\int_{-\infty}^{\infty}K(s_1,t)K(s_2,t)^* dt
\]
Si on remplace par la valeur du noyau $K(s,t)=e^{-i t s}$, on obtient:
\[
\braket{K(s_1)|K(s_2)}=\int_{-\infty}^{\infty}e^{-i t s_1}e^{i t s_2} dt
\]
\[
\braket{K(s_1)|K(s_2)}=\int_{-\infty}^{\infty}e^{i t (s_2-s_1)} dt
\]
On retrouve l'intégrale de Fourier, et on montre (voir \ref{eq_fourier_int1} ou \ref{eq_fourier_int2}), que $\int_{-\infty}^\infty e^{-i s t} dt=2\pi \delta(s)$, donc si $s_1 \ne s_2$, on a bien :
\[
\boxed{\forall s_1\ne s_2 \quad \quad  \scal{K(s_1)}{K(s_2)}=0 \quad avec \quad K(s,t)=e^{-its}}
\]
Le noyau est \textbf{orthogonal}.\\

De plus si $s_1=s_2=s$, on à :
\[
\boxed{\scal{K(s)}{K(s)}=2\pi}
\]

Il suffirait alors pour normer le noyau de prendre $K(s,t)=\frac{e^{-ist}}{\sqrt{2\pi}}$, ce qui est une convention possible, mais que nous n'utiliserons pas ici.

\subsection{Propriétés}

	\subsubsection{Linéarité:}
\label{eq_fourier_lin}
La propriété est triviale :
\[
	\applyop{F}{a f(t)+b g(t)}(s)=\int_{-\infty}^{\infty} (a f(t)+b g(t)) e^{-i s t} dt = a\int_{-\infty}^{\infty} f(t) e^{-i s t} dt + b\int_{-\infty}^{\infty} g(t) e^{-i s t} dt
\]
Donc
\begin{equation}
	\boxed{\applyop{F}{a f(t)+b g(t)}(s)=a\applyop{F}{f(t)}(s)+b\applyop{F}{g(t)}(s)}
\end{equation}
	\subsubsection{Contraction:}
\label{eq_fourier_cont}
\[
	\applyop{F}{f(a t)}(s)=\int_{-\infty}^{\infty} f(a t) e^{-i s t} dt 
\]
Si on effectue le changement de variable $t=at$ avec $dt=\frac{dt}{a}$ :
\[
	\applyop{F}{f(a t)}(s)=\frac{1}{a}\int_{-\infty}^{\infty} f(t) e^{-i s t} dt 
\]	
Donc 
\begin{equation}
	\boxed{\applyop{F}{f(a t)}(s)=\frac{1}{a}\applyop{F}{f(t)}(s)}
\end{equation}
	
	\subsubsection{Translation:}
	\label{eq_fourier_trans}
\[
	\applyop{F}{f(a+t)}(s)=\int_{-\infty}^{\infty} f(a+t) e^{-i s t} dt 
\]	
Si on effectue le changement de variable $t=a+t$ avec $dt=dt$ :
\[
	\applyop{F}{f(a+t)}(s)=\int_{-\infty}^{\infty} f(t) e^{-i s (t-a)} dt 
\]
\[
	\applyop{F}{f(a+t)}(s)=\int_{-\infty}^{\infty} f(t) e^{-i s t}e^{is a} dt =e^{is a}\int_{-\infty}^{\infty} f(t) e^{-i s t}dt 
\]
Donc 
\begin{equation}
	\boxed{\applyop{F}{f(a+t)}(s)=e^{is a}\applyop{F}{f(t)}(s)}
\end{equation}

	\subsubsection{Modulation:}
	\label{eq_fourier_mod}
\[
	\applyop{F}{f(t)e^{iat}}=\int_{-\infty}^{\infty} f(t)e^{iat} e^{-i s t} dt 
\]	
Si on compose les exponentielles:
\[
	\applyop{F}{f(t)e^{iat}}=\int_{-\infty}^{\infty} f(t) e^{-i (s-a)t} dt 
\]	
On obtient un décalage du spectre :
\begin{equation}
	\boxed{\applyop{F}{f(t)e^{iat}}(s)=\applyop{F}{f(t)}(s-a)}
\end{equation}

	\subsubsection{Convolution:}
	\label{eq_fourier_conv}
On définit le produit de convolution par :
\[
	f(t)\star g(t)=\int_{-\infty}^{\infty}f(t-\tau)g(\tau)d\tau=\int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau
\]	
Par conséquent la TF d'un produit de convolution s'écrit:
\[
\applyop{F}{f(t)\star g(t)}(s)=\int_{-\infty}^{\infty}  \left(\int_{-\infty}^{\infty}f(t-\tau)g(\tau)d\tau\right) e^{-i s t} dt
\]
\[
\applyop{F}{f(t)\star g(t)}(s)=\int_{-\infty}^{\infty}  \int_{-\infty}^{\infty}f(t-\tau)g(\tau)e^{-i s t} dt d\tau
\]
\[
\applyop{F}{f(t)\star g(t)}(s)=\int_{-\infty}^{\infty} g(\tau) \int_{-\infty}^{\infty}f(t-\tau)e^{-i s t} dt d\tau
\]
Maintenant si on pose $u=t-\tau$ donc $t=u+\tau$ et $dt=du$
\[
\applyop{F}{f(t)\star g(t)}(s)=\int_{-\infty}^{\infty} g(\tau) \int_{-\infty}^{\infty}f(u)e^{-i s (u+t)} du d\tau
\]
\[
\applyop{F}{f(t)\star g(t)}(s)=\int_{-\infty}^{\infty} g(\tau)e^{-i s \tau} \int_{-\infty}^{\infty}f(u)e^{-i s u} du d\tau
\]
Les intégrales sont alors séparées et on peut écrire:
\[
\applyop{F}{f(t)\star g(t)}(s)=\int_{-\infty}^{\infty} g(\tau)e^{-i s \tau}dt \int_{-\infty}^{\infty}f(u)e^{-i s u} du 
\]
Soit 
\begin{equation}
\boxed{\applyop{F}{f(t)\star g(t)}(s)=\applyop{F}{f(t)}(s)\applyop{F}{g(t)}(s)}
\end{equation}
	
	\subsubsection{Produit:\todo}
\[
\applyop{F}{f(t)g(t)}(s)=\int_{-\infty}^{\infty} f(t)g(t) e^{-i s t} dt
\]	

	\subsubsection{Dérivation:}
	\label{eq_fourier_deriv}
\[
\applyop{F}{f'(t)}(s)=\int_{-\infty}^{\infty} f'(t) e^{-i s t} dt
\]	
Si on fait une intégration par parties, en posant:
\[
\intp{e^{-i s t}}{-i s e^{-i s t}}{f'(t)}{f(t)}{-\infty}{\infty}
\]
On obtient :
\[
\applyop{F}{f'(t)}(s)=\left[ e^{-i s t} f(t) \right]_{-\infty}^{\infty}+i s\int_{-\infty}^{\infty} f(t) e^{-i s t} dt
\]	
Mais $\lim_{\pm\infty}f(t)=0$ car sinon la fonction de serait pas d'énergie finie, par conséquent le crochet est nul en module, et on obtient:

\begin{equation}
\boxed{ \applyop{F}{f'(t)}(s)=i s \applyop{F}{f(t)}(s) \quad\quad et\quad\quad \applyop{F}{f^{(n)}(t)}(s)=(i s)^n\applyop{F}{f(t)}(s) }
\end{equation}

\subsection{Exemples}

\subsubsection{Fonction porte $\Pi_T$}
\label{eq_fourier_porte}
Posons $f(t)=\Pi_T(t)$, on rappel que la fonction porte est définie par:
\[
\Pi_T(t)=\left\{
\begin{array}{ccc}
1&pour&t\in[-\frac{T}{2},\frac{T}{2}]\\
0&sinon&\\
\end{array}
\right.
\]
D'après la définition de la TF, on a:
\[
\hat{f}(s)=\int_{-\infty}^{\infty}\Pi_T(t)e^{-i s t} dt
\]
On peut alors remplacer les bornes d'intégration par:
\[
\hat{f}(s)=\int_{-\frac{T}{2}}^{\frac{T}{2}}e^{-i s t} dt
\]
Si on intègre, on obtient le crochet:
\[
\hat{f}(s)=\left[ \frac{e^{-i s t}}{-i s} \right]_{-\frac{T}{2}}^{\frac{T}{2}}= \frac{e^{-is \frac{T}{2}}-e^{ is \frac{T}{2}}}{-is} 
\]
On peut ici voir la forme d'Euler du sinus mais sinon on peut écrire $e^{iz}=cos(z)+i sin(z)$:
\[
\hat{f}(s)= \frac{ cos(-s \frac{T}{2})+i sin(-s \frac{T}{2}) - cos( s \frac{T}{2})-i sin( s \frac{T}{2})}{-is} 
\]
Soit avec $cos(-x)=cos(x)$ et $sin(-x)=-sin(x)$:
\[
\hat{f}(s)= \frac{-i2sin(s \frac{T}{2})}{-is} 
\]
Et si on simplifie on obtient finalement:
\[
\boxed{\applyop{F}{\Pi_T}(s) = \frac{2sin(s \frac{T}{2})}{s}}
\]

Dans le domaine fréquentiel si on pose $s=2\pi\nu$,on obtient bien :
\[
\boxed{\applyop{F}{\Pi_T}(2\pi\nu) = \frac{sin(\pi\nu T)}{\pi\nu}}
\]

\subsubsection{Fonction gaussienne normée}
\label{eq_fourier_gaussienne}
La fonction gaussienne normée, s'exprime par:
\[
g(t)=\frac{e^{-\frac{t^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}
\]
L'aire d'une gaussienne normée est égale à 1. c'est donc une fonction d'énergie finie. Et sa TF peut être calculée. D'après la définition de la TF, on a:
\[
\hat{f}(2\pi\nu)=\int_{-\infty}^{\infty}\frac{e^{-\frac{t^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}e^{-i2\pi\nu t} dt=\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{t^2}{2\sigma^2}-i2\pi\nu t} dt
\]
Or on peut écrire:
\[
-\frac{t^2}{2\sigma^2}-i2\pi\nu t= -\left(\frac{t}{\sigma \sqrt{2}}+\sigma \sqrt{2}i\pi\nu\right)^2+(\sigma \sqrt{2}i\pi\nu)^2
\]
Donc si on remplace:
\[
\hat{f}(2\pi\nu)=\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\left(\frac{t}{\sigma \sqrt{2}}+\sigma \sqrt{2}i\pi\nu\right)^2+(\sigma \sqrt{2}i\pi\nu)^2} dt
\]
On peut sortir le second terme de l'intégrale:
\[
F(\nu)=\frac{e^{(\sigma \sqrt{2}i\pi\nu)^2}}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\left(\frac{t}{\sigma \sqrt{2}}+\sigma \sqrt{2}i\pi\nu\right)^2} dt
\]
Maintenant si on pose $t=\sigma\sqrt{2}z-\sigma^2 2i\pi\nu$ et $dt=\sigma\sqrt{2}dz$, on obtient avec des bornes un peu étranges pour l'intégrale:
\[
\hat{f}(2\pi\nu)=\frac{e^{(\sigma \sqrt{2}i\pi\nu)^2}}{\sigma\sqrt{2\pi}}\int_{-\infty-\sigma^2 2i\pi\nu}^{\infty-\sigma^2 2i\pi\nu}e^{-z^2} \sigma\sqrt{2}dz
\]
\[
\hat{f}(2\pi\nu)=\frac{e^{(\sigma \sqrt{2}i\pi\nu)^2}}{\sqrt{\pi}}\int_{-\infty-\sigma^2 2i\pi\nu}^{\infty-\sigma^2 2i\pi\nu}e^{-z^2} dz
\]
Maintenant on peut décomposer l'intégrale en trois trajets dans le plan complexes:
\[
\int_{-\infty-ib}^{\infty-ib}e^{-z^2} dz=
\int_{-\infty-ib}^{-\infty}e^{-z^2} dz+\int_{-\infty}^{\infty}e^{-z^2} dz+\int_{\infty}^{\infty-ib}e^{-z^2} dz
\]
Or on sait que la fonction est paire par conséquent 
\[
\int_{-\infty-ib}^{-\infty}e^{-z^2} dz+\int_{\infty}^{\infty-ib}e^{-z^2} dz=0
\]
Et on déduit :
\[
\hat{f}(2\pi\nu)=\frac{e^{(\sigma \sqrt{2}i\pi\nu)^2}}{\sqrt{\pi}}\int_{-\infty}^{\infty}e^{-z^2} dz
\]
Il reste à calculer $I=\int_{-\infty}^{\infty}e^{-z^2} dz$ Mais on reconnait une intégrale de Gauss (voir \ref{eq_gauss_int}):
\[
	I=\sqrt{\pi}
\]
Et Finalement TF de la gaussienne vaut:

\[
\hat{f}(2\pi\nu)=\frac{e^{(\sigma \sqrt{2}i\pi\nu)^2}}{\sqrt{\pi}}\sqrt{\pi}
\]
\[
\hat{f}(2\pi\nu)=e^{-2(\sigma \pi\nu)^2}
\]

Si on pose $s=2\pi\nu$, cela donne finalement :
\begin{equation}
\boxed{\applyop{F}{\frac{e^{-\frac{t^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}}{(s)}=e^{-\frac{(\sigma s)^2}{2}}}
\end{equation}


\notes{On obtient également une gaussienne. \textbf{On remarquera que plus la gaussienne est dense dans le domaine temporelle et plus elle est étalée dans le domaine fréquentiel et inversement}.}

\newpage
\section{Transformée de Mellin}

\subsection{Définition}
\label{eq_mellin}
La transformée de Mellin est une transformation intégrale, sur l'espace des fonctions de carré sommable $L^2$, sur l'intervalle $I=[0,\infty]$, et de noyau (voir \ref{eq_transform_int}):
\begin{equation}
	\boxed{K(s,t)=t^{s-1}}
\end{equation}

\noindent
La transformée de Mellin $\hat{f}(s)$ d'une fonction $f(t)$, s'écrit alors :

\begin{equation}
\boxed{\hat{f}(s)=\int_{0}^{\infty}f(t) t^{s-1} dt}
\end{equation} 

Généralement, cette intégrale converge vers une fonction holomorphe \textbf{seulement dans une bande $B=[\alpha,\beta]$} , tel que $\Re(s)\in B$. Pour que la transformée de Mellin existe il faut que la fonction considérée l'emporte sur $t^{s-1}$, donc:

\[
Si
\left\{
\begin{array}{l}
\lim_{t\to 0^+} f(t) \approx O(t^{-\alpha})\\
\lim_{t\to \infty} f(t) \approx O(t^{-\beta})\\
\end{array}
\right.
\quad alors\quad  B\in[\alpha,\beta]
\]


Si on choisit un réel dans cette bande, $a\in B$, la fonction est alors holomorphe sur la droite complexe $D=[a-i\infty,a+i\infty]$, et la transformée inverse s'écrit :
\begin{equation}
\boxed{f(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty}\hat{f}(s) t^{-s} ds}
\end{equation} 

On introduira l'opérateur transformée de Mellin $\op{M}$ tel que:

\[
\begin{array}{l}
\hat{f}(s)=\applyop{M}{f(t)}(t,s)\\
f(t)=\applyopp{M}{-1}{\hat{f}(s)}(s,t)\\
\end{array}
\]

Par soucis de simplicité  lorsque la seconde variable du noyau n'est pas mentionnée on notera:

\[
\begin{array}{l}
\hat{f}(s)=\applyop{M}{f(t)}(s)\\
f(t)=\applyopp{M}{-1}{\hat{f}(s)}(t)\\
\end{array}
\]

\subsection{Lien avec la transformée de Fourier}
\label{eq_mellin_fourier}
Si on part de la définition:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty}f(t) t^{s-1} dt
\]
Et que on pose $t=e^{-u}$ soit $dt=-e^{-u} du$, on obtient que:
\[
\applyop{M}{f(t)}(s)=-\int_{\infty}^{-\infty}f(e^{-u}) e^{u(1-s)} e^{-u} du
\]
\[
\applyop{M}{f(t)}(s)=\int_{-\infty}^{\infty}f(e^{-u}) e^{-us} du
\]
\[
\applyop{M}{f(t)}(is)=\int_{-\infty}^{\infty}f(e^{-u}) e^{-ius} du
\]
Si on remplace la variable muette $u$ par $t$, pour être plus cohérent dans les notations:
\[
\applyop{M}{f(t)}(is)=\applyop{F}{f(e^{-t})}(s)
\]
Finalement si on pose $z=is$ soit $s=-iz$ à droite on reconnait une transformée de Fourier, donc on peut écrire:
\[
\boxed{\applyop{M}{f(t)}(z)=\applyop{F}{f(e^{-t})}(-iz)}
\]
De plus si on pose $z=a+ib$, on obtient:
\[
\applyop{M}{f(t)}(a+ib)=\applyop{F}{f(e^{-t})}(-i(a+ib))
\]
\[
\applyop{M}{f(t)}(a+ib)=\applyop{F}{f(e^{-t})}(b-ia)
\]
Et on à vu que pour un argument complexe (voir \ref{eq_fourier_cplx}) la transformée de Fourier s'exprime par $\applyop{F}{f(t)}(a+ib)=\applyop{F}{f(t)e^{bt}}(a)$, donc:

\[
\boxed{\applyop{M}{f(t)}(a+ib)=\applyop{F}{f(e^{-t})e^{-at}}(b)}
\]

Sous réserve que la transformation de Mellin existe ($a\in B$).

\subsection{Formule d'inversion}
\label{eq_mellin_inv}
On vient de montrer (voir \ref{eq_mellin_fourier}) que la Transformée de Mellin $\hat{f}(a+ib)$ d'une fonction $f(t)$ est liée à la transformée de Fourier par la relation:
\[
\hat{f}(a+ib)=\applyop{M}{f(t)}(a+ib)=\applyop{F}{f(e^{-t})e^{-at}}(b)
\]

La formule d'inversion de Fourier (voir \ref{eq_fourier_def}) donne :

\[
f(e^{-t})e^{-at}=\frac{1}{2\pi}\int_{-\infty}^{\infty} \hat{f}(a+ib) e^{ibt}db
\]
\[
f(e^{-t})=\frac{1}{2\pi}\int_{-\infty}^{\infty} \hat{f}(a+ib) e^{(a+ib)t}db
\]

Si on pose $u=e^{-t}$ alors on obtient:
\[
f(u)=\frac{1}{2\pi}\int_{-\infty}^{\infty} \hat{f}(a+ib) u^{-(a+ib)}db
\]

Si on repose $s=a+ib$ donc $b=i(a-s)$ et $db=\frac{ds}{i}$, donc:

\[
f(u)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \hat{f}(s) u^{-s}ds
\]
Et si on change $u$ en $t$ pour retrouver nos notations:
\[
\boxed{f(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \hat{f}(s) t^{-s}ds}
\]

Sous réserve que la transformation de Mellin existe ($a\in B$).

\subsection{Propriétés}

Pour toutes les démonstrations de propriétés suivantes, on considère que la transformée de Mellin de la fonction $f(t)$ converge sur une bande $B=[\alpha,\beta]$.

\subsubsection{Linéarité}
\label{eq_mellin_lin}
Partons de la définition
\[
\applyop{M}{a f(t) + b g(t)}{(s)}=\int_0^{\infty} \left(a f(t) + b g(t)\right) t^{s-1} dt
\]
Par propriété de l'intégration on peut alors trivialement écrire:
\[
\applyop{M}{a f(t) + b g(t)}{(s)}=a\int_0^{\infty} f(t) t^{s-1} dt+b\int_0^{\infty} g(t) t^{s-1} dt
\]

Finalement
\begin{equation}
\boxed{
\applyop{M}{a f(t) + b g(t)}{(s)}=a\applyop{M}{f(t)}{(s)} + b\applyop{M}{g(t)}{(s)}
}
\end{equation}

\subsubsection{Contraction}
\label{eq_mellin_cont}
Partons de la définition
\[
\applyop{M}{f(at)}{(s)}=\int_0^{\infty} f(at) t^{s-1} dt
\]
Si on fait le changement de variable $u=at$ soit $t=\frac{u}{a}$ donc $dt=\frac{du}{a}$:
\[
\applyop{M}{f(at)}{(s)}=\int_0^{\infty} f(u) \left(\frac{u}{a}\right)^{s-1} \frac{du}{a}
\]
On peut alors sortir $a^{-s}$ et remplacer la variable muette d'intégration $u$ par $t$ pour revenir à nos notations:
\[
\applyop{M}{f(at)}{(s)}=a^{-s}\int_0^{\infty} f(t) t^{s-1} dt
\]

Finalement
\begin{equation}
\boxed{
\applyop{M}{f(at)}{(s)}=a^{-s}\applyop{M}{f(t)}{(s)}\quad\quad avec\quad\quad B=[\alpha,\beta]
}
\end{equation}

\subsubsection{Transformée des dérivées kième $f^{(k)}(t)$}
\label{eq_mellin_deriv}
Partons de la définition
\[
\applyop{M}{f'(t)}{(s)}=\int_0^{\infty} f'(t) t^{s-1} dt
\]
Si on fait une intégration par partie en posant:
$\intp{t^{s-1}}{(s-1)t^{s-2}}{f'(t)}{f(t)}{0}{\infty}$, on obtient :

\[
\applyop{M}{f'(t)}{(s)}=\left[ f(t)t^{s-1} \right]_0^{\infty}-(s-1)\int_0^\infty f(t) t^{s-2} dt 
\]

Mais pour que l'intégrale existe $f(t)$ doit l'emporter sur $t^{s-1}$ en $0$ et $\infty$, donc le crochet est nul :

\begin{equation}
\boxed{\applyop{M}{f'(t)}{(s)}=-(s-1)\applyop{M}{f(t)}{(s-1)}}
\end{equation}

Si on itère la relation :

\[
\applyop{M}{f^{(k)}(t)}{(s)}=-(s-1)\applyop{M}{f^{(k-1)}(t)}{(s-1)}
\]
\[
\applyop{M}{f^{(k)}(t)}{(s)}=(-1)^2(s-1)(s-2)\applyop{M}{f^{(k-2)}(t)}{(s-2)}
\]

On aboutit à la formule générale:

\begin{equation}
\boxed{\applyop{M}{f^{(k)}(t)}{(s)}=(-1)^k \left(\prod_{n=1}^{k}(s-n)\right)\applyop{M}{f(t)}{(s-k)}}
\end{equation}

Maintenant si on exprime plutôt, la transformée de Mellin en fonction de la transformée de la dérivée, on obtient:
\[
\applyop{M}{f(t)}{(s-1)}=-\frac{1}{s-1}\applyop{M}{f'(t)}{(s)}
\]
Et si on fait le changement de variable $s-1 \to s$, alors
\begin{equation}
\boxed{\applyop{M}{f(t)}{(s)}=-\frac{1}{s}\applyop{M}{f'(t)}{(s+1)}}
\end{equation}

\subsubsection{Multiplication par $ln(t)^k$}
\label{eq_mellin_mul_ln}
Si on calcul les dérivées kièmes par rapport à $s$ de la Transformée de Mellin d'une fonction $f(t)$ convenable, on écrit:

\[
\left(\applyop{M}{f(t)}{(s)}\right)^{(k)}=\int_0^{\infty} f(t) \left(t^{s-1}\right)^{(k)} dt
\]
Ce qui donne
\[
\left(\applyop{M}{f(t)}{(s)}\right)^{(k)}=\int_0^{\infty} f(t) ln(t)^{k} t^{s-1} dt
\]
Finalement
\begin{equation}
\boxed{\applyop{M}{ln(t)^{k}f(t)}{(s)}=\left(\applyop{M}{f(t)}{(s)}\right)^{(k)}}
\end{equation}

\subsubsection{Multiplication par $t^a$}
\label{eq_mellin_mul_power}
Partons de la définition
\[
\applyop{M}{t^a f(t)}{(s)}=\int_0^{\infty} t^a f(t) t^{s-1} dt
\]
\[
\applyop{M}{t^a f(t)}{(s)}=\int_0^{\infty} f(t) t^{s+a-1} dt
\]
Donc trivialement
\begin{equation}
\boxed{
\applyop{M}{t^a f(t)}{(s)}=\applyop{M}{f(t)}{(s+a)}\quad\quad avec\quad\quad B=[\alpha-a,\beta-a]
}
\end{equation}

\subsubsection{Dérivée dans la transformée inverse}
\label{eq_mellin_deriv_inverse}

Si on part de la formule trouvée (voir \ref{eq_mellin_mul_ln}) :

\[
\applyop{M}{ln(t)^{k}f(t)}{(s)}=\left(\applyop{M}{f(t)}{(s)}\right)^{(k)}
\] 

En particulier si $k=1$ et si on pose $\hat{f}(s)=\applyop{M}{f(t)}{(s)}$ soit $f(t)=\applyopp{M}{-1}{\hat{f}(s)}{(t)}$, alors on obtient:

\[
\applyop{M}{ln(t) f(t)}{(s)}=\hat{f}'(s)
\]

Si on applique l'opérateur de transformation inverse sur les deux membres, on obtient:

\[
ln(t) f(t)=\applyopp{M}{-1}{\hat{f}'(s)}{(t)}
\]

Comme $f(t)=\applyopp{M}{-1}{\hat{f}(s)}{(t)}$, on obtient finalement:

\begin{equation}
\boxed{\applyopp{M}{-1}{\hat{f}(s)}{(t)}=\frac{1}{ln(t)}\applyopp{M}{-1}{\hat{f}'(s)}{(t)}}
\end{equation}

\subsubsection{Puissance de la variable}
\label{eq_mellin_var_power}
Partons de la définition
\[
\applyop{M}{f(t^a)}{(s)}=\int_0^{\infty} f(t^a) t^{s-1} dt
\]
Maintenant si on pose $u=t^a$ soit $t=u^{\frac{1}{a}}$ et $dt=\frac{1}{a} u^{\frac{1}{a}-1} du$, ce qui donne. Si $a>0$ les bornes ne changent pas, si $a<0$ les bornes sont inversées
\[
\applyop{M}{f(t^a)}{(s)}=\left\{\begin{array}{cc}\frac{1}{a} \int_0^{\infty} f(u) u^{\frac{s}{a}-1} du& a>0\\
\frac{1}{a} \int_\infty^{0} f(u) u^{\frac{s}{a}-1} du& a<0\\
\end{array}
\right.
\]
Finalement on peut résumer en une seule expression les deux cas 
\begin{equation}
\boxed{
\applyop{M}{f(t^a)}{(s)}=\frac{1}{|a|}\applyop{M}{f(t)}{\left(\frac{s}{a}\right)\quad\quad avec\quad\quad B=[\alpha a,\beta a]}
}
\end{equation}

\newpage
\subsection{Formule de Perron}
\label{eq_perron_formula}

La formule de Perron découle trivialement par changement de variable, en effet si on part de la relation:

\[
f(t)=\applyopp{M}{-1}{ \applyop{M}{f(t)}(S) } (S,t)
\]

Et que on pose $S=-s$, on obtient directement la relation:

\begin{equation}
\boxed{f(t)=\applyopp{M}{-1}{ \applyop{M}{f(t)}(-s) } (-s,t)}
\end{equation}

C'est à dire :

\begin{equation}
\boxed{\hat{f}(s)=\int_{0}^{\infty}f(t) t^{-s-1} dt \quad\quad \Rightarrow \quad\quad 
f(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty}\hat{f}(s) t^{s} ds}
\end{equation}

\subsubsection{Lien entre la formule de Perron inverse et la transformée de Mellin inverse}
\label{eq_perron_formula_inv_link_mellin}
Soit:
\[
f(t)=\applyopp{M}{-1}{ f(s) } (-s,t)
\]
C'est à dire:
\[
f(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} f(s) t^{s} ds
\]
Si on fait un changement de variable en posant $s=-S$ et $ds=-dS$, cela donne
\[
f(t)=\frac{1}{2i\pi}\int_{-a+i\infty}^{-a-i\infty} -f(-S) t^{-S} dS
\]
Et si on inverse les bornes on obtient :
\[
f(t)=\frac{1}{2i\pi}\int_{-a-i\infty}^{-a+i\infty} f(-S) t^{-S} dS
\]

Donc finalement, on a le lien :

\begin{equation}
	\boxed{\applyopp{M}{-1}{ f(s) } (-s,t)=\applyopp{M}{-1}{ f(-s) } (t)}
\end{equation}

\subsubsection{Dérivation dans la formule de Perron inverse}
\label{eq_perron_deriv_inverse}

Nous avons établi (voir \ref{eq_mellin_deriv_inverse}) que:

\[
\applyopp{M}{-1}{f(s)}{(t)}=\frac{1}{ln(t)}\applyopp{M}{-1}{f'(s)}{(t)}
\]

Et également (voir \ref{eq_perron_formula_inv_link_mellin}) que:
\[
\applyopp{M}{-1}{ f(s) } (-s,t)=\applyopp{M}{-1}{ f(-s) } (t)
\]
Par conséquent, on peut établir que:
\begin{eqnarray*}
\applyopp{M}{-1}{ f(s) } (-s,t)&=&\applyopp{M}{-1}{ f(-s) } (t)\\
&=&\frac{1}{ln(t)}\applyopp{M}{-1}{ (f(-s))' } (t)\\
&=&\frac{1}{ln(t)}\applyopp{M}{-1}{ -f'(-s) } (t)\\
&=&-\frac{1}{ln(t)}\applyopp{M}{-1}{ f'(s) } (-s,t)\\
\end{eqnarray*}

Finalement pour la formule de perron inverse on a :

\begin{equation}
\boxed{ \applyopp{M}{-1}{ f(s) } (-s,t)=-\frac{1}{ln(t)}\applyopp{M}{-1}{ f'(s) } (-s,t) }
\end{equation}

\subsubsection{L'intégrale basique : ( Méthode 1 : par les résidus )\workon\fig\todo}

Considérons la transformée de Perron inverse de la fonction $\frac{1}{s}$, soit:

\begin{eqnarray*}
f(t)&=&\applyopp{M}{-1}{\frac{1}{s}}{(-s,t)}\\
&=& \frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \frac{t^{s}}{s} ds\\&
\end{eqnarray*}

D'après le théorème des résidus l'intégrale sur le chemin $C=C_a \cup C_b$ :
\[
\frac{1}{2i\pi}\int_{C} \frac{t^s}{s} ds = \frac{1}{2i\pi}\int_{C_a} \frac{t^s}{s} ds + \frac{1}{2i\pi}\int_{C_b} \frac{t^s}{s} ds=\lim_{s\to 0} t^s
\]
Donc si $t>0$ le résidu vaut $1$ et $\forall a>0$ et $\forall R>0$ on a donc:
\[
f(t) = 1 - \frac{1}{2i\pi}\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \frac{t^{a+Re^{i\theta}}}{a+Re^{i\theta}} Rie^{i\theta} d\theta
\]

\begin{itemize}
\item Si $t=1$ cela donne : 
\begin{eqnarray*}
f(1) &=& 1 - \frac{1}{2i\pi}\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \frac{Rie^{i\theta} }{a+Re^{i\theta}}d\theta\\
&=& 1 - \frac{1}{2i\pi}\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} i\\
&=& \frac{1}{2}\\
\end{eqnarray*}

\item Si $0<t<1$ cela donne {\todo}:

\begin{eqnarray*}
f(t) &=& 1 - \frac{1}{2i\pi}\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \frac{t^{a+Re^{i\theta}}}{a+Re^{i\theta}} Rie^{i\theta} d\theta\\
&=& 1 - \frac{t^a}{2i\pi}\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} i t^{Re^{i\theta}} d\theta\\
\end{eqnarray*}

\end{itemize}

Finalement pour résumé, on obtient:

\begin{equation}
\boxed{\applyopp{M}{-1}{\frac{1}{s}}{(-s,t)}=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \frac{t^{s}}{s} ds=\left\{\begin{array}{lll}
0&si& t<1\\
\frac{1}{2}&si&t=1\\
1&si&t>1\\
\end{array}\right.}
\end{equation}

\subsubsection{L'intégrale basique : ( Méthode 2 : par identification )}
\label{eq_mellin_perron_basic}
Considérons la transformée de Perron inverse de la fonction $\frac{1}{s}$, en utilisant le lien entre la transformée de Perron inverse et la transformée de Mellin inverse $\applyopp{M}{-1}{ f(s) } (-s,t)=\applyopp{M}{-1}{ f(-s) } (t)$ ( voir \ref{eq_perron_formula_inv_link_mellin} ), on obtient:

\begin{eqnarray*}
f(t)&=&\applyopp{M}{-1}{\frac{1}{s}}{(-s,t)}\\
f(t)&=&\applyopp{M}{-1}{-\frac{1}{s}}{(s,t)}\\
\applyop{M}{f(t)}{(s)}&=&-\frac{1}{s}\\
\end{eqnarray*}

Or nous avons montrés que (voir \ref{eq_mellin_power}) que :

\[
\applyop{M}{\Heav(t-a)t^b}(s)=-\frac{a^{b+s}}{b+s} \quad\quad avec\quad\quad B=[-\infty,-\Re(b)]
\]

Si on prend $a=1$ et $ b=0$, cela donne: 

\[
\applyop{M}{\Heav(t-1)}(s)=-\frac{1}{s} \quad\quad avec\quad\quad B=[-\infty,0]
\]

\subsubsection{L'intégrale basique généralisation: ( Méthode 2 : par identification )}
\label{eq_mellin_perron_basic_gen}
Considérons la transformée de Perron inverse de la fonction $\frac{n^{-s}}{s}$, en utilisant le lien entre la transformée de Perron inverse et la transformée de Mellin inverse $\applyopp{M}{-1}{ f(s) } (-s,t)=\applyopp{M}{-1}{ f(-s) } (t)$ ( voir \ref{eq_perron_formula_inv_link_mellin} ), on obtient:

\begin{eqnarray*}
f(t)&=&\applyopp{M}{-1}{\frac{n^{-s}}{s}}{(-s,t)}\\
f(t)&=&\applyopp{M}{-1}{-\frac{n^{s}}{s}}{(s,t)}\\
\applyop{M}{f(t)}{(s)}&=&-\frac{n^{s}}{s}\\
\end{eqnarray*}

Or nous avons montrés que (voir \ref{eq_mellin_power}) que :

\[
\applyop{M}{\Heav(t-a)t^b}(s)=-\frac{a^{b+s}}{b+s} \quad\quad avec\quad\quad B=[-\infty,-\Re(b)]
\]

Si on prend $a=n$ et $b=0$, cela donne: 

\[
\applyop{M}{\Heav(t-n)}(s)=-\frac{n^s}{s} \quad\quad avec\quad\quad B=[-\infty,0]
\]

Finalement pour résumé, on obtient:

\begin{equation}
\boxed{\applyopp{M}{-1}{\frac{n^{-s}}{s}}{(-s,t)}=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \frac{1}{n^s}\frac{t^{s}}{s} ds=\Heav(t-n)=\left\{\begin{array}{lll}
0&si&t<n\\
\frac{1}{2}&si&t=n\\
1&si&t>n\\
\end{array}\right.}
\end{equation}

\subsubsection{Formule de Perron pour une série de Dirichlet}
\label{eq_perron_formula_dirichlet}

Soit une série de Dirichlet :

\[
f(s)=\sum_{n=1}^{\infty}\frac{a(n)}{n^s}
\]

D'après le résultat précédent (voir \ref{eq_mellin_perron_basic_gen}), si on calcule :

\begin{eqnarray*}
A(t)&=&\applyopp{M}{-1}{\frac{f(s)}{s}}(-s,t)\\
&=&\applyopp{M}{-1}{\frac{1}{s}\sum_{n=1}^{\infty}\frac{a(n)}{n^s}}(-s,t)\\
&=&\sum_{n=1}^{\infty} a(n) \applyopp{M}{-1}{\frac{n^{-s}}{s}}(-s,t)\\
&=&\sum_{n=1}^{\infty} a(n) \Heav(t-n)\\
&=&\sum_{n<t} a(n)\\
\end{eqnarray*}

On obtient finalement:

\begin{equation}
\boxed{A(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \frac{f(s)}{s} t^{s} ds \quad\quad avec\quad\quad f(s)=\sum_{n=1}^{\infty}\frac{a(n)}{n^s}\quad\quad et\quad\quad A(t)=\sum_{n<t} a(n)}
\end{equation}

Ce qui signifie également que $f(s)$ peut se représenter par l'intégrale:

\[
\frac{f(s)}{s}=\applyop{M}{A(t)}(-s)
\]

Soit finalement:

\begin{equation}
\boxed{f(s)=s \int_{0}^{\infty}A(t) t^{-s-1} dt \quad\quad avec\quad\quad f(s)=\sum_{n=1}^{\infty}\frac{a(n)}{n^s}\quad\quad et\quad\quad A(t)=\sum_{n<t} a(n)}
\end{equation}

\subsection{Exemples}
\subsubsection{Impulsion de Dirac}
\label{eq_mellin_delta}
Prenons $f(t)=\delta(t-a)$ avec $p\in\mathbb{R}$, la transformation de Mellin donne alors:

\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} \delta(t-a) t^{s-1} dt
\]

Ce qui permet d'extraire la valeur en zéro de l'intégrante , finalement :
\begin{equation}
\boxed{\applyop{M}{\delta(t-a)}(s)=a^{s-1} }
\end{equation}

\subsubsection{Fonction puissance}
\label{eq_mellin_power}
Prenons $f(t)=\Heav(t-a)t^b$ avec $\Heav(t)$ la fonction de Heaviside et $z\in\mathbb{C}$ et $a\in\mathbb{R}$, la transformation de Mellin donne alors:

\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} \Heav(t-a)t^b t^{s-1} dt
\]
Soit
\[
\applyop{M}{f(t)}(s)=\int_{a}^{\infty} t^{b+s-1} dt
\]
On intègre
\[
\applyop{M}{f(t)}(s)=\left[ \frac{t^{b+s}}{b+s} \right]_a^{\infty}
\]
Et si $\Re(b)+\Re(s)<0$ soit $\Re(s)<-\Re(b)$ ce qui spécifie la bande de convergence de l'intégrale, alors:
\[
\boxed{\applyop{M}{\Heav(t-a)t^b}(s)=-\frac{a^{b+s}}{b+s} \quad\quad avec\quad\quad B=[-\infty,-\Re(b)]}
\]

\subsubsection{Fonction exponentielle}
\label{eq_mellin_exp}
Prenons $f(t)=e^{-at}$ avec $a\in\mathbb{R^+}$, la transformation de Mellin donne alors:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} e^{-at} t^{s-1} dt
\]
Et si on pose $u=at$ soit $t=\frac{u}{a}$ et $dt=\frac{du}{a}$, les bornes ne changent pas, on obtient:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} e^{-u} \left(\frac{u}{a}\right)^{s-1} \frac{du}{a}
\]
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} e^{-u} u^{s-1} a^{-1-s+1} du=a^{-s}\int_{0}^{\infty} e^{-u} u^{s-1} du
\]
Et à droite on reconnais la fonction $\Gamma(s)$ que on introduira en détail plus tard (voir \ref{eq_gamma_func}).
\[
\boxed{\applyop{M}{e^{-at}}(s)=a^{-s}\Gamma(s) \quad\quad avec\quad\quad B=[0,\infty]}
\]
\subsubsection{Fonction exponentielle générale}
\label{eq_mellin_exp_gen}

Prenons $f(t)=e^{-at^b}$ avec $a,b\in\mathbb{R^+}$, la transformation de Mellin donne alors:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} e^{-at^b} t^{s-1} dt
\]
Maintenant si on pose $u=at^b$ soit $t=\left(\frac{u}{a}\right)^{\frac{1}{b}}$ ce qui implique $dt=\frac{1}{ab} \left(\frac{u}{a}\right)^{\frac{1}{b}-1}$, l'intégrale devient:

\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} e^{-u} \left(\frac{u}{a}\right)^{\frac{s-1}{b}} \frac{1}{ab} \left(\frac{u}{a}\right)^{\frac{1}{b}-1}
\]
\[
\applyop{M}{f(t)}(s)=\frac{a^{\frac{1-s}{b}-1+1-\frac{1}{b}}}{b}\int_{0}^{\infty} e^{-u} u^{\frac{s-1}{b}+\frac{1}{b}-1}
\]
\[
\applyop{M}{f(t)}(s)=\frac{a^{-\frac{s}{b}}}{b}\int_{0}^{\infty} e^{-u} u^{\frac{s}{b}-1}
\]

Finalement
\begin{equation}
\boxed{\applyop{M}{e^{-at^b}}(s)=\frac{a^{-\frac{s}{b}}}{b}\Gamma\left(\frac{s}{b}\right)\quad\quad avec\quad\quad B=[0,\infty]}
\end{equation}

\subsubsection{Fonction exponentielle imaginaire}
\label{eq_mellin_exp_img}
Prenons $f(t)=e^{iat}$ avec $a\in\mathbb{R}$, la transformation de Mellin donne alors:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} e^{iat} t^{s-1} dt
\]
Comme $|e^{iat}|=1$ l'intégrale converge uniquement si $0<Re(s)<1$. Maintenant, si on pose $u=-iat$ donc $t=\frac{i}{a}u$ et $dt=\frac{i}{a} du$, cela donne:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{-ia\infty} e^{-u} \left(\frac{i}{a}u\right)^{s-1} \frac{i}{a}du
\]
\[
\applyop{M}{f(t)}(s)=\left(\frac{i}{a}\right)^{s} \int_{0}^{-ia\infty} e^{-u} u^{s-1} du
\]
Mais $i=e^{i\frac{\pi}{2}}$ donc finalement
\[
\applyop{M}{f(t)}(s)=\frac{e^{i\frac{s\pi}{2}}}{a^s} \int_{0}^{-ia\infty} e^{-u} (u)^{s-1} du
\]

Mais l'intégrante est analytique sur $0\ge Re(s)$ donc on peut considéré que intégrer de $0$ à $-ia\infty$ donne le même résultat que d'intégrer de $0$ à $\infty$ (voir \ref{eq_sphere_riemann}), et Finalement : 

\begin{equation}
\boxed{\applyop{M}{e^{iat}}(s)=\frac{e^{i\frac{s\pi}{2}}}{a^s}\Gamma(s) \quad\quad avec\quad\quad B=[0,1]}
\end{equation}

\notes{Si substitue $a=-ai$, dans l'expression $\applyop{M}{e^{-at}}(s)=a^{-s}\Gamma(s)$, on retrouve bien ce résultat, mais il faut faire attention à la bande de convergence.}

Aussi si on calcul particulièrement :
\[
\applyop{M}{e^{-iat}}(s)=\frac{e^{i\frac{s\pi}{2}}}{(-a)^s}\Gamma(s)
\]
\[
\applyop{M}{e^{-iat}}(s)=\frac{e^{i\frac{s\pi}{2}}}{a^s e^{is\pi}}\Gamma(s)
\]
\[
\applyop{M}{e^{-iat}}(s)=\frac{e^{i(\frac{s\pi}{2}-s\pi)}}{a^s}\Gamma(s)
\]
Finalement
\begin{equation}
\boxed{\applyop{M}{e^{-iat}}(s)=\frac{e^{-i\frac{s\pi}{2}}}{a^s}\Gamma(s)\quad\quad avec\quad\quad B=[0,1]}
\end{equation}

\subsubsection{Fonction sinus}
\label{eq_mellin_sin}
Prenons $f(t)=sin(t)$, la transformation de Mellin donne alors:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} sin(t) t^{s-1} dt
\]
Si on prend la forme d'Euler pour le sinus $sin(t)=\frac{e^{it}-e^{-it}}{2i}$, on obtient :
\[
\applyop{M}{f(t)}(s)=\frac{1}{2i}\int_{0}^{\infty} (e^{it}-e^{-it}) t^{s-1} dt
\]
Soit par linéarité de la Transformation de Mellin (voir \ref{eq_mellin_lin}) :
\[
\applyop{M}{f(t)}(s)=\frac{1}{2i}\left(\applyop{M}{e^{it}}(s)-\applyop{M}{e^{-it}}(s)\right)
\]
D'après le résultat précédent (voir \ref{eq_mellin_exp_img}), on obtient alors:
\[
\applyop{M}{f(t)}(s)=\frac{\Gamma(s)}{2i} \left( e^{i\frac{s\pi}{2}}-e^{-i\frac{s\pi}{2}}\right)
\]

Finalement
\begin{equation}
\boxed{\applyop{M}{sin(t)}(s)=\Gamma(s)sin\left( \frac{s\pi}{2} \right) }
\end{equation}

\subsubsection{Fonction cosinus}
\label{eq_mellin_cos}

De manière très analogue, prenons $f(t)=sin(t)$, la transformation de Mellin donne alors:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} cos(t) t^{s-1} dt
\]
Si on prend la forme d'Euler pour le sinus $cos(t)=\frac{e^{it}+e^{-it}}{2}$, on obtient :
\[
\applyop{M}{f(t)}(s)=\frac{1}{2}\int_{0}^{\infty} (e^{it}+e^{-it}) t^{s-1} dt
\]
Soit par linéarité de la Transformation de Mellin (voir \ref{eq_mellin_lin}) :
\[
\applyop{M}{f(t)}(s)=\frac{1}{2}\left(\applyop{M}{e^{it}}(s)+\applyop{M}{e^{-it}}(s)\right)
\]
D'après le résultat précédent (voir \ref{eq_mellin_exp_img}), on obtient alors:
\[
\applyop{M}{f(t)}(s)=\frac{\Gamma(s)}{2} \left( e^{i\frac{s\pi}{2}}+e^{-i\frac{s\pi}{2}}\right)
\]
Finalement
\begin{equation}
\boxed{\applyop{M}{cos(t)}(s)=\Gamma(s)cos\left( \frac{s\pi}{2} \right) }
\end{equation}

\subsubsection{Fonction dérivée du logarithme}
\label{eq_mellin_deriv_log}

Prenons $f(t)=(t+1)^{-1}$, la transformation de Mellin donne alors:
\[
\applyop{M}{f(t)}(s)=\int_{0}^{\infty} (t+1)^{-1} t^{s-1} dt
\]

Si on pose $t=\frac{u}{1-u}$ alors $dt=\frac{1}{(1-u)^2}du$, l'intégrale devient :

\[
\applyop{M}{f(t)}(s)=\int_{0}^{1} \left(\frac{u}{1-u}+1\right)^{-1} \left(\frac{u}{1-u}\right)^{s-1} \frac{1}{(1-u)^2} du
\]
\[
\applyop{M}{f(t)}(s)=\int_{0}^{1} \frac{1}{1-u} \left(\frac{u}{1-u}\right)^{s-1} du
\]
\[
\applyop{M}{f(t)}(s)=\int_{0}^{1} \left(1-u\right)^{-s} u^{s-1} du
\]
On reconnait ici la fonction $B(s,1-s)=\int_0^1 t^{s-1} (1-t)^{-s} dt$ ( voir \ref{eq_gamma_comp2} ). Donc en fait on peut écrire que :

\[
\applyop{M}{(t+1)^{-1}}(s)=B(s,1-s)
\]

Mais nous savons par la formule des compléments ( voir \ref{eq_gamma_comp2} ) que $B(s,1-s)=\frac{\pi}{sin(\pi s)}$ donc finalement :

\begin{equation}
\boxed{\applyop{M}{(t+1)^{-1}}(s)=\frac{\pi}{sin(\pi s)}}
\end{equation}

\subsubsection{Fonction dérivée du logarithme\todo}

Finalement
\[
\boxed{\applyop{M}{(t+1)^{-n}}(s)=\frac{\Gamma(n-s)\Gamma(s)}{\Gamma(n)}}
\]

\subsubsection{Fonction logarithme}
\label{eq_mellin_log}
Partons de la relation obtenue (voir \ref{eq_mellin_deriv}) sur la transformée en fonction de la transformée de la dérivée:
\[
\applyop{M}{f(t)}{(s)}=-\frac{1}{s}\applyop{M}{f'(t)}{(s+1)}
\]
Dans notre cas cela donne:
\[
\applyop{M}{ln(t+1)}(s)=-\frac{1}{s}\applyop{M}{(t+1)^{-1}}{(s+1)}
\]
Or d'après le résultat précédent cela donne:
\[
\applyop{M}{ln(t+1)}(s)=-\frac{1}{s}\frac{\pi}{sin(\pi (s+1))}
\]
Mais $sin(t+\pi)=-sin(t)$ Finalement
\begin{equation}
\boxed{\applyop{M}{ln(t+1)}(s)=\frac{\pi}{s\ sin(\pi s)}}
\end{equation}

\subsubsection{Fonction logarithme intégrale\todo}

Prenons $f(t)=Li(t)$ (voir \ref{eq_ln_int}), la transformation de Mellin donne alors:

\[
\applyop{M}{Li(t)}(s)=\int_{0}^{\infty} Li(t) t^{s-1} dt
\]

Mais

\[
\applyop{M}{Li(t)}{(s)}=-\frac{1}{s}\applyop{M}{f'(t)}{(s+1)}
\]

Dans notre cas cela donne :

\[
\boxed{\applyop{M}{Li(t)}(s)=-\frac{1}{s}\int_0^{\infty}\frac{t^s}{ln(t)} dt}
\]


\newpage
\section{Fonction Exponentielle}
\subsection{Définition et unicité}
\label{eq_exp_serie}

\begin{tabular}{cc}
\begin{minipage}{12cm}
On définit la fonction exponentielle par le fait que la fonction est sa propre dérivée $exp'(z)=exp(z)$ et que $exp(0)=1$. Ces deux considérations permettent de définir la fonction par son développement en série de Taylor ( \ref{dl_exponentiel} ), on rappelle que :

\[
exp(z)=\sum_{n=0}^{\infty}\frac{exp'(0)}{n!}z^n
\]

\[
\boxed{exp(z)=\sum_{n=0}^{\infty}\frac{z^{n}}{n!}}
\]

Par propriété de l'unicité du développement de Taylor, on obtient l'unicité de la fonction exponentielle (\ref{unicite_dl}).
\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{exp}{$e^z$}
\end{minipage}\\
\end{tabular}

\subsection{Représentations}
\subsubsection{Représentation de Gauss}
\label{eq_exp_gauss}

Au voisinage d'un point $a$ le développement à l'ordre 1 d'une fonction $f(z)$ s'écrit:

\[
	f(a+h)\approx f(a)+f'(a)h
\]

Dans le cas de la fonction exponentielle recherchée cette relation s'écrit:

\[
	exp(a+h)\approx exp(a)+exp'(a)h
\]
Soit
\[
	exp(a+h)\approx exp(a)(1+h)
\]
A partir de cette relation on peut construire une suite infinie d'approximations qui converge vers la fonction. Si on part de $a=0$ et que on fait un petit pas $h=\frac{z}{n}$ avec n grand:

\[
	exp\left(\frac{z}{n}\right)\approx \left(1+\frac{z}{n}\right)
\]
Maintenant si on prend $a = \frac{z}{n}$ et toujours $h=\frac{z}{n}$, on obtient:
\[
	exp\left(2\frac{z}{n}\right)\approx exp\left(\frac{z}{n}\right)\left(1+\frac{z}{n}\right)=\left(1+\frac{z}{n}\right)^2
\]
Ainsi par récurrence, on obtient la relation :
\[
	exp\left(m\frac{z}{n}\right)\approx\left(1+\frac{z}{n}\right)^m
\]
Si on prend $m=n$, et que on fait des pas infinitésimaux les approximations deviennes des égalités et on obtient finalement:

\begin{equation}
\boxed{exp(z)=\lim_{n\to\infty}\left( 1+\frac{z}{n} \right)^n}
\end{equation}

\subsubsection{Équivalence des représentations}

Cette démonstration n'est pas nécessaire car nous avons déjà implicitement montré l'équivalence avec les raisonnement précédent, cependant elle est intéressante. Partons de la représentation de Gauss. Par le binôme de newton on a $(x+y)^n=\sum_{k=0}^{n} C^k_n x^{n-k} y^{k}$ (voir \ref{coef_bin}), donc:

\[
\left(1+\frac{z}{n}\right)^n=\sum_{k=0}^{n} C^k_n \frac{z^{k}}{n^{k}}
\]

Et $C^k_n=\frac{n!}{k!(n-k)!}$ :

\[
\left(1+\frac{z}{n}\right)^n=\sum_{k=0}^{n} \frac{n!}{k!(n-k)!} \frac{z^{k}}{n^{k}}
\]
\[
\left(1+\frac{z}{n}\right)^n=\sum_{k=0}^{n} \frac{n!}{n^k(n-k)!} \frac{z^{k}}{k!}
\]

Étudions la limite :

\[
	\lim_{n\to\infty} \frac{n!}{n^k(n-k)!} =\lim_{n\to\infty} \frac{(1\cdot 2\cdot 3 ... n )}{n^k(1\cdot 2\cdot 3 ... n-k)}=\lim_{n\to\infty}\frac{1}{n^k}\prod_{i=0}^{k-1}(n-i)=1
\]

Donc 

\[
\boxed{\lim_{n\to\infty}\left( 1+\frac{z}{n} \right)^n=\sum_{n=0}^{\infty}\frac{z^{n}}{n!}}
\]

\subsubsection{Représentation en puissances du nombre $e$}

Partons de la relation:
\[
exp(a) exp(b) = exp(a+b)
\]
Il s'en suit que :
\[
	exp(2a)=exp(a)^2
\]
Et par récurrence
\[
	\begin{array}{l}
		exp(na)=exp((n-1)a)exp(a)\\
		exp(na)=exp((n-2)a)exp(a)^2\\
		exp(na)=exp((n-3)a)exp(a)^3\\
		...\\
		exp(na)=exp(a)exp(a)^{n-1}\\
	\end{array}			
\]
Finalement pour $n$ entier:
\[
	\boxed{exp(na)=exp(a)^n \quad pour \quad n\in \mathbb{Z} } 
\]

Mais on à aussi 
\[
exp(na)=exp\left(m\frac{n}{m}a\right)=exp\left(\frac{n}{m}a\right)^m
\]
Donc 
\[
exp\left(\frac{n}{m}a\right)=exp(na)^{\frac{1}{m}}=exp(a)^{\frac{n}{m}}
\]
Finalement si on note $r=\frac{n}{m}$ un nombre rationnel, on obtient bien que:

\[
	\boxed{exp(ra)=exp(z)^r \quad pour \quad r\in \mathbb{Q} } 
\]

Mais comme un nombre réel peut être approché d'aussi près que l'on veut par un nombre rationnel, on peut sans difficulté étendre la formule aux nombres réels.

\[
	\boxed{exp(ra)=exp(a)^r \quad pour \quad r\in \mathbb{R} } 
\]

Et par conséquent si on note $e=exp(1)$ on à bien montré que sur les réels, mais on pourra encore étendre aux nombre complexes que:

\[
\boxed{exp(z)=e^z \quad avec \quad z\in\mathbb{C} \quad et \quad e=exp(1)}
\]

\subsubsection{Représentation de Moivre}
\label{eq_exp_trig}
D'après les séries de Taylor du sinus (voir \ref{dl_sin}) et du cosinus (voir \ref{dl_cos}) on peut écrire pour tout $z\in\mathbb{C}$ : 
\[
	cos(z)+i\ sin(z)= \sum_{n=0}^{\infty}\frac{z^{2 n}}{(2 n)!}(-1)^n + i \sum_{n=0}^{\infty}\frac{z^{1+2 n}}{(1+2 n)!}(-1)^n 
\]

Si on rassemble les termes pairs et impairs en une seule somme, on peut faire un petit tableau pour nous aider:
\[
	\begin{array}{|l|c|c|c|c|c|c|c|c|}
	\hline
	indice&0&1&2&3&4&5&6&7\\
	\hline
	signe&1&1&-1&-1&1&1&-1&-1\\
	\hline
	& &i& &i& &i& &i\\
	\hline
	resultat&1&i&-1&-i&1&i&-1&-i\\
	\hline
	\end{array}
\]
On obtient les puissances de $i$ : 
\[
	cos(z)+i\ sin(z)=\sum_{n=0}^{\infty}\frac{(iz)^{n}}{n!}
\]

Ce qui correspond bien au développement en série de la fonction exponentielle:

\begin{equation}
\boxed{ e^{iz}=cos(z)+i\sin(z) \quad pour \quad z\in\mathbb{C}}
\end{equation}

Maintenant 
\[
	\boxed{ e^{-iz}=cos(z)-i sin(z)}
\]

Et il s'en suit les identités d'Euler:

\begin{equation}
\boxed{cos(z)=\frac{e^{iz}+e^{-iz}}{2} \quad et \quad sin(z)=\frac{e^{iz}-e^{-iz}}{2i} \quad pour \quad z\in\mathbb{C}}
\end{equation}

\subsection{Propriétés}
\subsubsection{Relation fonctionnelle}

Partons de la définition en série en prenant des indices de sommation différents:
\[
exp(a) exp(b) = \sum_{n=0}^{\infty}\frac{a^{n}}{n!} \sum_{k=0}^{\infty}\frac{b^{k}}{k!}
\]
D'après le produit de cauchy (voir \ref{eq_produit_cauchy}) on a $\sum_{n=0}^\infty a_n \sum_{k=0}^\infty b_k= \sum_{n=0}^\infty \sum_{k=0}^n a_{k} b_{n-k}$, donc dans notre cas:

\[
exp(a) exp(b) = \sum_{n=0}^{\infty} \sum_{k=0}^{n} \frac{a^{k}}{k!} \frac{b^{n-k}}{(n-k)!}
\]
Si on réarrange et que on insère le terme $\frac{n!}{n!}$:
\[
exp(a) exp(b) = \sum_{n=0}^{\infty} \frac{1}{n!} \sum_{k=0}^{n} \frac{n!}{k!(n-k)!} a^{k}b^{n-k}
\]
On reconnait les coefficients binomiaux
\[
exp(a) exp(b) = \sum_{n=0}^{\infty} \frac{1}{n!} \sum_{k=0}^{n} C^k_n a^{k}b^{n-k}
\]
Et on reconnait le binôme de newton (voir \ref{coef_bin}), donc:
\[
exp(a) exp(b) = \sum_{n=0}^{\infty} \frac{(a+b)^n}{n!} 
\]
Ce qui correspond bien à la série de $exp(a+b)$, finalement:
\begin{equation}
\boxed{exp(a) exp(b) = exp(a+b)}
\end{equation}

\newpage
\section{Fonction Logarithme}

\subsection{Définition et détermination principale}
\label{eq_log}
Le logarithme complexe est une \textbf{fonction multivaluée} définie à $2i\pi$ près. En effet, pour un nombre complexe $s$, la forme polaire s'écrit:
\[
s= |s| e^{i (Arg_\alpha(s)+2k\pi) }
\]
Ceci implique que, étant donnée \textbf{une détermination d'une fonction argument} $\boxed{Arg_\alpha(s)\in[-\alpha,2\pi-\alpha[}$ avec $\alpha\in[0,2\pi]$, \textbf{il existe une infinité de branches} associées aux valeurs de $k$. Sur chacune de ces branche on peut définir une fonction logarithme complexe, par:

\begin{equation}
\boxed{Ln_{(k,\alpha)}(s)=ln(|s|)+ i (Arg_\alpha(s)+2k\pi)}
\end{equation}

Les logarithmes ainsi définis \textbf{ne sont pas des fonctions entières}. L'angle $\alpha$ définit \textbf{une coupure} dans le plan complexe. La fonction n'est pas dérivable sur la demi-droite d'angle $\alpha$, visuellement:

\begin{figure}[H]
\centering
\newcommand{\echelle}{0.08}
\begin{tabular}{|cc|}
\hline
\raisebox{-1cm}{\rotatebox{90}{\textbf{Branches ($k$)}}}&
\begin{tabular}{cccccccccc}
\multicolumn{9}{c}{\textbf{Déterminations ($\alpha$)}}\\
&$0$&$\frac{\pi}{4}$&$\frac{\pi}{2}$&
$\frac{3\pi}{4}$&$\pi$&$\frac{5\pi}{4}$&
$=\frac{6\pi}{4}$&$\frac{7\pi}{4}$&$=2\pi$\\
\raisebox{0.4cm}{$-1$}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_0.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_1.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_2.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_3.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_4.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_5.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_6.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_7.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_-1_8.png}\\
\raisebox{0.4cm}{$0$}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_0.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_1.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_2.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_3.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_4.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_5.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_6.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_7.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_0_8.png}\\
\raisebox{0.4cm}{$1$}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_0.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_1.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_2.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_3.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_4.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_5.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_6.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_7.png}&
\includegraphics[scale=\echelle]{./img/cpp/Ln_1_8.png}\\
\end{tabular}\\
\hline
\end{tabular}
\end{figure}

\begin{tabular}{cc}
\begin{minipage}{12cm}

Toutes ces définitions sont les \textbf{prolongements analytiques} (voir \ref{prolongement_analytique}) d'un même objet appelé logarithme complexe. On dit que le logarithme complexe possède \textbf{deux points de branchement} 0 et $\infty$ et une infinité de \textbf{branches}.\\

\begin{itemize}
\item La \textbf{détermination principale} du logarithme consiste alors à faire le choix $\boxed{\alpha=\pi}$, on le note généralement avec une majuscule :

\begin{equation}
\boxed{Ln(s)=ln(|s|)+ i (Arg_\pi(s))}
\end{equation}

Le logarithme ainsi définit, est analytique uniquement sur $\mathbb{C}\backslash \mathbb{R^{-}}$, c'est-à-dire qu'il faut retrancher \textbf{la demi-droite des réels négatifs}.\\

\item Une \textbf{détermination secondaire} souvent utilisée, consiste elle à faire le choix $\boxed{\alpha=0}$ :

\begin{equation}
\boxed{Ln(s)=ln(|s|)+ i (Arg_0(s))}
\end{equation}

Le logarithme ainsi définit, est analytique uniquement sur $\mathbb{C}\backslash \mathbb{R^{+}}$, c'est-à-dire qu'il faut retrancher \textbf{la demi-droite des réels positifs}.\\

\end{itemize}

Le choix de la \textbf{branche principale} $k=0$ n'est pas anodin, c'est la seule branche du logarithme complexe qui coïncide avec le logarithme népérien sur l'axe des réels, car $\forall x\in\mathbb{R}\ Arg(x)=0$, donc:

\[
Ln(x)=ln(x) \quad \quad \forall x\in\mathbb{R}
\]

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{Ln}{$Ln(s)$, détermination principale sur $\mathbb{C} \backslash \mathbb{R^{-}}$}
\graphfunction{Ln_0_0}{$Ln(s)$, détermination secondaire sur $\mathbb{C} \backslash \mathbb{R^{+}}$}
\end{minipage}\\
\end{tabular}

\subsection{Intégrale d'une fonction rationnelle} 

Ici \textbf{nous considérerons la détermination secondaire} du logarithme (voir \ref{eq_log}). Ainsi celui n'est pas définit sur les réels positifs mais ne pose aucun problème partout ailleurs. Soit l'intégrale :

\[
I=\int_0^{\infty} \frac{P(t)}{Q(t)} dt
\]

Ou $P(t)$ et $Q(t)$ sont deux polynômes tel que $deg(Q)-deg(P)\ge 2$ et si Q(t) n'a pas de pôles sur les réels positifs. Considérons l'intégrale de contour suivante:

\[
J=\oint_C \frac{P(s)}{Q(s)} Ln(s) ds
\]

Ou $C$ et le chemin décrit par la figure suivante:

\begin{center}
\begin{tikzpicture}[scale=0.5]
\TikzGraphC{-4}{-4}{4}{4}
\draw [red](2.5,2.5) node [above right] {$C_R$};
\draw [red](-0.2,0.2) node [above left] {$C_r$};
\draw [red](1.5,-0.3) node [below] {$C_a$};
\draw [red](1.5,0.3) node [above] {$C_b$};
\draw (0,2.3) node {$\bullet$} node [above right] {$\rho_1$};
\draw (1.2,1.2) node {$\bullet$} node [above right] {$\rho_2$};
\draw (1.2,-1.3) node {$\bullet$} node [below right] {$\rho_3$};
\draw [red,thick,->,>=stealth](3.5,0.3) arc (0:180:3.5);
\draw [red,thick,->,>=stealth](-3.5,-0.3) arc (180:360:3.5);
\draw [red,thick,<-,>=stealth] (3.5,0.3) -- (0,0.3);
\draw [red,thick,->,>=stealth] (3.5,-0.3) -- (0,-0.3);
\draw [red,thick,>=stealth] (-3.5,0.3) -- (-3.5,-0.3);
\draw [red,thick,<-,>=stealth]( 0, 0.3) arc (90:270:0.3);
\end{tikzpicture}
\end{center}

Avec $\rho_i$ les pôles de $Q(s)$. Cette intégrale peut se décomposer en 4 parties :

\[
J=\int_{C_a} \frac{P(s)}{Q(s)} Ln(s) ds + \int_{C_b} \frac{P(s)}{Q(s)} (Ln(s)) ds + \int_{C_r} \frac{P(s)}{Q(s)} (Ln(s)) ds + \int_{C_R} \frac{P(s)}{Q(s)} (Ln(s)) ds
\]

Mais lorsque $r\to 0$ et $R\to\infty$, les contributions de $C_r$ et $C_R$ sont nulle si $deg(Q)-deg(P)\ge 2$, on peut écrire :

\[
J=\int_\infty^{0} \frac{P(s)}{Q(s)} Ln(s) ds + \int_0^{\infty} \frac{P(s)}{Q(s)} (Ln(s)+2i\pi) ds
\]

Miraculeusement le logarithme se compense et on obtient:

\[
J=2i\pi \int_0^{\infty} \frac{P(s)}{Q(s)}  ds
\]

\[
J=2i\pi I
\]

Maintenant d'après le théorème des résidus (voir \ref{eq_residus}) on a (signe moins car on est dans le sens horaire):

\[
J=- 2i\pi \sum_{i=0}^{P} Res\left( \frac{P(s)}{Q(s)} Ln(s) ,\rho_i\right)
\]

Comme $J=2i\pi I$, on obtient donc pour I:

\begin{equation}
\boxed{\int_0^{\infty} \frac{P(t)}{Q(t)} dt=-\sum_{i=0}^{P} Res\left( \frac{P(s)}{Q(s)} Ln(s) ,\rho_i\right)}
\end{equation}

Ou $\rho_i$ sont les pôles de $Q(s)$.

\subsubsection{L'intégrale : $I=\int_0^{\infty} \frac{1}{(t^2+a^2)} dt$}

Ici \textbf{nous considérerons la détermination secondaire} du logarithme (voir \ref{eq_log}). Ainsi celui n'est pas définit sur les réels positifs mais ne pose aucun problème partout ailleurs. Soit l'intégrale :
\[
I=\int_0^{\infty} \frac{1}{(t^2+a^2)} dt
\]
On à les pôles $\rho_i={ia,-ia}$, et d'après le résultat précédent:

\[
I=- \sum_{i=0}^{P} Res\left( \frac{Ln(s)}{(s-ia)(s+ia)}  ,\rho_i\right)
\]

Donc
\[
I=-Res\left( \frac{Ln(s)}{(s^2+a^2)}  ,-ia \right)-Res\left( \frac{Ln(s)}{(s^2+a^2)} ,ia \right)
\]

Soit 
\[
I=-\lim_{s\to -ia} \frac{Ln(s)}{(s-ia)}  - \lim_{s\to ia} \frac{Ln(s)}{(s+ia)}
\]

\[
I=\frac{Ln(-ia)-Ln(ia)}{2ia}
\]

Ce qui donne avec la détermination considérée:

\[
I=\frac{i\frac{3\pi}{2}-i\frac{\pi}{2}}{2ia}=\frac{i\pi}{2ia}
\]

Finalement
\[
\boxed{\int_0^{\infty} \frac{1}{(t^2+a^2)} dt=\frac{\pi}{2a}}
\]

\subsubsection{L'intégrale : $I=\int_0^{\infty} \frac{t}{(t^2+1)(t+2)} dt$}


Ici \textbf{nous considérerons la détermination secondaire} du logarithme (voir \ref{eq_log}). Ainsi celui n'est pas définit sur les réels positifs mais ne pose aucun problème partout ailleurs. Soit l'intégrale :

\[
I=\int_0^{\infty} \frac{t}{(t^2+1)(t+2)} dt
\]
On à les pôles $\rho_i={i,-i,-2}$, et d'après le résultat précédent:
\[
I=-\sum_{i=0}^{P} Res\left( \frac{s}{(s-i)(s+i)(s+2)} Ln(s) ,\rho_i\right)
\]
Donc
\[
I=-Res\left( \frac{s}{(s^2+1)(s+2)} Ln(s) ,-i \right)-Res\left( \frac{s}{(s^2+1)(s+2)} Ln(s) ,i \right)-Res\left( \frac{s}{(s^2+1)(s+2)} Ln(s) ,-2 \right)
\]
Soit 
\[
I=-\lim_{s\to -i} \frac{s}{(s-i)(s+2)} Ln(s) - \lim_{s\to i} \frac{s}{(s+i)(s+2)} Ln(s) - \lim_{s\to -2} \frac{s}{(s^2+1)} Ln(s)
\]

\[
I=-\frac{Ln(-i)}{10}\left(2+i\right)-\frac{Ln(i)}{10}\left(2-i\right)+\frac{2}{5}( Ln(-2) )
\]

Mais ici il faut faire trés attention car $Ln(-2)$ \textbf{n'est pas définit sur la branche principale} avec la détermination $\alpha=\pi$, il faut choisir une autre détermination pour l'argument, $\alpha=0$ convient :

\[
I=-\frac{3i\pi}{20}\left(2+i\right)-\frac{i\pi}{20}\left(2-i\right)+\frac{2}{5}( ln(2)+i\pi )
\]

\[
I=-\frac{4i\pi}{10}+\frac{2\pi}{20}+\frac{4i\pi+4ln(2)}{10}
\]

\[
I=\frac{\pi}{10}+\frac{4ln(2)}{10}
\]

Finalement
\[
\boxed{\int_0^{\infty} \frac{t}{(t^2+1)(t+2)} dt=\frac{\pi+ln(16)}{10}}
\]

\newpage
\section{Les exponentielles intégrales : $E_n(s)$ }
\subsection{Définition}
\label{eq_exp_int}
Les exponentielles intégrales sont définies par les intégrales de la forme :

\begin{equation}
\boxed{E_n(s)=\int_1^{\infty} \frac{e^{-st}}{t^n} dt \quad avec \quad \Re(s)>0}
\end{equation}

\subsection{Propriétés\workon}
\subsubsection{Dérivées}
Si on considère la dérivée par rapport à $s$ :
\[
\frac{dE_n(s)}{ds}=\int_1^{\infty} \frac{-t e^{-st}}{t^n} dt=-\int_1^{\infty} \frac{ e^{-st}}{t^{n-1}} dt
\]
Finalement
\begin{equation}
\boxed{\frac{dE_n(s)}{dz}=-E_{n-1}(s)}
\end{equation}

Et plus généralement par itération de la relation précédente :

\begin{equation}
\boxed{\frac{d^k E_n(s)}{ds^k}=(-1)^k E_{n-k}(s)}
\end{equation}

\subsubsection{Récurrence}
\label{eq_exp_int_rec}

Si on part de la définition:
\[
E_n(s)=\int_1^{\infty} \frac{e^{-st}}{t^n} dt
\]
Et que on fait une intégration par partie en posant :
\[
\intp{\frac{1}{t^n}}{-\frac{n}{t^{n+1}}}{e^{-st}}{\frac{e^{-st}}{-s}}{}{}
\]
Cela donne 
\[
E_n(s)=\left[ \frac{e^{-st}}{-s t^n} \right]_1^{\infty}-\int_1^{\infty} \frac{n}{t^{n+1}}\frac{e^{-st}}{s} dt
\]
Comme $\Re(s)>0$ le crochet se simplifie 
\[
E_n(s)=\frac{e^{-s}}{s}- \frac{n}{s} \int_1^{\infty} \frac{e^{-st}}{t^{n+1}} dt
\]
Soit
\[
s E_n(s) = e^{-s} - n E_{n+1}(s)
\]
Finalement
\begin{equation}
\boxed{ n E_{n+1}(s) = e^{-s} - s E_n(s)}
\end{equation}


\subsection{Représentations\workon}
\subsubsection{Construction par récurrence}

Si on part de la relation de récurrence obtenue (voir \ref{eq_exp_int_rec}) :
\[
n E_{n+1}(s) = e^{-s} - s E_n(s)
\]
On reconnait alors une suite aritmético-géométrique :
\[
E_{n+1}=A E_n + B \quad avec\quad A=-\frac{s}{n} \quad et\quad B=\frac{e^{-s}}{n}
\]
Le terme de rang $n$ s'exprime alors en fonction du terme de rang $1$ par :
\[
E_{n}=A^{n-1}E_1+B\sum_{k=0}^{n-2}A^k
\]
Si on remplace A et B par leurs expressions, on obtient finalement :

\begin{equation}
\boxed{E_{n}(s)=\left(-\frac{s}{n}\right)^{n-1}E_1(s)+\frac{e^{-s}}{n}\sum_{k=0}^{n-2}\left(-\frac{s}{n}\right)^k}
\end{equation}

On peut donc construire toutes les fonctions $E_n$ par récurrence si on connait le cas $n=1$.

\newpage
\section{L'exponentielle intégrale : $Ei(s)$ }
\label{eq_ei}
\subsection{Définition}
\begin{tabular}{cc}
\begin{minipage}{12cm}
On définit l'exponentielle intégrale par :

\begin{equation}
\boxed{Ei(s)=\int_{-\infty}^{s} \frac{e^t}{t}dt }
\end{equation}

L'intégrante est singulière en $0$, l'intégrale est donc à prendre au sens de la valeur principale de Cauchy soit:

\[
\boxed{Ei(s)=\lim_{\epsilon\to 0} \left(\int_{-\infty}^{-\epsilon}  \frac{e^t}{t}dt +\int_{\epsilon}^{s} \frac{e^t}{t}dt \right)}
\]

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{ei}{$Ei(s)$, prolongée sur $\mathbb{C}$}
\end{minipage}\\
\end{tabular}

\subsection{Propriétés}
\subsubsection{Dérivées}
\label{eq_ei_deriv}
La dérivée de l'exponentielle intégrale:
\[
Ei'(s)=\left[\frac{e^t}{t}\right]_{-\infty}^{s}=\frac{e^{s}}{s}-\lim_{t\to-\infty} \frac{e^t}{t}
\]
Mais l'exponentielle l'emporte dans la limite, celle-ci est donc nulle, donc :

\begin{equation}
\boxed{Ei'(s)=\frac{e^{s}}{s}}
\end{equation}

\subsubsection{Lien avec $E_1(s)$}

Si on part de la définition de $E_1(s)$ (voir \ref{eq_exp_int}):

\[
E_1(s)=\int_1^{\infty} \frac{e^{-st}}{t} dt
\]

Si on pose $T=-st$ alors $t=-\frac{T}{s}$ et $dt=-\frac{dT}{s}$ donc 

\[
E_1(s)=\int_{-s}^{-\infty} \frac{e^{T}}{T} dT
\]

Et si on remplace la variable muette $T$ par $t$ pour retrouver nos notations et que on inverse les bornes cela donne :

\[
E_1(s)=-\int_{-\infty}^{-s} \frac{e^{t}}{t} dt
\]

On reconnait alors l'exponentielle intégrale définie précedement, donc :

\begin{equation}
\boxed{E_1(s)=-E_i(-s)}
\end{equation}

\subsubsection{Lien avec $Li(s)$\todo}

\subsection{Représentations}
\subsubsection{Développement en série (Autour de $1$)}

Si on  part de l'expression de la dérivée de l'exponentielle intégrale (voir \ref{eq_ei_deriv}):
\[
Ei'(s)=\frac{e^{s}}{s}
\]
Et si on remplace l'exponentielle par son développement en  série de Taylor (voir \ref{dl_exponentiel}) :  
\[
Ei'(s)=\frac{1}{s}\sum_{n=0}^{\infty}\frac{s^{n}}{n!}=\sum_{n=0}^{\infty}\frac{s^{n-1}}{n!}=\frac{1}{s}+\sum_{n=1}^{\infty}\frac{s^{n-1}}{n!}
\]
Si on intègre cette expression, on obtient:

\[
Ei(s)=C+ln(s)+\sum_{n=1}^{\infty}\frac{s^{n}}{n!n}
\]

Avec C une constante d'intégration, mais on peut également écrire que pour $s=1$:
\[
Ei(1)=C+\sum_{n=1}^{\infty}\frac{1}{n!n}
\]
D'où on tire
\[
C=Ei(1)-\sum_{n=1}^{\infty}\frac{1}{n!n}
\]
Considérons maintenant l'intégrale:
\[
\int_0^1 \frac{e^t-1}{t} dt= \int_0^1 \frac{ \sum_{n=0}^{\infty}\frac{t^n}{n} -1}{t} dt=\int_0^1 \sum_{1}^{\infty} \frac{t^{n-1}}{n!} dt =\sum_{1}^{\infty} \frac{1^{n}}{n!n} 
\]
Donc on obtient pour la constante $C$ : 
\[
C=Ei(1)-\int_0^1 \frac{e^t-1}{t} dt
\]
\[
C=\int_{-\infty}^{1}\frac{e^t}{t} dt -\int_0^1 \frac{e^t-1}{t} dt
\]
Si on décompose la première intégrale en trois parties :
\[
C=\int_{-\infty}^{-1}\frac{e^t}{t} dt+\lim_{\epsilon\to0^{+}}\left(\int_{-1}^{-\epsilon}\frac{e^t}{t} dt + \int_{\epsilon}^{1}\frac{e^t}{t} dt \right) -\int_0^1 \frac{e^t-1}{t} dt
\]
Si on réorganise un peu :
\[
C=\int_{-\infty}^{-1}\frac{e^t}{t} dt+\lim_{\epsilon\to0^{+}}\left(-\int_{-\epsilon}^{-1}\frac{e^t}{t} dt + \int_{\epsilon}^{1}\frac{e^t}{t} dt -\int_\epsilon^1 \frac{e^t-1}{t} dt \right)
\]
\[
C=-\int_{1}^{\infty}\frac{e^{-t}}{t} dt+\lim_{\epsilon\to0^{+}}\left(-\int_{\epsilon}^{1}\frac{e^{-t}}{t} dt + \int_{\epsilon}^{1}\frac{e^t}{t} dt -\int_\epsilon^1 \frac{e^t-1}{t} dt \right)
\]
Les trois intégrales de droite peuvent alors être regroupées, on obtient :
\[
C=\int_{0}^1 \frac{1-e^{-t}}{t} dt - \int_{1}^{\infty}\frac{e^{-t}}{t} dt
\]
Maintenant si on fait deux intégrations par parties en posant :
\[
\intp{1-e^{-t}}{e^{-t}}{\frac{1}{t}}{ln(t)}{}{} \quad et \quad \intp{e^{-t}}{-e^{-t}}{\frac{1}{t}}{ln(t)}{}{}
\]
Cela donne
\[
C=\left[ (1-e^{-t})ln(t) \right]_0^1 - \int_0^1 ln(t) e^{-t} dt - \left[ e^{-t}ln(t) \right]_1^\infty - \int_1^\infty ln(t) e^{-t} dt
\]
Mais les crochet sont nuls et si on regroupe les deux intégrales on obtient :
\[
C=- \int_0^\infty ln(t) e^{-t} dt
\]
On reconnait ici la dérivée de la fonction $\Gamma$ (voir \ref{eq_gamma_diff}) évaluée en 1:

\[
C=- \Gamma'(1)
\]
Or on a montré que $\Gamma'(1)=-\gamma$ ( voir \ref{eq_gamma_prime_1}), donc finalement, le développement de l'exponentielle intégrale s'écrit:

\begin{equation}
\boxed{Ei(s)=\gamma+ln(s)+\sum_{n=1}^{\infty}\frac{s^{n}}{n!n}}
\end{equation}

\notes{La série $\sum_{n=1}^{\infty}\frac{s^{n}}{n!n}$ définit une fonction entière nommée $Ein(s)$}

\subsubsection{Développement en série (Autour de $\infty$)\todo}

Si on part de la définition :
\[
Ei(s)=\int_{-\infty}^{s} \frac{e^t}{t}dt
\]
Et que on fait une intégration par partie en posant:
\[
\intp{\frac{1}{t}}{-\frac{1}{t^2}}{e^{t}}{e^{t}}{}{}
\]
Cela donne
\[
Ei(s)=\left[ \frac{e^t}{t} \right]_{-\infty}^{s}+ \int_{-\infty}^{s} \frac{e^t}{t^2} dt
\]
\[
Ei(s)=\frac{e^s}{s}+ \int_{-\infty}^{s} \frac{e^t}{t^2} dt
\]

\newpage
\section{Logarithme intégral : $Li(s)$}
\subsection{Définition}
\label{eq_ln_int}

\begin{tabular}{cc}
\begin{minipage}{12cm}
On définit le logarithme intégral par l'intégrale :

\begin{equation}
\boxed{Li(s)=\int_{0}^{s}\frac{du}{ln(u)}}
\end{equation}

L'intégrante est singulière en $1$, l'intégrale est donc à prendre au sens de la valeur principale de Cauchy soit:

\[
\boxed{Li(s)=\lim_{\epsilon\to 0} \left(\int_0^{1-\epsilon} \frac{du}{ln(u)}+\int_{1+\epsilon}^{s} \frac{du}{ln(u)}\right)}
\]

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{li}{$Li(s)$, prolongée sur $\mathbb{C}$}
\end{minipage}\\
\end{tabular}

\subsection{Propriétés}

\subsubsection{Dérivées}
\label{eq_li_deriv}
La dérivée du logarithme intégral est triviale:

\begin{equation}
\boxed{Li'(s)=\frac{1}{ln(s)}}
\end{equation}

\subsubsection{Lien avec $Ei$}

Si on part de la définition de $Li(s)$:
\[
Li(s)=\int_{0}^{s}\frac{du}{ln(u)}
\]
Si on pose $u=e^v$ soit $du=e^v dv$ l'intégrale devient:
\[
Li(s)=\int_{-\infty}^{ln(s)}\frac{e^v}{v}dv
\]
Soit finalement 
\begin{equation}
\boxed{Li(s)=Ei(ln(s))}
\end{equation}

\subsection{Représentations\workon}

\subsubsection{Calcul de $Li(t^s)$}
\label{eq_li_power}
Soit $Re(s)>0$ 

\[
Ei(s\ ln(t) )=\int_{-\infty}^{s ln(t)} \frac{e^u}{u}du
\]

Si on pose $v=e^{\frac{u}{s}}$ et $u=s ln(v)$ et $du=\frac{s}{v} dv$, cela donne 

\[
Ei(s\ ln(t) )=\int_{0}^{t} \frac{e^{s ln(v)}}{s ln(v)}\frac{s}{v} dv
\]
\[
Ei(s\ ln(t) )=\int_{0}^{t} \frac{v^{s-1}}{ln(v)} dv
\]

Finalement 
\[
\boxed{Ei(s\ ln(t) )=\int_0^{t} \frac{u^{s-1}}{ln(u)} du }
\]

Comme 
\[
Li(t)=Ei(ln(t))
\]

On déduit que 
\[
Li(t^s)=Ei(ln(t^s))=Ei(sln(t))
\]

Donc 
\[
\boxed{Li(t^s)=\int_0^{t} \frac{u^{s-1}}{ln(u)} du }
\]

\newpage
\section{Fonction Gamma $\Gamma(s)$}
\subsection{Définition}
\label{eq_gamma}

\begin{tabular}{cc}
\begin{minipage}{12cm}



La fonction gamma est une extension de la fonction factorielle $\Gamma(n)= (n-1)!$ .On peut montrer la relation fonctionnelle $\Gamma(s+1)=s\Gamma(s)$.\\ 

On peut définir $\Gamma(s)$ par l'intégrale :
\begin{equation}
\boxed{\Gamma(s)=\int_0^{\infty} t^{s-1}e^{-t} dt}
\end{equation}

Cette formule est \textbf{valide pour $Re(s)>0$} mais on peut prolonger la fonction à l'ensemble du plan complexe de nombreuses façon, par exemple avec sa formule des compléments de Legendre (voir \ref{eq_gamma_comp1}) ou sa relation fonctionnelle (voir \ref{eq_gamma_func}).\\

La fonction $\Gamma$ est également la transformée de Mellin (voir \ref{eq_mellin}) de la fonction $f(t)=e^{-t}$ (voir \ref{eq_mellin_exp}).

\[
\Gamma(s)=\applyop{M}{e^{-t}}(s)
\]

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{gamma_512}{$\Gamma(s)$}
\end{minipage}
\end{tabular}

\subsection{Représentations}
\subsubsection{Représentation de Gauss : (première forme)}
\label{eq_gamma_gauss_form1}
Si on part de la forme de Gauss de la fonction exponentielle (voir \ref{eq_exp_gauss}):
\[
e^t=\lim_{n\to\infty} \left(1+\frac{t}{n} \right)^n
\]
La fonction $\Gamma(s)$ s'exprime alors par:
\[
\Gamma(s)=\int_0^{\infty}t^{s-1} \lim_{n\to\infty} \left(1+\frac{-t}{n} \right)^n\ dt
\]
Si on sort la limite de l'intégrale, on obtient:
\[
\Gamma(s)=\lim_{n\to\infty}\int_0^{\infty}t^{s-1} \left(1+\frac{-t}{n} \right)^n\ dt
\]
Ici dans cette expression on peut remplacer la borne supérieure de l'intégrale par $n$ car $n\to\infty$, on obtient:
\[
\Gamma(s)=\lim_{n\to\infty}\int_0^{n}t^{s-1} \left(1+\frac{-t}{n} \right)^n\ dt
\]
Maintenant si on fait une intégration par partie:

\[
\intp{\left(1+\frac{-t}{n} \right)^n}{-\frac{n}{n}\left(1+\frac{-t}{n} \right)^{n-1}}{t^{s-1}}{\frac{t^s}{s}}{0}{n}
\]
Mais on peut voir que le crochet est nul et  on obtient:
\[
\Gamma(s)=\lim_{n\to\infty}\frac{n}{s n}\int_0^{n}t^{s} \left(1+\frac{-t}{n} \right)^{n-1}\ dt
\]
Et si on recommence on obtient :
\[
\Gamma(s)=\lim_{n\to\infty}\frac{n}{s n}\frac{(n-1)}{(s+1) n}\int_0^{n}t^{s+1} \left(1+\frac{-t}{n} \right)^{n-2}\ dt
\]
Et si on recommence encore on obtient :
\[
\Gamma(s)=\lim_{n\to\infty}\frac{n}{s n}\frac{(n-1)}{(s+1) n}\frac{(n-2)}{(s+2) n}\int_0^{n}t^{s+2} \left(1+\frac{-t}{n} \right)^{n-3}\ dt
\]
Et ça continue encore et encore jusqu'à :
\[
\Gamma(s)=\lim_{n\to\infty}\frac{n}{s n}\frac{(n-1)}{(s+1) n}\frac{(n-2)}{(s+2) n}...\frac{1}{(s+n-1) n}\int_0^{n}t^{s+n-1} \ dt
\]
On peut intégrer facilement le dernier terme:
\[
\Gamma(s)=\lim_{n\to\infty}\frac{n}{s n}\frac{(n-1)}{(s+1) n}\frac{(n-2)}{(s+2) n}...\frac{1}{(s+n-1) n}\left[ \frac{t^{s+n}}{s+n}\right]_0^{n}
\]
\[
\Gamma(s)=\lim_{n\to\infty}\frac{n}{s n}\frac{(n-1)}{(s+1) n}\frac{(n-2)}{(s+2) n}...\frac{1}{(s+n-1) n} \frac{n^{s+n}}{s+n}
\]
Au numérateur on reconnait une factorielle $n!$, on peut réécrire:
\[
\Gamma(s)=\lim_{n\to\infty} n^{s+n} \frac {n!}{n^n}\frac{1}{s}\frac{1}{(s+1)}\frac{1}{(s+2)}...\frac{1}{(s+n-1)} \frac{1}{s+n}
\]
Et on peut écrire les termes restant sous forme d'un produit :

\[
\Gamma(s)=\lim_{n\to\infty} n^{s+n} \frac {n!}{n^n} \prod_{k=0}^n \frac{1}{s+k}
\]
Mais on peut encore simplifier par $n^n$:

\[
\Gamma(s)=\lim_{n\to\infty} n^{s} n! \prod_{k=0}^n \frac{1}{k+s}
\]
On peut également rentrer la factorielle dans le produit si on commence à 1, ce qui donne :

\begin{equation}
\boxed{\Gamma(s)=\lim_{n\to\infty} \frac{n^{s}}{s} \prod_{k=1}^n \frac{k}{k+s}}
\end{equation}

\subsubsection{Représentation de Gauss : (seconde forme)}
\label{eq_gamma_gauss_form2}
Il existe une seconde forme utile de cette expression que on retrouve souvent dans la littérature, si on part de :
\[
\Gamma(s)=\lim_{n\to\infty} \frac{n^{s}}{s} \prod_{k=1}^n \frac{k}{k+s}
\]
Mais en fait on peut ajouter $1$ sans conséquences pour la limite:
\[
\Gamma(s)=\lim_{n\to\infty} \frac{(n+1)^{s}}{s} \prod_{k=1}^n \frac{k}{k+s}
\]
Maintenant on peut remarquer que

\[
(n+1)^s=\left(\frac{(n+1)!}{n!}\right)^s=\prod_{k=1}^{n}\left(\frac{k+1}{k}\right)^s=\prod_{k=1}^{n}\left(1+\frac{1}{k}\right)^s
\]

Si on remplace, on obtient finalement:

\begin{equation}
\boxed{\Gamma(s)=\frac{1}{s} \prod_{k=1}^\infty \left( 1+\frac{s}{k}\right)^{-1}\left(1+\frac{1}{k}\right)^s }
\end{equation}

\subsubsection{Représentation de Hadamard-Weierstrass}
\label{eq_gamma_hw}
Partons de la représentation de Gauss obtenue précédemment (voir \ref{eq_gamma_gauss_form1}):
\[
\Gamma(s)=\lim_{n\to\infty} \frac{n^{s}}{s} \prod_{k=1}^n \frac{k}{k+s}
\]
Si on divise par $k$ dans le produit, on obtient:
\[
\Gamma(s)=\lim_{n\to\infty} \frac{n^{s}}{s} \prod_{k=1}^n \frac{1}{1+\frac{s}{k}}
\]
Mais $n^s=e^{s ln(n)}$, donc:
\[
\Gamma(s)=\lim_{n\to\infty} \frac{e^{s ln(n)}}{s} \prod_{k=1}^n \frac{1}{1+\frac{s}{k}}
\]
On peut introduire la série harmonique comme:
\[
\Gamma(s)=\lim_{n\to\infty} e^{s (\sum_{k=1}^n \frac{1}{k}-\sum_{k=1}^n \frac{1}{k}+ln(n))} \frac{1}{s} \prod_{k=1}^n \frac{1}{1+\frac{s}{k}}
\]
Si on découpe l'exponentielle:
\[
\Gamma(s)=\lim_{n\to\infty} e^{s (\sum_{k=1}^n \frac{1}{k}) }e^{-s (\sum_{k=1}^n \frac{1}{k}-ln(n))} \frac{1}{s} \prod_{k=1}^n \frac{1}{1+\frac{s}{k}}
\]
De plus $e^{s (\sum_{k=1}^n \frac{1}{k}) }=\prod_{k=1}^\infty e^\frac{s}{k}$, on peut rentrer ce résultat dans le produit et on obtient:
\[
\Gamma(s)=\lim_{n\to\infty} e^{-s (\sum_{k=1}^n \frac{1}{k}-ln(n))} \frac{1}{s} \prod_{k=1}^n \frac{e^\frac{s}{k}}{1+\frac{s}{k}}
\]
Si on passe à la limite on reconnait la constante d'Euler-Mascheroni définie par :
\label{eq_eular_mascheronis}
\begin{equation}
	\boxed{\gamma=\lim_{n\to\infty} \sum_{k=1}^n \frac{1}{k}-ln(n)}
\end{equation}

Finalement
\begin{equation}
\boxed{\Gamma(s)=\frac{e^{-\gamma s}}{s} \prod_{k=1}^{\infty} \frac{e^{\frac{s}{k}}}{(1+\frac{s}{k})}}
\end{equation}

\subsection{Propriétés}
\subsubsection{Relation fonctionnelle}
\label{eq_gamma_func}
Si on part de la définition de $\Gamma(s)$:

\[
\Gamma(s)=\int_0^{\infty} t^{s-1}e^{-t} dt 
\]
Si on fait une intégration par partie en posant:
\[
\intp{e^{-t}}{-e^{-t}}{t^{s-1}}{\frac{1}{s}t^{s}}{0}{\infty}
\]
On obtient:
\[
\Gamma(s)=\left[\frac{1}{s}t^{s}e^{-t}\right]_0^{\infty}-\int_0^{\infty}-e^{-t}\frac{1}{s}t^{s}\ dt
\]
La première partie est nulle car l'exponentielle l'emporte sur tout polynôme:
\[
\Gamma(s)=\frac{1}{s}\int_0^{\infty}t^{s} e^{-t}\ dt
\]
Soit

\begin{equation}
\boxed{\Gamma(s+1)=s\Gamma(s)}
\end{equation}

\noindent
Cette relation permet d'étendre par prolongement analytique la fonction $\Gamma(s)$ à l'ensemble du plan complexe.

\subsubsection{Lien avec la factorielle et formule de récurrence}
\label{eq_gamma_rec}

Si on part de la relation fonctionnelle (voir \ref{eq_gamma_func}) :
\[
\Gamma(s+1)=s\Gamma(s)
\]
Du coup par récurrence :
\[
\begin{array}{l}
\Gamma(s+n)=(s+n-1)\Gamma(s+n-1)\\
\Gamma(s+n)=(s+n-1)(s+n-2)\Gamma(s+n-2)\\
\Gamma(s+n)=(s+n-1)(s+n-2)...(s+1)\Gamma(s+1)\\
\end{array}
\]
Finalement 
\[
\boxed{\Gamma(s+n)=\Gamma(s+1)\prod_{k=1}^{n-1}(s+k)}
\]
Et aussi si on va plus loin
\[
\boxed{\Gamma(s+n)=\Gamma(s)\prod_{k=0}^{n-1}(s+k) \quad Soit \quad \Gamma(s)=\frac{\Gamma(s+n)}{\prod_{k=0}^{n-1}(s+k)}}
\]
En particulier pour $s=0$
\[
\Gamma(n)= \Gamma(1) (n-1)! 
\]
Mais $\Gamma(1)=1$ (voir \ref{eq_gamma_one}) on obtient la relation avec la factorielle:
\begin{equation}
\boxed{\Gamma(n)= (n-1)! \quad ou \quad n!=\Gamma(n+1)}
\end{equation}

\subsubsection{Formule des compléments}
\label{eq_gamma_comp1}

D'après la représentation de Gauss (voir \ref{eq_gamma_gauss_form1}):

\[
\Gamma(s)\Gamma(-s)=\lim_{n\to\infty} \frac{n^{s}}{s}\frac{n^{-s}}{-s} \prod_{k=1}^n \frac{k}{k+s} \frac{k}{k-s}
\]
Comme $n$ n'apparait plus ailleurs que sur la borne du produit, on peut écrire :
\[
\Gamma(s)\Gamma(-s)= \frac{1}{s}\frac{1}{-s} \prod_{k=1}^\infty \frac{1}{(1+\frac{s}{k})(1-\frac{s}{k})} 
\]
Et $\Gamma(s+1)=s\Gamma(s)$ implique que $\Gamma(1-s)=-s\Gamma(-s)$
\[
-s\Gamma(s)\Gamma(-s)=\frac{1}{s} \prod_{k=1}^\infty \frac{1}{(1+\frac{s}{k})(1-\frac{s}{k})} 
\]
\[
\Gamma(s)\Gamma(1-s)=\frac{1}{s} \prod_{k=1}^\infty \frac{1}{(1-\frac{s^2}{k^2})} 
\]
Maintenant si on se rappelle de la forme en produit de H.W du sinus $sin(z)= z \prod_{n=1}^\infty \left(1-\frac{z^2}{(n\pi)^2}\right)$ (voir \ref{eq_sinus_product})

\[
\Gamma(s)\Gamma(1-s)= \frac{1}{ s \prod_{k=1}^\infty (1-\frac{s^2}{k^2})}
\]
et
\[
sin(\pi s)= \pi s \prod_{n=1}^\infty \left(1-\frac{s^2}{n^2}\right)
\]
Finalement 
\begin{equation}
\boxed{\Gamma(s)\Gamma(1-s)= \frac{\pi}{sin(\pi s)}}
\end{equation}

\subsubsection{Dérivées k-ièmes}
\label{eq_gamma_diff}
Si on dérive k fois l'expression, on trouve:
\[
\Gamma^{(k)}(s)= \int_0^{\infty} \applyopp{D}{k}{t^{s-1}} e^{-t} dt
\]
Comme $t^{s-1}=e^{(s-1)ln(t)}$ on obtient trivialement que :
\begin{equation}
\boxed{\Gamma^{(k)}(s)= \int_0^{\infty} ln(t)^k t^{s-1} e^{-t} dt}
\end{equation}

Pour l'étude de la dérivée logarithmique, voir \ref{eq_digamma}.

\subsection{La fonction Beta : $B(x,y)$}
\label{eq_gamma_beta}
On définit la fonction Beta pour tout complexes $x$ et $y$ par :
\begin{equation}
\boxed{B(x,y)=\int_0^1 t^{x-1} (1-t)^{y-1} dt}
\end{equation}
Nous allons voir que cette fonction de deux paramètres est en lien étroit avec la fonction $\Gamma$ que nous étudions ici.



\subsubsection{Lien avec la fonction Gamma}
\label{eq_gamma_beta_link}
Si on part de 
\[
\Gamma(x)\Gamma(y)=\int_0^{\infty} u^{x-1}e^{-u} du \int_0^{\infty} v^{y-1}e^{-v} dv
\]
\[
\Gamma(x)\Gamma(y)=\int_{u=0}^{\infty}\int_{v=0}^{\infty} u^{x-1} v^{y-1}e^{-(u+v)} dv du
\]
Maintenant on va faire le changement de variable: $u=z t$ et $v=z(1-t)$, le déterminant du Jacobien de ce changement s'exprime par:
\[
|J(z,t)|=\left|\begin{array}{cc}
t&z\\
(1-t)& -z\\
\end{array}\right|=|-z|=z
\]
Du coup on obtient :
\[
\Gamma(x)\Gamma(y)=\int_{z=0}^{\infty}\int_{t=0}^{1} (zt)^{x-1} (z(1-t))^{y-1}e^{-z} |J(z,t)| dt dz
\]
Si on sépare en deux intégrales:
\[
\Gamma(x)\Gamma(y)=\int_{z=0}^{\infty} z^{x+y-1} e^{-z} dz\int_{t=0}^{1} t^{x-1} (1-t)^{y-1}  dt
\]
On reconnait les fonctions $\Gamma(x+y)$ et $B(x,y)$
\[
\Gamma(x)\Gamma(y)=\Gamma(x+y) B(x,y)
\]
Soit finalement
\begin{equation}
\boxed{B(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}}
\end{equation}

Cette relation nous enseigne également que la fonction Beta est symétrique sur l'inversion des entrées:

\begin{equation}
\boxed{B(x,y)=B(y,x)}
\end{equation}

\subsubsection{Formule de duplication de Legendre}
\label{eq_gamma_beta_legendre}
Considérons, la fonction $B(s,s)$ :
\[
B(s,s)=\int_0^1 t^{s-1} (1-t)^{s-1} dt
\]
Si on pose $t=\frac{1+u}{2}$, soit $dt=\frac{du}{2}$, on obtient:
\[
B(s,s)=\int_{-1}^1 \left(\frac{1+u}{2}\right)^{s-1} \left(\frac{1-u}{2}\right)^{s-1} \frac{du}{2}
\]
Si on sort les puissances de $2$ :
\[
B(s,s)=\frac{1}{2^{2s-1}}\int_{-1}^1 (1+u)^{s-1} (1-u)^{s-1} du
\]
Si on regroupe les termes et que on remarque que on à une fonction pair donc que on peut extraire un facteur $2$:
\[
B(s,s)=\frac{2}{2^{2s-1}}\int_{0}^1 (1-u^2)^{s-1} du
\]
Mais maintenant si on considère la fonction :
\[
B\left(\frac{1}{2},s\right)=\int_0^1 t^{-\frac{1}{2}} (1-t)^{s-1} dt 
\]
Si on pose $t=u^2$ ce qui donne $dt = 2u du$, on obtient alors:
\[
B\left(\frac{1}{2},s\right)=\int_0^1 u^{-1} (1-u^2)^{s-1} 2 u du 
\]
\[
B\left(\frac{1}{2},s\right)=2\int_0^1 (1-u^2)^{s-1} du 
\]
Ce qui est l'intégrale que nous avions obtenue précédemment, donc :
\[
\boxed{B(s,s)=\frac{B\left(\frac{1}{2},s\right)}{2^{2s-1}}}
\]

Mais d'après la relation obtenue précédemment (voir \ref{eq_gamma_beta_link}):

\[
	2^{2s-1} \frac{\Gamma(s)\Gamma(s)}{\Gamma(2s)}= \frac{\Gamma\left(\frac{1}{2}\right)\Gamma(s)}{\Gamma\left(\frac{1}{2}+s\right)}
\]
Si on simplifie et que on remplace $\Gamma\left(\frac{1}{2}\right)$ par $\sqrt{\pi}$ (voir \ref{eq_gamma_half})

\[
	2^{2s-1} \frac{\Gamma(s)}{\Gamma(2s)}= \frac{\sqrt{\pi}}{\Gamma\left(\frac{1}{2}+s\right)}
\]

Soit, finalement
\begin{equation}
\boxed{\Gamma(2s)=2^{2s-1}\pi^{-\frac{1}{2}} \Gamma(s)\Gamma\left(s+\frac{1}{2}\right)}
\end{equation}

\subsubsection{Formule des compléments\todo}
\label{eq_gamma_comp2}

Une autre façon de montrer la formule des compléments obtenue (voir \ref{eq_gamma_comp1} ), et de partir du fait que d'après la relation obtenue précédemment (voir \ref{eq_gamma_beta_link}), on a:

\[
B(s,1-s)=\frac{\Gamma(s)\Gamma(1-s)}{\Gamma(1)}=\Gamma(s)\Gamma(1-s)
\]
Et d'apres la définition de la fonction Beta (voir \ref{eq_gamma_beta})
\[
B(s,1-s)=\int_0^1 t^{s-1} (1-t)^{-s} dt
\]

Finalement
\begin{equation}
\boxed{\Gamma(s)\Gamma(1-s)= \frac{\pi}{sin(\pi s)}}
\end{equation}

\subsection{Valeurs particulières}

\subsubsection{Pôles et résidus pour les entiers négatifs : $\Gamma(-n)$ }

Si on part de la représentation de Gauss obtenue (voir \ref{eq_gamma_gauss_form2}) :

\[
\Gamma(s)=\frac{1}{s} \prod_{k=1}^\infty \left( 1+\frac{s}{k}\right)^{-1}\left(1+\frac{1}{k}\right)^s
\]

Cette relation met clairement en évidence le fait que $\Gamma(s)$ possède \textbf{un pôle simple pour chaque entier négatif}. Aussi si on rappelle la formule de recurence obtenue (voir \ref{eq_gamma_rec}) :

\[
\Gamma(s)=\frac{\Gamma(s+n)}{\prod_{k=0}^{n-1}(s+k)}
\]

Soit

\[
(s+n)\Gamma(s)=\frac{\Gamma(s+n+1)}{\prod_{k=0}^{n-1}(s+k)}
\] 

Calculons les résidus (voir \ref{eq_residus_def}) associés à ces pôles:

\begin{eqnarray*}
Res(\Gamma,-n)&=&\lim_{s\to -n} (s+n) \Gamma(s)\\
&=&\lim_{s\to -n}\frac{\Gamma(s+n+1)}{\prod_{k=0}^{n-1}(s+k)}\\
&=&\lim_{s\to -n}\frac{1}{\prod_{k=0}^{n-1}(s+k)}\\
&=&\frac{1}{(-n)(-n+1)(-n+2)...(-n+n-2)(-n+n-1) }\\
&=&\frac{1}{(-n)(-n+1)(-n+2)...(-2)(-1) }\\
&=&\frac{(-1)^n}{n!}\\
\end{eqnarray*}

Finalement on a :

\[
\boxed{Res(\Gamma,-n)=\frac{(-1)^n}{n!}}
\]

\subsubsection{Valeur en 0 : $\Gamma(0)$}
\label{eq_gamma_zero}

Partons de la relation (voir \ref{eq_gamma_comp1}) :
\[
	\Gamma(s)\Gamma(1-s)= \frac{\pi}{sin(\pi s)}
\]
Si on regarde 
\[
	\lim_{s\to 0} \Gamma(s) = \lim_{s\to 0}\frac{\pi}{sin(\pi s)} \frac{1}{\Gamma(1-s)}
\]
Mais $\lim_{s\to 0} \Gamma(1-s)=\Gamma(1)=1$ et de plus $sin(x)\approx x$ lorsque $x\to 0$, donc:
\[
	\lim_{s\to 0} \Gamma(s) = \lim_{s\to 0}\frac{\pi}{\pi s}=\lim_{s\to 0}\frac{1}{s}
\]
Finalement
\begin{equation}
\boxed{\lim_{s\to 0} \Gamma(s)=\pm \infty}
\end{equation}
La fonction possède donc un pôle d'ordre 1 en zéro, calculons le résidu de ce pôle:

\[
Res(\Gamma,0)=\lim_{s\to 0} s \Gamma(s)
\]
Mais avec la relation fonctionnelle, on obtient que:
\[
Res(\Gamma,0)=\lim_{s\to 0} \Gamma(s+1)=\Gamma(1)
\]
Donc
\begin{equation}
\boxed{Res(\Gamma,0)=1}
\end{equation}

\subsubsection{Valeur en 1 : $\Gamma(1)$}
\label{eq_gamma_one}
Partons de la définition:
\[
	\Gamma(s)=\int_0^{\infty} t^{s-1}e^{-t} dt
\]
\[
	\Gamma(1)=\int_0^{\infty} e^{-t} dt=\left[ -e^{-t} \right]_0^{\infty}
\]
\[
	\boxed{\Gamma(1)=1}
\]

Ce qui vérifie bien que la factorielle en zero vaut $\Gamma(1)=0!=1$.

\subsubsection{La constante d'Euler-Mascheroni : $\Gamma'(1)$}
\label{eq_gamma_prime_1}

Comme $\Gamma(1)=1$, on a, avec la fonction digamma (voir \ref{eq_digamma})
\[
\Gamma'(1)=\frac{\Gamma'(1)}{\Gamma(1)}=\psi(1)
\]
Or $\psi$ s'écrit (voir \ref{eq_digamma_sum}) :  
\[
\psi(s)=-\gamma+\sum_{k=1}^{\infty} \frac{s-1}{k(k+s-1)}
\]
Si on prend la valeur en $s=1$ :
\[
\psi(1)=-\gamma
\]
Donc 
\begin{equation}
\boxed{\Gamma'(1)=-\gamma}
\end{equation}

\notes{Ceci fournit une nouvelle formule bien commode pour la constante $\gamma$.}

\subsubsection{Valeur en $\frac{1}{2}$ par une intégrale de Gauss : $\Gamma(\frac{1}{2})$}
\label{eq_gauss_int}
\[
\Gamma\left(\frac{1}{2}\right)=\int_0^{\infty} t^{-\frac{1}{2}}e^{-t} dt
\]
Posons $t=k u^2$ et $dt=2 u k\ du$, on obtient:
\[
\Gamma\left(\frac{1}{2}\right)=\int_0^{\infty} (k u^2)^{-\frac{1}{2}}e^{-(k u^2)} 2 u k\ du
\]
\[
\Gamma\left(\frac{1}{2}\right)=2\sqrt{k} \int_0^{\infty} e^{-k u^2}\ du
\]
Ici comme la fonction est paire on peut rentrer le facteur $2$ dans l'intégrale et changer les bornes pour obtenir :
\[
\Gamma\left(\frac{1}{2}\right)=\sqrt{k} \int_{-\infty}^{\infty} e^{-k u^2}\ du
\]
Ici on obtient une intégrale de Gauss $I=\int_{-\infty}^{\infty} e^{-k u^2}\ du$ dont le résultat est connu mais que nous allons recalculer. L'astuce consiste à calculer $I^2$:

\[
I^2=\int_{-\infty}^{\infty}e^{-kx^2} dx \int_{-\infty}^{\infty}e^{-ky^2} dy=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}e^{-ky^2}e^{-kx^2} dx dy=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty}e^{-k(y^2+x^2)} dx dy
\]
Maintenant si on pose $x=rcos(\theta)$ et $y=rsin(\theta)$ et $dxdy=|J(r,\theta)|drd\theta$ le déterminant du jacobien s'exprime par :

\[
|J(r,\theta)|=\left|
\begin{array}{cc}
\frac{\partial x}{\partial r}&\frac{\partial x}{\partial \theta}\\
\frac{\partial y}{\partial r}&\frac{\partial y}{\partial \theta}\\
\end{array}
\right|=\left|
\begin{array}{cc}
cos(\theta)&-r sin(\theta)\\
sin(\theta)&r cos(\theta)\\
\end{array}
\right|=r cos(\theta)^2+r sin(\theta)^2 =r
\]
Donc 
\[
	I^2=\int_{0}^{2\pi} \int_{0}^{\infty}e^{-k r^2} r d\theta dr
\]
Mais comme l'intégrale ne dépend plus de $\theta$ on peut écrire :
\[
	I^2=\int_{0}^{2\pi} d\theta \int_{0}^{\infty}r e^{-kr^2} dr = 2\pi \int_{0}^{\infty}r e^{-kr^2} dr 
\]
Si on pose $s=r^2$ donc $r=\sqrt{s}$ et $dr=\frac{1}{2\sqrt{s}}ds$
\[
I^2=2\pi \int_{0}^{\infty}\sqrt{s} e^{-ks} \frac{1}{2\sqrt{s}}ds= \pi \int_{0}^{\infty} e^{-ks} ds=\pi \left[ -\frac{1}{k}e^{-ks}\right]_{0}^{\infty}=\frac{\pi}{k}
\]
Finalement on obtient l'intégrale de gauss :
\begin{equation}
	\boxed{I=\int_{-\infty}^{\infty} e^{-k u^2}\ du=\sqrt{\frac{\pi}{k}}}
\end{equation}
Et finalement
\begin{equation}
\boxed{\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}}
\end{equation}

\subsubsection{Valeur en $\frac{1}{2}$ par la formule des compléments : $\Gamma(\frac{1}{2})$}
\label{eq_gamma_half}
On peut aussi utiliser la formule des compléments de legendre (voir \ref{eq_gamma_comp1} ou \ref{eq_gamma_comp2}):

\[
\Gamma(s)\Gamma(1-s)= \frac{\pi}{sin(\pi s)}
\]
Si on prend $s=\frac{1}{2}$, on obtient:
\[
	\Gamma\left(\frac{1}{2}\right)^2= \frac{\pi}{sin\left(\frac{\pi}{2}\right)}
\]

Mais $sin\left(\frac{\pi}{2}\right)=1$, donc:
\begin{equation}
\boxed{\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}}
\end{equation}

\subsubsection{Valeurs aux entiers plus un demi: $\Gamma(n+\frac{1}{2})$}
\label{eq_gamma_half_plus}
Soit un entier $n$, on a:
\[
\Gamma(s+n)=(s+n-1)\Gamma(s+n-1)
\]
\[
\Gamma(s+n)=(s+n-1)(s+n-2)...(s+1)(s)\Gamma(s)
\]
Ce qui peut s'écrire:
\[
\Gamma(s+n)=\Gamma(s) \prod_{k=0}^{k=n-1}(s+k)
\]
Si on prend $s=\frac{1}{2}$, on obtient, (voir \ref{eq_gauss_int}):
\[
\Gamma\left(\frac{1}{2}+n\right)=\sqrt{\pi} \prod_{k=0}^{k=n-1}\left(\frac{1}{2}+k\right)
\]
Si on factorise dans le produit par $\frac{1}{2}$:
\[
\Gamma\left(\frac{1}{2}+n\right)=\frac{\sqrt{\pi}}{2^{n}} \prod_{k=0}^{k=n-1}\left(1+2k\right)
\]
Si on décale les indices de 1. et que on insère les termes pairs:
\[
\Gamma\left(\frac{1}{2}+n\right)=\frac{\sqrt{\pi}}{2^{n}} \prod_{k=1}^{k=n}\left(2k-1\right) \frac{2k}{2k}
\]
Soit 
\[
\Gamma\left(\frac{1}{2}+n\right)=\frac{(2n)!\sqrt{\pi}}{2^{n}} \prod_{k=1}^{k=n} \frac{1}{2k}
\]
Finalement
\[
\boxed{\Gamma\left(\frac{1}{2}+n\right)=\frac{(2n)!\sqrt{\pi}}{2^{2n}n!}}
\]
\subsubsection{Formule de Stirling : $\Gamma(\infty)$ }
\label{eq_gamma_stirling}

La formule de Stirling est un équivalent lorsque $n\to\infty$ de la fonction $n!$. On rappelle que (voir \ref{eq_gamma_rec}):
\[
	n!=\Gamma(n+1)
\]
Donc par définition de la fonction $\Gamma$ :
\[
	n!=\int_0^\infty t^ne^{-t} dt
\]
On cherche alors un équivalent de $t^ne^{_t}$ lorsque $n\to\infty$, si on prend le logarithme, on obtient:

\[
	n!=\int_0^\infty e^{ln(t^ne^{-t})} dt 
\]

\[
	n!=\int_0^\infty e^{nln(t)-t} dt 
\]

Maintenant si on fait le changement de variable $t=n+u$ on a $dt=du$ et $t=0 \rightarrow u=-n$

\[
	n!=\int_{-n}^\infty e^{nln(n+u)-n-u} du
\]

On factorise par $n$ dans le logarithme:

\[
	n!=\int_{-n}^\infty e^{nln\left(n\left(1+\frac{u}{n}\right)\right)-n-u} du
\]

\[
	n!=\int_{-n}^\infty e^{nln\left(1+\frac{u}{n}\right)+nln(n)-n-u} du
\]

Comme $\frac{u}{n}\to 0$ on peut approximer le logarithme par son développement de Taylor (voir \ref{eq_dl_ln}): $ln(1+z)=\sum_{k=1}^{\infty} \frac{(-1)^{k+1} z^k}{k}$

\[
	n!=\int_{-n}^\infty e^{n\left(\sum_{k=1}^{\infty} \frac{(-1)^{k+1} u^k}{kn^k}\right)+nln(n)-n-u} du
\]
Si on rentre le $n$ dans la somme :
\[
	n!=\int_{-n}^\infty e^{\left(\sum_{k=1}^{\infty} \frac{(-1)^{k+1} u^k}{kn^{k-1}}\right)+nln(n)-n-u} du
\]
Le premier terme de la somme vaut $u$ et s'annule avec le $-u$ donc on peut écrire :
\[
	n!=\int_{-n}^\infty e^{\left(\sum_{k=2}^{\infty} \frac{(-1)^{k+1} u^k}{kn^{k-1}}\right)+nln(n)-n} du
\]
Prenons les premiers termes de la somme:
\[
\sum_{k=2}^{\infty} \frac{(-1)^{k+1} u^k}{kn^{k-1}}\approx\frac{-u^2}{2n}+\frac{u^3}{3n^2}-\frac{u^4}{4n^3}+...
\]
Cela donne :
\[
	n!\approx \frac{n^n}{e^n} \int_{-n}^\infty e^{\frac{-u^2}{2n}+\frac{u^3}{3n^2}-\frac{u^4}{4n^3}+...} du
\]
Mais lorsque $n\to\infty$ on peut garder seulement le premier terme et changer la borne inférieure et écrire :
\[
	n!\approx\frac{n^n}{e^n} \int_{-\infty}^\infty e^{-\frac{u^2}{2n}} du
\]
On se retrouve alors avec une intégrale de Gauss $\int_{-\infty}^{\infty} e^{-k u^2}\ du=\sqrt{\frac{\pi}{k}}$ (voir \ref{eq_gauss_int}), avec ici $k=\frac{1}{2n}$, on obtient la formule d'approximation de Stirling:
\begin{equation}
	\boxed{n!\approx\frac{n^n}{e^n} \sqrt{2n\pi} \quad lorsque \quad n\to\infty}
\end{equation}

\subsection{Les intégrales d'Euler}
\label{eq_gamma_euler_int}
Les deux intégrales que nous allons dériver, sont des cas très généraux, des recettes de cuisine, qui nous permettrons de calculer certaines intégrales non triviales comme les intégrales de Fresnel ou les intégrales de Dirichlet que nous verrons par la suite. On démarre avec l'expression intégrale de $\Gamma(s)$:
\[
\Gamma(s)=\int_0^{\infty} t^{s-1}e^{-t} dt
\]
Si on pose $t=p u^n$ on a $dt=npu^{n-1}\ du$ et l'expression devient :
\[
\Gamma(s)=\int_0^{\infty} (p u^n)^{s-1}e^{-p u^n} npu^{n-1}\ du
\]
\[
\Gamma(s)=\int_0^{\infty} p^{s-1+1} u^{ns-n+n-1} e^{-p u^n} n\ du
\]
Finalement on obtient :
\[
\frac{\Gamma(s)}{n p^{s}}= \int_0^{\infty} u^{ns-1} e^{-p u^n} \ du
\]
Si maintenant on considère que $p=a+ib=\rho e^{i\theta}$

\[
\frac{\Gamma(s)}{n \rho^{s}} e^{-is\theta}= \int_0^{\infty} u^{ns-1} e^{-(a+ib) u^n} \ du
\]
\begin{equation}
\boxed{\frac{\Gamma(s)}{n \rho^{s}} e^{-is\theta}= \int_0^{\infty} u^{ns-1} e^{-a u^n } e^{-i b u^n} \ du}
\end{equation}
Maintenant pour tour $s\in\Re$ on prend partie imaginaire et partie réel, on obtient

\begin{equation}
\boxed{
\left\{
\begin{array}{l}
\frac{\Gamma(s)}{n \rho^{s}} cos(s\theta) = \int_0^{\infty} u^{ns-1} e^{-a u^n } cos(b u^n ) \ du\\
\frac{\Gamma(s)}{n \rho^{s}} sin(s\theta) = \int_0^{\infty} u^{ns-1} e^{-a u^n } sin(b u^n ) \ du\\
\end{array}
\right.
}
\end{equation}

\subsubsection{Intégrales de Dirichlet}
\label{eq_gamma_dirichlet_int}

Partons de la seconde intégrale d'Euler:
\[
\frac{\Gamma(s)}{n \rho^{s}} sin(s\theta) = \int_0^{\infty} u^{ns-1} e^{-a u^n } sin(b u^n ) \ du
\]
Posons $a=0$ par conséquent $\left\{\begin{array}{l}\rho=b\\\theta=atan(\frac{b}{0})=\frac{\pi}{2} sgn(b) \end{array}\right.$, ce qui donne :

\[
\frac{\Gamma(s)}{nb^{s}} sin(s\frac{\pi}{2})sgn(b) = \int_0^{\infty} u^{ns-1} sin(b u^n ) \ du
\]

Maintenant on voudrait prendre la limite lorsque $s\to 0$ mais la fonction gamma possède un pôle en zero. Utilisons la formule des compléments de Legendre $\Gamma(s)\Gamma(1-s)= \frac{\pi}{sin(\pi s)}$ (voir \ref{eq_gamma_comp1}), donc :

\[
\frac{\pi}{nb^{s}sin(\pi s)\Gamma(1-s)} sin(s\frac{\pi}{2})sgn(b) = \int_0^{\infty} u^{ns-1} sin(b u^n ) \ du
\]

Maintenant si $s\to 0$ on obtient:
\[
\frac{\pi}{n\ sin(\pi s)\Gamma(1)} sin(s\frac{\pi}{2})sgn(b) = \int_0^{\infty} \frac{sin(b u^n )}{u} \ du
\]
Comme $\Gamma(1)=0!=1$, et $sin(z)\approx z$, on obtient :
\begin{equation}
\boxed{\int_0^{\infty} \frac{sin(b u^n )}{u} \ du = \frac{\pi}{2n} sgn(b)}
\end{equation}

Le cas le plus connu est pour $n=1$ et $b=1$, on obtient:
\begin{equation}
\boxed{\int_0^{\infty} \frac{sin(u)}{u} \ du = \frac{\pi}{2}}
\end{equation}

\subsubsection{Intégrales de Fresnel}
\label{eq_gamma_fresnel_int}

Partons de la forme globale de l'intégrale d'Euler (voir \ref{eq_gamma_euler_int}) :
\[
\frac{\Gamma(s)}{n \rho^{s}} e^{-is\theta}= \int_0^{\infty} u^{ns-1} e^{-a u^n } e^{-i b u^n} \ du
\]
Posons $\left\{\begin{array}{l}a=0\\s=\frac{1}{n}\\b=1 \end{array}\right.$ par conséquent $\left\{\begin{array}{l}\rho=1\\\theta=atan(\frac{1}{0})=\frac{\pi}{2} \end{array}\right.$, ce qui donne :

\begin{equation}
\boxed{\frac{\Gamma\left( \frac{1}{n} \right)}{n} e^{-\frac{i\pi}{2n}}= \int_0^{\infty} e^{-i u^n} \ du}
\end{equation}

En particulier pour $n=2$ on obtient :

\[
\frac{\Gamma\left(\frac{1}{2}\right)}{2} e^{-i\frac{\pi}{4}}= \int_0^{\infty} e^{-i u^2} \ du
\]

Mais on sait que $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ (voir \ref{eq_gauss_int}), donc :

\[
\frac{\sqrt{\pi}}{2} e^{-i\frac{\pi}{4}}= \int_0^{\infty} e^{-i u^2} \ du
\]
Mais $e^{-i\frac{\pi}{4}}=cos(\frac{\pi}{4})-i\ sin(\frac{\pi}{4})$ soit finalement :
\begin{equation}
\boxed{\int_0^{\infty} e^{-i u^2} \ du = \left( \frac{\sqrt{2\pi}}{4}-i\frac{\sqrt{2\pi}}{4} \right)}
\end{equation}
Ce qui, si on sépare les  parties réelles et imaginaires se traduit par:
\begin{equation}
\boxed{\left\{\begin{array}{l}
\int_0^{\infty} cos(z^2) dz= \frac{\sqrt{2\pi}}{4}\\
\int_0^{\infty} sin(z^2) dz= \frac{\sqrt{2\pi}}{4}\\
\end{array}\right.}
\end{equation}

Les intégrales incomplètes sont souvent notées :
\begin{equation}
\boxed{\left\{\begin{array}{l}
C(t)=\int_0^{t} cos(z^2) dz\\
S(t)=\int_0^{t} sin(z^2) dz\\
\end{array}\right.}
\end{equation}

La courbe paramétrée par $(x,y)=(C(t),S(t))$ est appelée \textbf{spirale de Cornu}.

\subsubsection{Intégrale de Fourier}
\label{eq_gamma_exp_int}
Partons de la forme globale de l'intégrale d'Euler (voir \ref{eq_gamma_euler_int}) :
\[
\frac{\Gamma(s)}{n \rho^{s}} e^{-is\theta}= \int_0^{\infty} u^{ns-1} e^{-a u^n } e^{-i b u^n} \ du
\]

Posons $\left\{\begin{array}{l}n=1\\a=0\\s=1 \end{array}\right.$ par conséquent $\left\{\begin{array}{l}\rho=b\\\theta=atan(\frac{b}{0})=\frac{\pi}{2}sgn(b) \end{array}\right.$, ce qui donne :

\[
\frac{\Gamma(1)}{b} e^{-i\frac{\pi}{2}sgn(b)}= \int_0^{\infty} e^{-i b u} \ du
\]

Finalement :
\begin{equation}
\boxed{\int_0^{\infty} e^{-i b u} \ du = i \frac{sgn(b)}{b}}
\end{equation}

De plus :
\[
\int_{-\infty}^{\infty} e^{-i b u} \ du=\int_{-\infty}^{0} e^{-i b u} \ du+\int_{0}^{\infty} e^{-i b u} \ du
\]
\[
\int_{-\infty}^{\infty} e^{-i b u} \ du=-\int_{0}^{-\infty} e^{-i b u} \ du+\int_{0}^{\infty} e^{-i b u} \ du
\]

Si on remplace $u=-v$ et $du=-dv$ dans la première intégrale :
\[
\int_{-\infty}^{\infty} e^{-i b u} \ du=\int_{0}^{\infty} e^{i b v} \ dv+\int_{0}^{\infty} e^{-i b u} \ du
\]
Donc si on remplace:
\[
\int_{-\infty}^{\infty} e^{-i b u} \ du=\frac{i}{b}-\frac{i}{b}
\]
Finalement
\begin{equation}
\boxed{\int_{-\infty}^{\infty} e^{-i b u} \ du=0 \quad avec \quad b\ne 0}
\end{equation}

Ce qui est conforme au résultat trouvé voir (voir \ref{eq_fourier_int1} ou \ref{eq_fourier_int2} ).

\subsubsection{Intégrales de Wallis}

Si on part de la définition de la fonction $B(x,y)$ (voir \ref{eq_gamma_beta}) :

\[
	B(x,y)=\int_0^{1}t^{x-1}(1-t)^{y-1}dt
\]

Et si on fait le changement de variable $t=sin(\theta)^2$, soit $(1-t)=cos(\theta)^2$ et $dt=2cos(\theta)sin(\theta)$, l'intégrale donne alors:

\[
	B(x,y)=2\int_0^{\frac{\pi}{2}}sin(\theta)^{2x-1} cos(\theta)^{2y-1}d\theta
\]

Par conséquent si on pose $x=\frac{z+1}{2}$ et $y=\frac{1}{2}$, on obtient:

\[
B\left(\frac{z+1}{2},\frac{1}{2}\right)=2\int_0^{\frac{\pi}{2}}sin(\theta)^{z} d\theta
\]

Si on pose $W(z)=\int_0^{\frac{\pi}{2}}sin(\theta)^{z}$, l'intégrale de Wallis d'argument $z$, on obtient:

\[
B\left(\frac{z+1}{2},\frac{1}{2}\right)=2 W(z)
\]

La propriété de symétrie de la fonction Beta donne directement que:

\begin{equation}
\boxed{W(z)=\int_0^{\frac{\pi}{2}}sin(\theta)^{z} d\theta=\int_0^{\frac{\pi}{2}}cos(\theta)^{z} d\theta}
\end{equation}

La fonction $B(x,y)$ s'exprime en fonction de la fonction $\Gamma(s)$ (voir \ref{eq_gamma_beta_link} ) par $B(x,y)=\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}$, ce qui donne :

\[
W(z)=\frac{1}{2}\frac{\Gamma(\frac{z+1}{2})\Gamma(\frac{1}{2})}{\Gamma(\frac{z}{2}+1)}
\]

Finalement comme $\Gamma\left(\frac{1}{2}\right)=\sqrt{\pi}$ (voir \ref{eq_gamma_half}), on obtient:
\begin{equation}
\boxed{W(z)=\frac{\sqrt{\pi}}{2}\frac{\Gamma\left(\frac{z+1}{2}\right)}{\Gamma\left(\frac{z+2}{2}\right)}}
\end{equation}

\newpage
\section{Fonctions Digamma et Polygamma: $\psi^{(n)}(s)$}
\label{eq_digamma}
\label{eq_poly_gamma}

\begin{tabular}{cc}
\begin{minipage}{12cm}
La dérivée logarithmique de la fonction $\Gamma(s)$, est nommée fonction $\psi(s)$. 
\begin{equation}
	\boxed{\psi(s)=\frac{\Gamma'(s)}{\Gamma(s)}=\left(ln(\Gamma(s))\right)'}
\end{equation}
La fonction Digamma est la première des fonctions Polygamma qui en sont les dérivées successives:
\[
	\boxed{\psi^{(n)}(s)= \applyopp{D}{n}{\psi}(s)}
\]

Si on considère la fonction $\Gamma$ comme une généralisation de la factorielle, alors la fonction $\psi$ est une généralisation des nombres harmoniques $H_n$, on démontrera que (voir \ref{eq_digamma_har}) :
\[
\boxed{\psi(n)=H_{n-1}-\gamma}
\]
\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{digamma_512}{$\psi(z)$}
\end{minipage}\\
\end{tabular}


\subsection{Représentation à partir du produit de Weierstrass de la fonction $\Gamma(s)$ }
\label{eq_gamma_log}
\label{eq_digamma_sum}

Le produit de Weierstrass de la fonction $\Gamma(s)$, ( voir \ref{eq_gamma_hw} ) :

\[
\Gamma(s)=\frac{e^{-\gamma s}}{s} \prod_{k=1}^{\infty} \frac{e^{\frac{s}{k}}}{(1+\frac{s}{k})}
\]

Si on prend le logarithme, on obtient :

\[
ln(\Gamma(s))=ln(e^{-\gamma s})-ln(s)+\sum_{k=1}^{\infty} ln\left(\frac{e^{\frac{s}{k}}}{(1+\frac{s}{k})}\right)
\]
\begin{equation}
\boxed{ln(\Gamma(s))=-\gamma s-ln(s)+\sum_{k=1}^{\infty} \frac{s}{k} - ln\left(1+\frac{s}{k}\right)}
\end{equation}

Maintenant si on prend la dérivée de chaque terme, on obtient un première expression :
\begin{equation}
\boxed{\psi(s)=-\gamma-\frac{1}{s}+\sum_{k=1}^{\infty} \frac{1}{k}-\frac{1}{k+s}}
\end{equation}

Si on sépare en deux sommes que on rentre le terme $\frac{1}{s}$ et que on décale les indices puis que on rassemble les deux sommes :
\[
\psi(s)=-\gamma+\sum_{k=1}^{\infty} \frac{1}{k}- \frac{1}{k+s-1}
\]

On obtient une seconde expression
\begin{equation}
\boxed{\psi(s)=-\gamma+\sum_{k=1}^{\infty} \frac{s-1}{k(k+s-1)}}
\end{equation}

Cette équation est valide sur l'ensemble du plan complexe sauf aux entiers négatifs. Elle permet en autre de se rendre compte que $\boxed{\psi(1)=-\gamma}$

\subsection{Relation fonctionnelle }
\label{eq_digamma_func}
Si on part de la relation fonctionnelle de la fonction $\Gamma(s)$, (voir \ref{eq_gamma_func})\\
\[
\Gamma(s+1)=s\Gamma(s)
\]
Si on prend la dérivée logarithmique, on obtient directement :
\begin{equation}
\boxed{\psi(s+1)=\frac{1}{s}+\psi(s)}
\end{equation}

\subsection{Lien avec les nombres harmoniques}
\label{eq_digamma_har}
Si on par de l'équation fonctionnelle trouvée précédemment (voir \ref{eq_digamma_func}):
\[
\psi(s+1)=\frac{1}{s}+\psi(s)
\]
Ceci implique que :
\[
\psi(s)=\frac{1}{s-1}+\psi(s-1)
\]
Et par récurrence on trouve que :
\[
\psi(n)=\frac{1}{n-1}+\psi(n-1)
\]
\[
\psi(n)=\frac{1}{n-1}+\frac{1}{n-2}+\psi(n-2)
\]
\[
\psi(n)=\frac{1}{n-1}+\frac{1}{n-2}+...+1+\psi(1)
\]
\[
\psi(n)=\sum_{k=1}^{n-1}\frac{1}{k}+\psi(1)
\]
On reconnait alors les nombres Harmoniques $H_n=\sum_{k=1}^n\frac{1}{k}$ et la constante d'Euler-Mascheroni $\psi(1)=-\gamma$: 
\begin{equation}
\boxed{\psi(n)=H_{n-1}-\gamma}
\end{equation}

\subsection{Formule de réflexion}

Si on part de la formule de réflexion :
\[
\Gamma(s)\Gamma(1-s)= \frac{\pi}{sin(\pi s)}
\]
Si on calcule la dérivée logarithmique, d'abord prenons le logarithme :
\[
ln(\Gamma(s))+ln(\Gamma(1-s))= ln(\pi)-ln(sin(\pi s))
\]
Maintenant, si on prend la dérivée :
\[
\psi(s)-\psi(1-s)= - \frac{\pi cos(\pi s)}{sin(\pi s)}=-\pi cot(\pi s)
\]
Finalement comme $cot(s)=cot(-s)$
\begin{equation}
\boxed{\psi(s)-\psi(1-s)=\pi cot(\pi s)}
\end{equation}

\subsection{Forme de Gauss \verif}
\label{eq_digamma_gauss_form}

Si on part de la forme de gauss de la fonction $\Gamma(s)$ (voir \ref{eq_gamma_gauss_form2}), on écrit:

\[
\Gamma(s)=\frac{1}{s} \prod_{k=1}^\infty \left( 1+\frac{s}{k}\right)^{-1}\left(1+\frac{1}{k}\right)^s 
\]

Donc:

\begin{eqnarray*}
\psi(s)&=&\left(ln(\Gamma(s))\right)'\\
&=&\left( ln\left(\frac{1}{s}\right) + \sum_{k=1}^\infty s\ ln\left(1+\frac{1}{k}\right)- ln\left( 1+\frac{s}{k}\right)\right)'\\
\end{eqnarray*}

Finalement

\[
\boxed{\psi(s)=-\frac{1}{s}+\sum_{k=1}^\infty \left( ln\left(1+\frac{1}{k}\right)-\frac{1}{k+s}\right) }
\]

\newpage
\section{Fonctions Theta de Jacobi: $\Theta(z)$}
\subsection{Définition}
\label{eq_theta_jacobi}

\begin{tabular}{cc}
\begin{minipage}{12cm}
Il existe plusieurs formes de fonctions théta de Jacobi généralement on note la forme générale :

\[
	\vartheta(u,v)=\sum_{n=-\infty}^{\infty}e^{i\pi(n^2v+2nu)}
\]

Ici on s'intéressera à la forme particulière:
\begin{equation}
\boxed{\theta(z)=\vartheta(0,v)=\sum_{n=-\infty}^{\infty}z^{n^2} \quad avec \quad z=e^{i\pi v}}
\end{equation}
Cette définition est \textbf{valide dans le cercle unité c'est à dire pour $|z|<1$}. \\

On s'intéressera plus particulièrement à $\Theta(z)=\theta(e^{-z\pi})$:
\begin{equation}
\boxed{\Theta(z)=\vartheta(0,iz)=\sum_{n=-\infty}^{\infty}e^{-n^2\pi z}}
\end{equation}
Cette définition est \textbf{valide pour $Re(z)>0$}. C'est en fait la même fonction mais avec un changement de variable exponentiel/logarithmique. Qui déroule le cercle unité sur l'axe des imaginaires\\

\notes{Ces fonctions ont servies à Riemann pour démontrer l'équation fonctionnelle de la fonction $\zeta(s)$. On notera également que les sommes partielles convergent très rapidement. car $e^{-n^2}$ devient vite très négligeable.}

\vspace{1cm}

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{thetaq_64}{$\theta(z)$}
\graphfunction{theta_64}{$\Theta(z)$}
\end{minipage}\\
\end{tabular}



\subsection{Relation fonctionnelle}
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}e^{-n^2\pi z}
\]
Ici on va utiliser la formule de sommation de Poisson (voir \ref{eq_poisson_formula}):
\[
\sum_{k=-\infty}^{\infty}g(k)=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} g(s)e^{-i 2\pi n s} ds\right)
\]
Si on prend $g(k)=e^{-k^2\pi z}$, on obtient:
\[
\sum_{k=-\infty}^{\infty}e^{-k^2\pi z}=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} e^{-s^2\pi z}e^{-i 2\pi n s} ds\right)
\]
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} e^{-s^2\pi z-i 2\pi n s} ds\right)
\]
Si on factorise le terme dans l'exponentielle par $z\pi$, on obtient:
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} e^{-z\pi(s^2+i \frac{2 n s}{z})} ds\right)
\]
On peut compléter le terme dans l'exponentielle pour faire apparaitre un carré, en effet:
\[
\left(s+i\frac{n}{z}\right)^2=s^2+i\frac{2ns}{z}+i^2\frac{n^2}{z^2}
\]
Donc
\[
\left(s+i\frac{n}{z}\right)^2+\frac{n^2}{z^2}=s^2+i\frac{2ns}{z}
\]
Si on remplace on obtient:
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} e^{-z\pi((s+i\frac{n}{z})^2+\frac{n^2}{z^2})} ds\right)
\]
Si on sépare en deux exponentielles:
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}\left(\int_{-\infty}^{\infty} e^{-z\pi(s+i\frac{n}{z})^2}e^{-\pi\frac{n^2}{z}} ds\right)
\]
Le second terme exponentiel ne dépend pas de $s$ et peut sortir de l'intégrale, on obtient:
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}e^{-\pi\frac{n^2}{z}}\left(\int_{-\infty}^{\infty} e^{-z\pi(s+i\frac{n}{z})^2} ds\right)
\]
ici on peut poser $u=s+i\frac{n}{z}$ et $du=ds$ et les bornes deviennent:
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}e^{-\pi\frac{n^2}{z}}\left(\int_{-\infty+i\frac{n}{z}}^{\infty+i\frac{n}{z}} e^{-z\pi u^2} du\right)
\]
Intéressons nous à l'intégrale dans le plan complexe on peut parcourir la boucle:
\[
\int_{-R}^{R} e^{-z\pi u^2} du=\int_{-R}^{-R+iA} e^{-z\pi u^2} du+\int_{-R+iA}^{R+iA} e^{-z\pi u^2} du+\int_{R+iA}^{R} e^{-z\pi u^2} du
\]
Mais
\[
\lim_{R\to\infty} \int_{-R}^{-R+iA} e^{-z\pi u^2} du=\lim_{R\to\infty} \int_{R+iA}^{R} e^{-z\pi u^2} du=0
\]
Donc dans notre cas l'intégrale peut se simplifier par une intégrale sur les reels, et on obtient:
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}e^{-\pi\frac{n^2}{z}}\left(\int_{-\infty}^{\infty} e^{-z\pi u^2} du\right)
\]
On reconnait alors, une intégrale de Gauss $\int_{-\infty}^{\infty} e^{-a u^2} du=\sqrt{\frac{\pi}{a}}$, dans notre cas $a=z\pi$ et cela donne :
\[
\Theta(z)=\sum_{n=-\infty}^{\infty}e^{-\pi\frac{n^2}{z}}\sqrt{\frac{\pi}{z\pi}}
\]
Finalement on peut sortir la racine de la somme 
\[
\Theta(z)=\sqrt{\frac{1}{z}}\sum_{n=-\infty}^{\infty}e^{-\pi\frac{n^2}{z}}
\]
Finalement on obtient l'équation fonctionnelle:
\label{eq_theta_jacobi_func}
\begin{equation}
\boxed{\Theta(z)=\sqrt{\frac{1}{z}}\Theta\left(\frac{1}{z}\right)}
\end{equation}
\newpage
\section{Fonction Zeta de Riemann : $\zeta(s)$}
\subsection{Définition}
\label{eq_zeta}

\begin{tabular}{cc}
\begin{minipage}{12cm}
On définit la fonction zêta de Riemann par la série:
\begin{equation}
\boxed{\zeta(s)=\sum_{n=1}^{\infty} \frac{1}{n^{s}}=1+\frac{1}{2^{s}}+\frac{1}{3^{s}}+\frac{1}{4^{s}}+\frac{1}{5^{s}}+...}
\end{equation}
Cette définition est \textbf{valide pour $Re(s)>1$}. Mais on verra que la fonction peut se prolonger à l'ensemble du plan complexe sauf 1.\\

Elle est un cas particulier de série de Dirichlet qui sont de la forme:
\[
	\boxed{L(\chi,s)=\sum_{n=1}^{\infty} \frac{\chi(n)}{n^{s}}}
\]
Ces fonctions définissent un cadre plus large que nous ne développerons pas pour l'instant.

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{zeta_form4_512}{$\zeta(s)$, prolongée sur $\mathbb{C}$}
\end{minipage}\\
\end{tabular}

\subsection{Lien avec les nombres premiers : Produit d'Euler}
\label{eq_zeta_euler_product}

On part de la définition :
\[
\zeta(s)=1+\frac{1}{2^{s}}+\frac{1}{3^{s}}+...
\]
Si on calcul la valeur de :
\begin{eqnarray*}
\frac{1}{2^s}\zeta(s)&=&\frac{1}{2^s}+\frac{1}{2^{s}2^s}+\frac{1}{3^{s}2^s}+...\\
&=&\frac{1}{2^s}+\frac{1}{4^{s}}+\frac{1}{6^{s}}+...\\
\end{eqnarray*}

Maintenant si on soustrait et que on factorise on suprime tout les termes paires de la somme et on obtient:
\[
\left(1-\frac{1}{2^s}\right)\zeta(s)=1+\frac{1}{3^s}+\frac{1}{5^s}+\frac{1}{7^s}+\frac{1}{9^s}+...
\]
Maintenant si on répète l'opération sur le terme précédent on obtient :
\[
\frac{1}{3^s}\left(1-\frac{1}{2^s}\right)\zeta(s)=\frac{1}{3^s}+\frac{1}{3^s3^s}+\frac{1}{5^s3^s}+\frac{1}{7^s3^s}...
\]
\[
\frac{1}{3^s}\left(1-\frac{1}{2^s}\right)\zeta(s)=\frac{1}{3^s}+\frac{1}{9^s}+\frac{1}{15^s}+\frac{1}{21^s}...
\]
Et si on soustrait les termes multiple de 3, on obtient:
\[
\left(1-\frac{1}{3^s}\right)\left(1-\frac{1}{2^s}\right)\zeta(s)=1+\frac{1}{5^s}+\frac{1}{7^s}+\frac{1}{11^s}...
\]
Finalement on voit que on peut répéter l'opération pour tout les nombres premiers c'est une sorte de \textbf{crible Eratosthène} et on obtient: 
\[
\prod_{k=1}^\infty \left(1-\frac{1}{p_n^s}\right)\zeta(s)=1 
\]
Ou $p_n$ désigne le n-ième nombre premier. 

\begin{tabular}{cc}
\begin{minipage}{12cm}
Finalement on obtient :
\begin{equation}
\boxed{\zeta(s)= \prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)^{-1}}
\end{equation}

Cette définition est \textbf{valide pour $Re(s)>1$}\\

\notes{Cette définition n'étend donc pas le domaine de convergence. Mais, sur son domaine de définition, elle converge plus rapidement vers la fonction $\zeta(s)$}
\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{zeta_form1_512}{$\zeta(s)$, comme produit Eulérien}
\end{minipage}\\
\end{tabular}



\subsection{Lien avec plusieurs fonctions arithmétiques}

\subsubsection{Lien avec la fonction de comptage des nombres premiers}
\label{eq_zeta_pi}
Si on part du produit d'Euler (voir \ref{eq_zeta_euler_product}) :
\[
\zeta(s)= \prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)^{-1}
\]
Et que on prend le logarithme, on obtient :
\[
ln(\zeta(s))= - \sum_{p\in\mathbb{P}} ln\left(1-\frac{1}{p^s}\right)
\]
Maintenant on peut astucieusement introduire $\pi(k)$ et remplacer $p$ par $k$ en écrivant:
\[
ln(\zeta(s))= - \sum_{k=2}^\infty \left(\pi(k)-\pi(k-1)\right) ln\left(1-\frac{1}{k^s}\right)
\]
Maintenant si on développe et que on sépare en deux sommes:
\[
ln(\zeta(s))= \sum_{k=2}^\infty \pi(k-1) ln\left(1-\frac{1}{k^s}\right)- \sum_{k=2}^\infty\pi(k)ln\left(1-\frac{1}{k^s}\right)
\]
Dans la première somme on peut commencer à 3 car $\pi(1)=0$ il n'y a pas de nombre premier inférieur à 1:
\[
ln(\zeta(s))= \sum_{k=3}^\infty \pi(k-1) ln\left(1-\frac{1}{k^s}\right)- \sum_{k=2}^\infty\pi(k)ln\left(1-\frac{1}{k^s}\right)
\]
On peut toujours dans la première somme changer $k$ en $k+1$ en ajustant l'indice
\[
ln(\zeta(s))= \sum_{k=2}^\infty \pi(k) ln\left(1-\frac{1}{(k+1)^s}\right)- \sum_{k=2}^\infty\pi(k)ln\left(1-\frac{1}{k^s}\right)
\]
Si on regroupe les deux sommes on obtient maintenant:
\[
ln(\zeta(s))= \sum_{k=2}^\infty \pi(k) \left(ln\left(1-\frac{1}{(k+1)^s}\right)-ln\left(1-\frac{1}{k^s}\right)\right)
\]
Maintenant on peut remarquer que le terme entre parenthèses est le résultat d'une intégrale pour la dérivée de $ln(1-x^{-s})$, en effet on a :
\[
\frac{d\ (ln(1-x^{-s}))}{dx}= \frac{s}{x(x^s-1)}
\]
Et
\[
\int_k^{k+1} \frac{s}{x(x^s-1)} dx = [ (ln(1-x^{-s}) ]_k^{k+1}=ln\left(1-\frac{1}{(k+1)^s}\right)-ln\left(1-\frac{1}{k^s}\right)
\]
Donc on peut remplacer et on obtient :
\[
ln(\zeta(s))= \sum_{k=2}^\infty \pi(k) \int_k^{k+1} \frac{s}{x(x^s-1)} dx
\]
On peut ensuite voir que $\pi(x)$ est constant pour $x\in[k,k+1]$ donc on peut rentrer $\pi(x)$ dans l'intégrale
\[
ln(\zeta(s))= \sum_{k=2}^\infty \int_k^{k+1} \frac{s\pi(x)}{x(x^s-1)} dx
\]
Enfin on peut fusionner la somme et l'intégrale
\label{eq-zeta-integrale-pi}
\begin{equation}
\boxed{\frac{ln(\zeta(s))}{s}= \int_2^{\infty} \frac{\pi(x)}{x(x^s-1)} dx}
\end{equation}

Cette définition est \textbf{valide pour $Re(s)>1$}

\subsubsection{Lien avec la fonction de comptage des nombres premiers de Riemann}
\label{eq_zeta_J}
Il existe un autre lien très important trouvé par Riemann en 1859 et publié dans (\cite{ref_riemann1}), dont la démonstration n'est pas inclue en détail dans le papier mais que nous allons redémontrer en détails. Cette relation est très similaire à la précédente. Avec la fonction $J(x)$ que nous allons introduire ici et qui est liée à la fonction $\pi(x)$.\\ 

Comme dans la démonstration précédente, on part du produit d'Euler (voir \ref{eq_zeta_euler_product}) :
\[
\zeta(s)= \prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)^{-1}
\]
On prend le logarithme :
\[
ln(\zeta(s))= - \sum_{p\in\mathbb{P}} ln\left(1-\frac{1}{p^s}\right)
\]
Maintenant si on utilise le fait que $ln(1-z)=-\sum_{n=1}^\infty \frac{z^n}{n}$ si $|z|<1$ or si $\Re(s)>1$ alors $|p^{-s}|<1$, l'utilisation de la série est donc légitime et on obtient une formule énoncé dans l'article de Riemann:
\begin{equation}
\boxed{ln(\zeta(s))=  \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{1}{n p^{ns}}}
\end{equation}
Si on inverse l'ordre de sommation:
\[
ln(\zeta(s))=  \sum_{n=1}^\infty \frac{1}{n} \sum_{p\in\mathbb{P}} p^{-ns}
\]
Maintenant l'astuce est d'introduire le fait que toujours si $\Re(s)>1$:
\[
I(z,s)=s\int_z^{\infty}t^{-s-1}dt=s\left[\frac{t^{-s}}{-s}\right]_z^\infty=\left[-t^{-s}\right]_z^\infty=z^{-s}
\]
Donc si on remplace par l'intégrale cela donne :
\[
ln(\zeta(s))=  \sum_{n=1}^\infty \frac{1}{n} \sum_{p\in\mathbb{P}} I(p^n,s)
\]
\[
ln(\zeta(s))=  \sum_{n=1}^\infty \frac{1}{n} \sum_{p\in\mathbb{P}} s\int_{p^n}^{\infty}t^{-s-1}dt
\]
Comme précédemment on peut astucieusement introduire $\pi(k)$ et remplacer $p$ par $k$, on peut également sortir $s$ de l'intégrale:
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=2}^\infty (\pi(k)-\pi(k-1))\int_{k^n}^{\infty}t^{-s-1}dt
\]
Maintenant en séparant la somme en deux que on décale l'indice  
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=2}^\infty \pi(k)\int_{k^n}^{\infty}t^{-s-1}dt-\sum_{k=1}^\infty \pi(k)\int_{(k+1)^n}^{\infty}t^{-s-1}dt
\]
Puis que on dit que $\pi(2)=0$ donc en fait on peut rassembler les sommes et factoriser par $\pi(x)$:
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=1}^\infty \pi(k)\left(\int_{k^n}^{\infty}t^{-s-1}dt-\int_{(k+1)^n}^{\infty}t^{-s-1}dt\right)
\]
On peut rassembler les deux intégrales, et on obtient:
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=1}^\infty \pi(k)\left(\int_{k^n}^{(k+1)^n}t^{-s-1}dt\right)
\]

Dans l'intégrale si on fait le changement de variable $u=t^\frac{1}{n}$ soit $t=u^n$, on a $du=\frac{1}{n}t^{\frac{1}{n}-1}dt$ et $dt=nu^{n-1}du$, on obtient:
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=1}^\infty \pi(k)\left(\int_{k}^{k+1}n u^{-(s+1)n}u^{n-1}du\right)
\]
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=1}^\infty \pi(k)\left(\int_{k}^{k+1}n u^{-ns-1}du\right)
\]
Comme $\pi(x)$ est constant pour $x\in[k,k+1]$ on peut le rentrer dans l'intégrale ainsi que la somme sur l'indice $n$.
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \sum_{k=1}^\infty \int_{k}^{k+1}\pi(u) n u^{-ns-1}du
\]
On peut alors simplifier l'intégrale et la somme:
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \int_{1}^{\infty}\pi(u) n u^{-ns-1}du
\]
Et maintenant si on refait le changement de variable $u=t^\frac{1}{n}$ soit $t=u^n$, on a $du=\frac{1}{n}t^{\frac{1}{n}-1}dt$ et $dt=nu^{n-1}du$, cette fois ci les bornes ne changent pas et on obtient:
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \int_{1}^{\infty}\pi(t^\frac{1}{n}) n t^{(-ns-1)\frac{1}{n}}\frac{1}{n}t^{\frac{1}{n}-1}dt
\]
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \int_{1}^{\infty}\pi(t^\frac{1}{n}) t^{-s-\frac{1}{n}+\frac{1}{n}-1}dt
\]
\[
\frac{ln(\zeta(s))}{s}=  \sum_{n=1}^\infty \frac{1}{n} \int_{1}^{\infty}\pi(t^\frac{1}{n}) t^{-s-1}dt
\]
Et finalement si on rentre la somme sur $n$ dans l'intégrale, on obtient:
\[
\frac{ln(\zeta(s))}{s}=  \int_{1}^{\infty} \sum_{n=1}^\infty \frac{1}{n}\pi(t^\frac{1}{n}) t^{-s-1}dt
\]
Donc si on définit $J(t)$:
\begin{equation}
\boxed{J(t)=\sum_{n=1}^\infty \frac{1}{n}\pi(t^\frac{1}{n})\quad \quad et \quad \quad \frac{ln(\zeta(s))}{s}=  \int_{1}^{\infty} J(t) t^{-s-1}dt}
\end{equation}
On obtient bien la formule avancée par Riemann dans son article! La formule est valide pour $\Re(s)>1$.\\

Aussi en particulier comme $J(t)=0$ pour $t\in[0,1]$, on peut écrire l'intégrale avec la borne inférieure égale à zéro, ce qui correspond à une transformée de Mellin (voir \ref{eq_mellin}), soit :

\begin{equation}
\boxed{ln(\zeta(s))=  s\int_{0}^{\infty} J(t) t^{-s-1}dt\quad\quad soit\quad\quad ln(\zeta(s)) = s \applyop{M}{J(t)}(-s)}
\end{equation}

\subsubsection{Lien avec la fonction de Möbius}
\label{eq_zeta_mu}
D'après le résultat sur le produit de Dirichlet obtenu en \ref{eq_produit_dirichlet} :

\[
\left(\sum_{n=1}^{\infty}\frac{f(n)}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{g(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{(f\star g)(n)}{n^s}
\]
Si on prend $f(n)=\mathbbm{1}(n)$ et $g(n)=\mu(n)$, alors la convolution $(f\star g)(n)=\delta_1(n)$ (voir \ref{eq_dirichlet_mu}) et on obtient :

\[
\left(\sum_{n=1}^{\infty}\frac{1}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{\mu(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{\delta_1(n)}{n^s}
\]
Mais le seul terme non-nul de $\delta_1(n)$, s'obtient pour $n=1$, et $\delta_1(1)=1$, donc la somme de droite est égale à 1:
\[
\zeta(s)\left(\sum_{n=1}^{\infty}\frac{\mu(n)}{n^s}\right)=1
\]
Finalement on obtient trivialement:
\[
\boxed{\frac{1}{\zeta(s)}=\sum_{n=1}^\infty \frac{\mu(n)}{n^s}}
\]
Cette définition est \textbf{valide pour $Re(s)>1$}

\subsubsection{Lien avec la fonction de Mertens}
\label{eq_zeta_mertens}
Si on introduit la fonction de sommatoire de Mertens:

\begin{equation}
\boxed{M(t)=\sum_{n\le t} \mu(n)}
\end{equation}

La formule de Perron sur les séries de Dirichlet (voir \ref{eq_perron_formula_dirichlet} ), appliquée à la formule précédente $\frac{1}{\zeta(s)}=\sum_{n=1}^\infty \frac{\mu(n)}{n^s}$ donne directement que :

\begin{equation}
\boxed{M(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \frac{1}{s\zeta(s)} t^{s} ds}
\end{equation}

Ce qui implique également que on a la transformée de Mellin suivante:

\begin{equation}
\boxed{\frac{1}{\zeta(s)}=s\int_0^\infty M(t) t^{-s-1} dt\quad\quad\quad soit \quad\quad\quad \frac{1}{\zeta(s)}=s \applyop{M}{M(t)}(-s)}
\end{equation}


\subsubsection{Lien avec la fonction nombre de diviseur}
\label{eq_zeta_d}
D'après le résultat sur le produit de Dirichlet obtenu en \ref{eq_produit_dirichlet} :
\[
\left(\sum_{n=1}^{\infty}\frac{f(n)}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{g(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{(f\star g)(n)}{n^s}
\]
Si on prend $f(n)=g(n)=\mathbbm{1}(n)$, alors la convolution $(f\star g)(n)=d(n)$ (voir \ref{eq_dirichlet_d}) et on obtient :

\[
\left(\sum_{n=1}^{\infty}\frac{1}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{1}{n^s}\right)=\sum_{n=1}^{\infty}\frac{d(n)}{n^s}
\]

Finalement on obtient trivialement:
\[
\boxed{\zeta^2(s)=\sum_{j=1}^\infty \frac{d(n)}{n^s}}
\]

\subsubsection{Lien avec la fonction sommes des diviseurs}
\label{eq_zeta_sigma}
D'après le résultat sur le produit de Dirichlet obtenu en \ref{eq_produit_dirichlet} :
\[
\left(\sum_{n=1}^{\infty}\frac{f(n)}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{g(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{(f\star g)(n)}{n^s}
\]
Si on prend $f(n)=\mathbbm{1}(n)$ et $g(n)=I(n)$, alors la convolution $(f\star g)(n)=\sigma(n)$ (voir \ref{eq_dirichlet_sigma}) et on obtient :

\[
\left(\sum_{n=1}^{\infty}\frac{1}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{n}{n^s}\right)=\sum_{n=1}^{\infty}\frac{\sigma(n)}{n^s}
\]

Finalement on obtient trivialement:
\begin{equation}
\boxed{\zeta(s)\zeta(s-1)=\sum_{j=1}^\infty \frac{\sigma(n)}{n^s}}
\end{equation}

\subsubsection{Lien avec l'indicatrice d'Euler}
\label{eq_zeta_ind_euler}

D'après le résultat sur le produit de Dirichlet obtenu en \ref{eq_produit_dirichlet} :
\[
\left(\sum_{n=1}^{\infty}\frac{f(n)}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{g(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{(f\star g)(n)}{n^s}
\]
Si on prend $f(n)=I(n)$ et $g(n)=\mu(n)$, alors la convolution $(f\star g)(n)=\varphi(n)$ (voir \ref{eq_dirichlet_phi}) et on obtient :

\[
\left(\sum_{n=1}^{\infty}\frac{n}{n^s}\right)\left(\sum_{n=1}^{\infty}\frac{\mu(n)}{n^s}\right)=\sum_{n=1}^{\infty}\frac{\varphi(n)}{n^s}
\]

Si on utilise le résultat obtenu pour la fonction $\mu$ (voir \ref{eq_zeta_mu}), alors:

\begin{equation}
\boxed{\frac{\zeta(s-1)}{\zeta(s)}=\sum_{n=1}^{\infty}\frac{\varphi(n)}{n^s}}
\end{equation}


\subsubsection{Lien avec la fonction de von Mangoldt}
\label{eq_zeta_mangoldt}

Si on part de la formule obtenue (voir \ref{eq_zeta_J}) pour le logarithme de la fonction zêta:

\[
ln(\zeta(s))=  \sum_{p\in\mathbb{P}}^\infty \sum_{n=1}^\infty \frac{1}{n p^{ns}}
\]

Maintenant si on prend la dérivée des deux membres, on obtient bien la dérivée logarithmique de zêta à gauche, cela donne:

\[
\frac{\zeta'(s)}{\zeta(s)}=  \sum_{p\in\mathbb{P}}^\infty \sum_{n=1}^\infty \left(\frac{1}{n p^{ns}}\right)'
\]
Et la dérivée $(n^{-1} p^{-ns})'=-ln(p)p^{-ns}$
\[
\frac{\zeta'(s)}{\zeta(s)}=  \sum_{p\in\mathbb{P}}^\infty \sum_{n=1}^\infty -\frac{ln(p)}{p^{ns}}
\]
Si on introduit maintenant la fonction de von Mangoldt définie par :
\begin{equation}
\boxed{
\Lambda(n)=
\left\{
\begin{array}{ll}
ln(p) & Si\quad n=p^k\quad et\quad k\ge 1\\
0 & Sinon\\
\end{array}
\right.}
\end{equation}
Donc
\[
\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}=\sum_{p\in\mathbb{P}}^\infty\sum_{k=1}^{\infty} \frac{\Lambda(p^k)}{(p^k)^s}=\sum_{p\in\mathbb{P}}^\infty\sum_{k=1}^{\infty} \frac{ln(p)}{p^{ks}}
\]

Finalement

\begin{equation}
\boxed{\frac{\zeta'(s)}{\zeta(s)}=-\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}}
\end{equation}

\subsubsection{Lien avec la fonction sommatoire de von Mangoldt}
\label{eq_zeta_mangoldt_sum}

Si on introduit la fonction de sommatoire de von Mangoldt:

\begin{equation}
\boxed{\Psi(t)=\sum_{n\le t} \Lambda(n)}
\end{equation}

De manière très analogue à la formule intégrale obtenue pour la fonction de comptage des nombres premiers de Riemann (voir \ref{eq_zeta_J} ), on peut obtenir une formule intégrale pour la fonction sommatoire $\Psi(t)$ de von Mangoldt. Partons de la formule obtenue précédement (voir \ref{eq_zeta_mangoldt}) :

\[
\frac{\zeta'(s)}{\zeta(s)}=-\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}
\]

Si on applique , la formule sommatoire d'Abel (voir \ref{eq_abel2}):

\[
\sum_{n=\floor{x}+1}^{\floor{y}} u_n \varphi(n)= U(y) \varphi(y)-U(x) \varphi(x)  - \int_x^y U(t)\varphi'(t) dt \quad\quad avec \quad\quad U(w)=\sum_{n=a}^{\floor{w}}u_n
\]

Avec $u_n=\Lambda(n)$ et $\varphi(n)=\frac{1}{n^s}$ et $x=1$ et $y=\infty$ et $a=1$ et donc $U(w)=\Psi(w)=\sum_{n=1}^{\floor{w}}\Lambda(n)$, on obtient:

\[
\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}= \Psi(\infty) \varphi(\infty)-\Psi(1) \varphi(1)  - \int_1^\infty \Psi(t)\varphi'(t) dt
\]

Mais $\Psi(1) \varphi(1)=0$ car $\Psi(1)=0$, et si $\Re(s)>1$ alors $\Psi(\infty) \varphi(\infty)=0$ et $\varphi'(n)=-s n^{-s-1}$ donc:

\[
\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}= s\int_1^\infty \Psi(t) n^{-s-1} dt
\]

Et comme $\frac{\zeta'(s)}{\zeta(s)}=-\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}$ (voir \ref{eq_zeta_mangoldt}), on obtient:

\begin{equation}
\boxed{-\frac{\zeta'(s)}{\zeta(s)}=s\int_1^\infty \Psi(t) n^{-s-1} dt}
\end{equation}

Aussi comme $\Psi(t)=0 \quad \forall t\in[0,1]$, on peut écrire la relation sous la forme d'une transformée de Mellin (voir \ref{eq_mellin}):

\begin{equation}
\boxed{-\frac{\zeta'(s)}{\zeta(s)}= s\applyop{M}{\Psi(t)}{(-s)} }
\end{equation}

La formule de Perron appliquée à une série de Dirichlet (voir \ref{eq_perron_formula_dirichlet}) donne directement le résultat :

\begin{equation}
\boxed{\Psi(t)= \applyopp{M}{-1}{-\frac{1}{s}\frac{\zeta'(s)}{\zeta(s)}}{(-s,t)} }
\end{equation}

\subsection{Lien avec $ \Gamma(s) $}
\label{eq_zeta_gamma}
Si on part de la définition de $\Gamma(s)$ (voir \ref{eq_gamma}):
\[
\Gamma(s)=\int_0^{\infty} t^{s-1}e^{-t} dt
\]
Si on fait le changement de variable $t = n u$ on obtient $dt=n du$ et:
\[
\Gamma(s)=\int_0^{\infty} (n u)^{s-1}e^{-n u} n du
\]
\[
\Gamma(s)=n^s\int_0^{\infty} u^{s-1}e^{-nu} du
\]
Donc 
\[
\Gamma(s)\frac{1}{n^s}=\int_0^{\infty} u^{s-1}e^{-nu} du
\]
On peut effectuer cette opération pour tout $n$ et sommer les égalités:
\[
\sum_{n=1}^{\infty} \Gamma(s) \frac{1}{n^s} =\sum_{n=1}^{\infty} \int_0^{\infty} u^{s-1}e^{-nu} du
\]
On peut factoriser à gauche par $\Gamma(s)$ et à droite on peut rentrer la somme dans l'intégrale, on obtient :
\[
 \Gamma(s) \sum_{n=1}^{\infty} \frac{1}{n^s} =\int_0^{\infty} \sum_{n=1}^{\infty}  u^{s-1}e^{-nu} du
\]
On reconnait la fonction $\zeta(s)$ à gauche et à droite on peut sortir $u^s-1$ de la somme:
\[
 \Gamma(s) \zeta(s) =\int_0^{\infty} u^{s-1} \sum_{n=1}^{\infty} e^{-nu} du
\]
Comme $e^{-nu}<1$ on peut remplacer par la forme clause de la somme géométrique, c'est à dire $\sum_{n=0}^{\infty} e^{-nu}=\frac{1}{1-e^{-u}}$ mais comme on commence à $n=1$ il faut retrancher 1, on obtient alors :
\[
 \Gamma(s) \zeta(s) =\int_0^{\infty} u^{s-1} \left(\frac{1}{1-e^{-u}}-1\right) du
\]
On peut alors simplifier le terme entre parenthèses en :
\[
 \Gamma(s) \zeta(s) =\int_0^{\infty} u^{s-1} \left(\frac{1}{1-e^{-u}}-\frac{1-e^{-u}}{1-e^{-u}}\right) du
\]

\[
 \Gamma(s) \zeta(s) =\int_0^{\infty} u^{s-1} \left(\frac{e^{-u}}{1-e^{-u}}\right) du
\]
Finalement on obtient une jolie relation entre les deux fonctions:

\begin{equation}
\boxed{\Gamma(s) \zeta(s) =\int_0^{\infty}  \left(\frac{u^{s-1}}{e^{u}-1}\right) du}
\end{equation}

Cette relation est \textbf{valide pour $Re(s)>1$}\\

On remarquera ici qu'il s'agit d'une transformée de Mellin (voir \ref{eq_mellin}):

\begin{equation}
\boxed{\Gamma(s) \zeta(s) = \applyop{M}{\frac{1}{e^{t}-1}}{(s)}}
\end{equation}

\subsection{Fonction $\eta(s)$ : Prolongement analytique sur $\Re(s)>0$ Sauf 1}
\label{eq_zeta_eta}

Si on défini la fonction êta par :
\begin{equation}
\boxed{\eta(s)=\sum_1^{\infty} \frac{(-1)^{n-1}}{ n^{s}}=1-\frac{1}{2^{s}}+\frac{1}{3^{s}}-...}
\end{equation}
Si on soustrait cette suite à $\zeta(s)$, on obtient :
\[
\zeta(s)-\eta(s)=2 \left( \frac{1}{2^{s}} + \frac{1}{4^{s}}+ \frac{1}{6^{s}}+ \frac{1}{8^{s}}+... \right)
\]
Si on factorise par $\frac{1}{2^{s}}$ :
\[
\zeta(s)-\eta(s)=\frac{2}{2^{s}} \left( 1 + \frac{1}{2^{s}}+ \frac{1}{3^{s}}+ \frac{1}{4^{s}}+... \right)
\]
On reconnait alors $\zeta(s)$ :
\[
\zeta(s)-\eta(s)=\frac{2}{2^{s}} \zeta(s)
\]

\begin{tabular}{cc}
\begin{minipage}{12cm}
Soit 
\[
\eta(s)=\zeta(s)\left(1-2^{1-s}\right)
\]
Donc finalement

\begin{equation}
\boxed{\zeta(s)=\frac{\eta(s)}{\left(1-2^{1-s}\right)}}
\end{equation}
On peut voir que on a trouvé une fonction définie sur un domaine plus grand et qui est égale à $\zeta(s)$ sur ce domaine. On a donc trouvé un \textbf{prolongement analytique} de $\zeta(s)$. En fait on a extrait le pôle en 1 de la série. Cette nouvelle équation constituera notre nouvelle définition de $\zeta(s)$.\textbf{ Valide pour $Re(s)>0$}.\\
\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{zeta_form2_512}{$\zeta(s)$, prolongée sur le demi plan de Poincaré}
\end{minipage}\\
\end{tabular}

\subsection{Équation fonctionnelle symétrique: Prolongement analytique sur $\Re(s)<0$ }
\label{eq_zeta_func_sym}
Partons de la fonction $\Gamma(s)$ (voir \ref{eq_gamma_func}),on a:

\[
	\Gamma(s)=\int_0^{\infty} t^{s-1}e^{-t} dt 
\]
Remplaçons $s$ par $\frac{s}{2}$, on obtient:
\[
	\Gamma\left(\frac{s}{2}\right)=\int_0^{\infty} t^{\frac{s}{2}-1}e^{-t} dt 
\]
Si on effectue le changement de variable $t= n^2\pi x$ avec $dt= n^2\pi dx$, on obtient:
\[
	\Gamma\left(\frac{s}{2}\right)=\int_0^{\infty} (n^2\pi x)^{\frac{s}{2}-1}e^{-n^2\pi x} n^2\pi dx 
\]
\[
	\Gamma\left(\frac{s}{2}\right)=(n^2\pi)^\frac{s}{2}\int_0^{\infty} x^{\frac{s}{2}-1}e^{-n^2\pi x} dx 
\]
\[
	\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)n^{-s}=\int_0^{\infty} x^{\frac{s}{2}-1}e^{-n^2\pi x} dx 
\]
Et cette relation est valide $\forall n$ donc on peut sommer toutes ces expressions, et obtenir:
\[
	\sum_{n=1}^{\infty} \pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)n^{-s}=\sum_{n=1}^{\infty}\int_0^{\infty} x^{\frac{s}{2}-1}e^{-n^2\pi x} dx 
\]
A gauche on reconnais $\zeta(s)$ et à droite on peut rentrer la somme dans l'intégrale.
\[
	\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\int_0^{\infty} x^{\frac{s}{2}-1}\sum_{n=1}^{\infty}e^{-n^2\pi x} dx 
\]
Maintenant regardons le terme de droite ressemble beaucoup à la fonction $\Theta(x)=\sum_{n=0}^{\infty}e^{-n^2\pi x}$ de Jacobi (voir \ref{eq_theta_jacobi}), en effet:

\[
	\Theta(x)=\sum_{n=0}^{\infty}e^{-n^2\pi x} dx=1+ 2\sum_{n=1}^{\infty}e^{-n^2\pi x} dx 
\]
Soit
\[
\sum_{n=1}^{\infty}e^{-n^2\pi x}=\frac{\Theta(x)-1}{2}
\]
Posons $\Psi(x)=\frac{\Theta(x)-1}{2}$, notre expression peut s'écrire:
\[
	\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\int_0^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx 
\]
Nous allons cher cher à utiliser l'équation fonctionnelle de la fonction $\theta$ (voir \ref{eq_theta_jacobi_func}):

\[
\Theta(x)=\sqrt{\frac{1}{x}}\Theta\left(\frac{1}{x}\right)
\]
Pour $\Psi(x)$ cela devient :
\[
1+2\Psi(x)=\sqrt{\frac{1}{x}}\left(1+2\Psi\left(\frac{1}{x}\right)\right)
\]
\[
\Psi(x)=\frac{1}{2}\left(x^{-\frac{1}{2}}+2x^{-\frac{1}{2}}\Psi\left(\frac{1}{x}\right)-1\right)
\]
Si on remplace:

\[
\int_0^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx=\frac{1}{2}\int_0^{\infty} x^{\frac{s}{2}-1} \left(x^{-\frac{1}{2}}+2x^{-\frac{1}{2}}\Psi\left(\frac{1}{x}\right)-1\right)\ dx
\]

\[
=\frac{1}{2}\int_0^{\infty} 2x^{\frac{s}{2}-\frac{3}{2}}\Psi\left(\frac{1}{x}\right)+x^{\frac{s}{2}-\frac{3}{2}}-x^{\frac{s}{2}-1}\ dx
\]

\[
=\int_0^{\infty} x^{\frac{s}{2}-\frac{3}{2}}\Psi\left(\frac{1}{x}\right)\ dx+\frac{1}{2}\int_0^{\infty} x^{\frac{s}{2}-\frac{3}{2}}-x^{\frac{s}{2}-1}\ dx
\]
Calculons d'abord:

\[
\int_0^{1} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_0^{1} x^{\frac{s}{2}-\frac{3}{2}}\Psi\left(\frac{1}{x}\right)\ dx+\frac{1}{2}\int_0^{1} x^{\frac{s}{2}-\frac{3}{2}}-x^{\frac{s}{2}-1}\ dx
\]
La seconde intégrale se calcule facilement, et on obtient :
\[
\int_0^{1} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_0^{1} x^{\frac{s}{2}-\frac{3}{2}}\Psi\left(\frac{1}{x}\right)\ dx+\frac{1}{(s-1)s}
\]
Maintenant si on pose $u=\frac{1}{x}$ et $dx=-\frac{1}{u^2}du$, on obtient:
\[
\int_0^{1} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_1^{\infty} u^{-\frac{s}{2}+\frac{3}{2}}\Psi\left(u\right) \frac{1}{u^2} \ du+\frac{1}{(s-1)s}
\]
\[
\int_0^{1} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_1^{\infty} u^{-\frac{s}{2}-\frac{1}{2}}\Psi\left(u\right) \ du+\frac{1}{(s-1)s}
\]
Finalement si on rajoute la seconde partie:
\[
\int_0^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_0^{1} x^{\frac{s}{2}-1}\Psi(x)\ dx+\int_1^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx
\]
Si on remplace par ce que on vient de trouver:
\[
\int_0^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_1^{\infty} x^{-\frac{s}{2}-\frac{1}{2}}\Psi\left(x\right) \ dx+\frac{1}{(s-1)s}+\int_1^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx
\]
Si on rassemble les deux intégrales, on obtient:
\[
\int_0^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_1^{\infty} \left(x^{\frac{s}{2}-1}+x^{-\frac{s}{2}-\frac{1}{2}}\right)\Psi\left(x\right) \ dx+\frac{1}{(s-1)s}
\]
Soit si on divise par $x$ dans l'intégrale:
\[
\int_0^{\infty} x^{\frac{s}{2}-1}\Psi(x)\ dx=\int_1^{\infty} x^{-1}\left(x^{\frac{s}{2}}+x^{\frac{1-s}{2}}\right)\Psi\left(x\right) \ dx+\frac{1}{(s-1)s}
\]
Et la on peut voir que l'intégrale reste inchangée si on change $s$ en $1-s$, donc on peut écrire très joliment:

\begin{equation}
\boxed{\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\pi^{-\frac{1-s}{2}}\Gamma\left(\frac{1-s}{2}\right)\zeta(1-s)}
\end{equation}

\subsection{Équation fonctionnelle asymétrique}
\label{eq_zeta_func_asym}

\begin{tabular}{cc}
\begin{minipage}{12cm}

On part du résultat précédent:
\[
\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\pi^{-\frac{1-s}{2}}\Gamma\left(\frac{1-s}{2}\right)\zeta(1-s)
\]
On en déduit facilement que 
\[
\zeta(s)=\pi^{s-\frac{1}{2}}\frac{\Gamma\left(\frac{1-s}{2}\right)}{\Gamma\left(\frac{s}{2}\right)}\zeta(1-s)
\]

Maintenant si on rappelle la formule de duplication obtenue sur la fonction $\Gamma$ (voir \ref{eq_gamma_beta_legendre}) :

\[
\Gamma(2s)=2^{2s-1}\pi^{-\frac{1}{2}} \Gamma(s)\Gamma\left(s+\frac{1}{2}\right)
\]

Si on remplace $s$ par $\frac{1-s}{2}$, cela donne:

\[
\Gamma(1-s)=2^{-s}\pi^{-\frac{1}{2}} \Gamma\left(\frac{1-s}{2}\right)\Gamma\left(1-\frac{s}{2}\right)
\]

Mais en remplacent $s$ par $\frac{s}{2}$ dans la formule des compléments (voir \ref{eq_gamma_comp1}), on trouve que :

\[
\Gamma\left(1-\frac{s}{2}\right)=\frac{\pi}{sin\left(\frac{\pi s}{2}\right)\Gamma\left(\frac{s}{2}\right)}
\]
Donc 
\[
\Gamma(1-s)=2^{-s}\pi^{-\frac{1}{2}} \Gamma\left(\frac{1-s}{2}\right)\frac{\pi}{sin\left(\frac{\pi s}{2}\right)\Gamma\left(\frac{s}{2}\right)}
\]

\[
\frac{\Gamma\left(\frac{1-s}{2}\right)}{\Gamma\left(\frac{s}{2}\right)}=\pi^{-\frac{1}{2}} 2^s sin\left(\frac{s\pi}{2}\right)\Gamma(1-s)
\]

Finalement, on obtient en remplaçant dans l'expression de $\zeta(s)$:

\begin{equation}
\boxed{\zeta(s)=\pi^{s-1} 2^s sin\left(\frac{s\pi}{2}\right)\Gamma(1-s)\zeta(1-s)}
\end{equation}

Cette relation nous permet alors d'étendre la fonction $\zeta(s)$ à l'ensemble du plan complexe.

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{zeta_form3_512}{$\zeta(s)$, prolongée sur l'ensemble du plan complexe sauf 1 avec  l'équation fonctionnelle}

\begin{figure}[H]
\centering
{\animategraphics[loop,controls,width=6cm]{10}{./img/cpp/spheres/zeta_form3_sphere}{0}{9}}
\title{Graphique de la fonction $\zeta(s)$ prolongée sur la sphère de Riemann.}
\end{figure}


\end{minipage}
\end{tabular}

\newpage
\subsection{Fonction $\xi(s)$ de Riemann}
\label{eq_zeta_xi}

\begin{tabular}{cc}
\begin{minipage}{12cm}
A partir de la relation fonctionnelle symétrique (voir \ref{eq_zeta_func_sym}) on peut poser:

\begin{equation}
	\boxed{\bar{\xi}(s)=\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)}
\end{equation}
La fonction vérifie donc la relation fonctionnelle $\bar{\xi}(s)=\bar{\xi}(1-s)$\\

Riemann définit ensuite la fonction:

\[
\xi(s)=\frac{1}{2}s(s-1)\bar{\xi(s)}
\]

\begin{equation}
\boxed{\xi(s)=\frac{1}{2}s(s-1)\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)}
\end{equation}

\noindent
Le nouveau terme en $s(s-1)$ ne brise pas la propriété de symétrie $\xi(s)=\xi(1-s)$. Mais il permet de compenser le pôle d'ordre 1 de la fonction $\Gamma(s)$ en zéro (voir \ref{eq_gamma_zero}) et le pôle d'ordre 1 de la fonction $\zeta(s)$ en 1 (voir \ref{eq_zeta1}).\\

La fonction devient ainsi une fonction entière. Le facteur $\frac{1}{2}$ est introduit pour pouvoir simplifier l'expression en utilisant la relation fonctionnelle de la fonction $\Gamma$:
\begin{equation}
\boxed{\xi(s)=(s-1)\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}+1\right)\zeta(s)}
\end{equation}

Cette expression résout l'indétermination en zéro, on obtient en effet :

\[
\boxed{\xi(0)=-\zeta(0)}
\]

\end{minipage}
&
\begin{minipage}{7cm}
\graphfunction{xi_hat_512}{$\bar{\xi}(s)$}
\graphfunction{xi_form0_512}{$\xi(s)$}
\end{minipage}
\end{tabular}

\subsubsection{Représentation de Hadamard-Weierstrass de $\xi(s)$ et dérivée logarithmique de $\zeta(s)$ \proof}
\label{eq_zeta_xi_hw}
La fonction $\xi(s)$ obtenue est une \textbf{fonction entière}. Elle peut donc être factorisée par la formule de Hadamard-Weierstrass ( voir \ref{eq_hadamard_weierstrass} ):
\[
\xi(s)= s^m e^{P_{\rho}(s)} \prod_{n=1}^N E_{\rho}\left(\frac{s}{Z_n}\right) \quad\quad\quad avec \quad\quad\quad E_\rho(s)=\left\{
	\begin{array}{ll}
	(1-s) &si\ \rho=0\\
	(1-s)e^{(\frac{s}{1}+\frac{s}{2}+...+\frac{s}{\rho})} &sinon\\ 
	\end{array}
	\right.
\]
Supposons pour le moment que $\xi(s)$ est d'ordre 1 (on le démontrera ensuite {\proof}), c'est à dire que $\rho=1$. La fonction ne s'annule pas en zéro donc $m=0$. Et si on note $Z_n=\rho_i$ les zéros de la fonction $\xi(s)$ alors on obtient :
\[
\xi(s)= e^{A+B s} \prod_{i=1}^N \left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}
\]
Maintenant il nous reste à identifier les constantes $A$ et $B$, pour identifier $A$ nous avons obtenu juste avant que:
\[
\xi(s)=(s-1)\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}+1\right)\zeta(s)
\]
Et si on prend la valeur en 0 :
\[
\xi(0)=-\zeta(0)
\]
Or $\zeta(0)=-\frac{1}{2}$ (voir \ref{eq_zeta0}), donc:
\[
e^A=\frac{1}{2} \rightarrow \boxed{A=-ln(2)}
\]

La calcul de B n'est pas si trivial, calculons d'abord la dérivée logarithmique de $\xi(s)$:

\begin{eqnarray*}
\frac{\xi'(s)}{\xi(s)}&=&ln\left(e^{A+B s} \prod_{i=1}^\infty \left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}\right)'\\
&=&\left(A+Bs + \sum_{i=1}^\infty \left(ln\left(1-\frac{s}{\rho_i}\right)+\frac{s}{\rho_i}\right)\right)'\\
&=&B+ \sum_{i=1}^\infty \left(\frac{-\frac{1}{\rho_i}}{1-\frac{s}{\rho_i}}+\frac{1}{\rho_i}\right)\\
&=&B+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}+\frac{1}{\rho_i}\right)\\
\end{eqnarray*}

Calculons maintenant la dérivée logarithmique avec l'autre expression que nous avons de $\xi(s)$, c'est parti:


\begin{eqnarray*}
\frac{\xi'(s)}{\xi(s)}&=&ln\left(\frac{1}{2}s(s-1)\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)\right)'\\
&=&ln\left((s-1)\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}+1\right)\zeta(s)\right)'\\
&=&\left(ln(s-1)-\frac{s}{2}ln(\pi)+ln\left(\Gamma\left(\frac{s}{2}+1\right)\right)+ln(\zeta(s))\right)'\\
&=&\frac{1}{s-1}-\frac{ln(\pi)}{2}+\frac{\left(\Gamma\left(\frac{s}{2}+1\right)\right)'}{\Gamma\left(\frac{s}{2}+1\right)}+\frac{\zeta'(s)}{\zeta(s)}\\
&=&\frac{1}{s-1}-\frac{ln(\pi)}{2}+\frac{1}{2}\frac{\Gamma'\left(\frac{s}{2}+1\right)}{\Gamma\left(\frac{s}{2}+1\right)}+\frac{\zeta'(s)}{\zeta(s)}\\
\end{eqnarray*}

Et si on égalise les deux formules précédentes on obtient une formule importante sur la dérivée logarithmique de $\zeta(s)$:

\begin{equation}
\boxed{\frac{\zeta'(s)}{\zeta(s)} = B-\frac{1}{s-1}+\frac{ln(\pi)}{2}-\frac{1}{2}\frac{\Gamma'\left(\frac{s}{2}+1\right)}{\Gamma\left(\frac{s}{2}+1\right)}+ \sum_{i=1}^N \left(\frac{1}{s-\rho_i}+\frac{1}{\rho_i}\right)}
\end{equation}

Maintenant si on prend la limite lorsque $s\to 0$, on obtient:

\[
\frac{\zeta'(0)}{\zeta(0)}=B+1+\frac{ln(\pi)}{2}-\frac{1}{2}\frac{\Gamma'\left(1\right)}{\Gamma\left(1\right)}
\]

Mais il se trouve que on sait calculer toutes ses quantités, en effet $\zeta(0)=-\frac{1}{2}$ et $\zeta'(0)=-\frac{1}{2}ln(2\pi)$ (voir \ref{eq_zeta0} et \ref{eq_zeta_prime0}), de plus $\Gamma(1)=1$ et $\Gamma'(1)=-\gamma$ (voir \ref{eq_gamma_prime_1}), donc :

\[
ln(2\pi)=B+1+\frac{ln(\pi)}{2}+\frac{1}{2}\gamma
\]
D'ou on tire :
\[
B=ln(2\pi)-\frac{ln(\pi)}{2}-1-\frac{1}{2}\gamma=ln(2)+ln(\pi)-\frac{ln(\pi)}{2}-1-\frac{1}{2}\gamma
\]
\[
B=\frac{ln(4)+ln(\pi)}{2}-1-\frac{1}{2}\gamma
\]
Après simplification on trouve bien :

\[
\boxed{B=\frac{1}{2}ln(4\pi)-\frac{1}{2}\gamma-1}
\]

Finalement si on récapitule :
\begin{equation}
\boxed{\xi(s)=e^{A+Bs}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}} \quad\quad avec \quad\quad A=-ln(2) \quad et\quad B=\frac{1}{2}ln(4\pi)-\frac{1}{2}\gamma-1}
\end{equation}

On retrouvera ces résultats dans \cite{ref_menici} (page 7-9) et \cite{ref_titshmarch}(page 31). Cependant pour que les produits partiels converges il faut que les pôles soit sommés par paires symétriques $\rho_i$ et $1-\rho_i$, Soit :

\[
\xi(s)=e^{A+Bs}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}\left(1-\frac{s}{1-\rho_i}\right)e^{\frac{s}{1-\rho_i}}
\]
\[
\xi(s)=e^{A+Bs}\prod_{i=0}^\infty\left(1-\left(\frac{s}{\rho_i}+\frac{s}{1-\rho_i}\right)+\frac{s^2}{\rho_i(1-\rho_i)}\right)e^{\frac{s}{\rho_i}+\frac{s}{1-\rho_i}}
\]
Et comme $e^A=\frac{1}{2}$, on peut l'expliciter et :

\begin{equation}
\boxed{\xi(s)=\frac{1}{2}e^{Bs}\prod_{i=0}^\infty\left(1+\frac{s(s-1)}{\rho_i(1-\rho_i)}\right)e^{\frac{s}{\rho_i(1-\rho_i)}}\quad\quad avec \quad\quad B=\frac{1}{2}ln(4\pi)-\frac{1}{2}\gamma-1}
\end{equation}

On peut alors vérifier que on obtient bien la même fonction:

\graphfunction{xi_form1_512}{$\xi(s)$, par son produit de Hadamard-Weierstrass}

\subsubsection{Simplification de la représentation de Hadamard-Weierstrass de $\xi(s)$ et dérivée logarithmique de $\zeta(s)$}
\label{eq_zeta_xi_prod}
Maintenant il existe une autre expression pour la constante $B$ obtenue précedement (voir \ref{eq_zeta_xi_hw}), en effet si on utilise la relation fonctionnelle de la fonction $\xi(s)$ :

\[
\frac{\xi'(s)}{\xi(s)}=-\frac{\xi'(1-s)}{\xi(1-s)}
\]
Soit 
\begin{eqnarray*}
B+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}+\frac{1}{\rho_i}\right)&=&-B- \sum_{i=1}^\infty \left(\frac{1}{1-s-\rho_i}+\frac{1}{\rho_i}\right)\\
\sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}+\frac{2}{\rho_i}+\frac{1}{1-s-\rho_i}\right)&=&-2B\\
-\frac{1}{2}\sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}+\frac{1}{1-s-\rho_i}\right)-\sum_{i=1}^{\infty}\frac{1}{\rho_i}&=&B\\
-\frac{1}{2}\sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}-\frac{1}{s-(1-\rho_i)}\right)-\sum_{i=1}^{\infty}\frac{1}{\rho_i}&=&B\\
\end{eqnarray*}

Mais si $\rho_i$ est un zéro, alors $1-\rho_i$ l'est aussi donc finalement les termes de gauches s'annulent et on obtient simplement:

\begin{equation}
\boxed{B=-\sum_{i=1}^{\infty}\frac{1}{\rho_i}=\frac{1}{2}ln(4\pi)-\frac{1}{2}\gamma-1}
\end{equation}

On montre de ce fait que la somme des inverses des zéros converge et également que l'expression de la dérivée logarithmique de $\xi(s)$ est simplement:

\[
\frac{\xi'(s)}{\xi(s)}=B+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}+\frac{1}{\rho_i}\right)
\]
Soit
\begin{equation}
\boxed{\frac{\xi'(s)}{\xi(s)}=\sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}\right)}
\end{equation}

Par conséquent la dérivée logarithmique de la fonction zéta trouvée précedement (voir \ref{eq_zeta_xi_hw}), s'exprime plus simplement par:

\[
\frac{\zeta'(s)}{\zeta(s)} = B-\frac{1}{s-1}+\frac{ln(\pi)}{2}-\frac{1}{2}\frac{\Gamma'\left(\frac{s}{2}+1\right)}{\Gamma\left(\frac{s}{2}+1\right)}+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}+\frac{1}{\rho_i}\right)
\]
Soit
\[
\boxed{\frac{\zeta'(s)}{\zeta(s)} = \frac{1}{1-s}+\frac{ln(\pi)}{2}-\frac{1}{2}\frac{\Gamma'\left(\frac{s}{2}+1\right)}{\Gamma\left(\frac{s}{2}+1\right)}+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}\right)}
\]


Et par conséquent la fonction $\xi$ possède également un développement plus simple, on rappelle que $A=-ln(2)$ c'est à dire que $e^{A}=\frac{1}{2}$,si on développe, on obtient:

\begin{eqnarray*}
\xi(s)&=&e^{A+Bs}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}\\
&=&\frac{1}{2}e^{-s\sum_{i=1}^{\infty}\frac{1}{\rho_i}}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}\\
&=&\frac{1}{2}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}e^{-\frac{s}{\rho_i}}\\
\end{eqnarray*}

Finalement les exponentielles se simplifies et on obtient simplement:

\begin{equation}
\boxed{\xi(s)=\frac{1}{2}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)}
\end{equation}

\subsubsection{Représentation en produit de Hadamard-Weierstrass de $\zeta(s)$}
\label{eq_zeta_hw}
La représentation de $\zeta(s)$ découle de la représentation de $\xi(s)$, si on reprend l'expression de $\xi(s)$ (voir \ref{eq_zeta_xi}):
\[
\xi(s)=(s-1)\pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}+1\right)\zeta(s)
\]
D'où on tire simplement:
\[
\zeta(s)=\frac{\pi^\frac{s}{2}}{(s-1)\Gamma\left(\frac{s}{2}+1\right)}\xi(s)
\]
Si on remplace par l'expression de $\xi(s)$ (voir \ref{eq_zeta_xi_prod})

\begin{equation}
\boxed{\zeta(s)=\frac{\pi^\frac{s}{2}}{2(s-1)\Gamma\left(\frac{s}{2}+1\right)}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)}
\end{equation}

De même que pour $\xi$ pour que les produits partiels converges il faut que les pôles soit sommés par paires symétriques $\rho_i$ et $1-\rho_i$, Soit :

\begin{equation}
\boxed{\zeta(s)=\frac{\pi^{\frac{s}{2}}}{2(s-1)\Gamma\left(\frac{s}{2}+1\right)}\prod_{i=0}^\infty\left(1+\frac{s(s-1)}{\rho_i(1-\rho_i)}\right)}
\end{equation}

Et si on trace le graphique avec cette méthode, on obtient:

\graphfunction{zeta_form5_512}{$\zeta(s)$, par son produit de Hadamard-Weierstrass}

\notes{Donc si on possède une liste des zéros non triviaux de $\zeta(s)$ on pourra utiliser cette formule. De plus sous l'hypothèse de Riemann, les couples de pôles sont des complexes conjugués. Donc $\rho_i(1-\rho_i)=|\rho_i|^2$. On notera que ici nous avons utilisés les 512 premiers zéros non triviaux, le produit converge assez lentement dés que on s'éloigne de la ligne critique.}

\newpage
\subsection{Calcul avec la formule d'Euler-Maclaurin\verif}

On rappel que la formule d'Euler-Maclaurin (voir \ref{eq_maclaurin}) donne :
\[
\sum_a^{b-1} f(z) = \int_a^b f(z) dz - \frac{f(b)-f(a)}{2}+ \sum_{n=1}^{\infty}\frac{B_{2n}}{2n!}(f^{(2n-1)}(b)-f^{(2n-1)}(a))
\]
Si on prends $f(z)=\frac{1}{z^s}$, le dérivées n-ièmes par rapport à $z$ s'expriment par:
\[
f^{(n)}(z)=\frac{(-1)^n}{z^{s+n}}\prod_{k=0}^{n-1}(s+k)
\]
Et 
\[
f^{(2n-1)}(z)=\frac{-1}{z^{s+2n-1}}\prod_{k=0}^{2(n-1)}(s+k)
\]

La formule donne alors:

\[
\sum_a^{b-1} \frac{1}{z^s} = \int_a^b \frac{1}{z^s} dz - \frac{\frac{1}{b^s}-\frac{1}{a^s}}{2}+ \sum_{n=1}^{\infty}\frac{B_{2n}}{2n!}\left(\frac{-1}{b^{s+2n-1}}\prod_{k=0}^{2(n-1)}(s+k)-\frac{-1}{a^{s+2n-1}}\prod_{k=0}^{2(n-1)}(s+k)\right)
\]
Calculons le crochet et simplifions un peu:
\[
\sum_a^{b-1} \frac{1}{z^s} = \left[\frac{z^{1-s}}{1-s}\right]_a^b - \frac{1}{2b^s}+\frac{1}{2a^s}+ \sum_{n=1}^{\infty}\frac{B_{2n}}{2n!}\left(\frac{1}{a^{s+2n-1}}-\frac{1}{b^{s+2n-1}}\right)\prod_{k=0}^{2(n-1)}(s+k)
\]

Si on prend $a=1$, on obtient alors:

\[
\sum_1^{b-1} \frac{1}{z^s} = \frac{b^{1-s}-1}{1-s} - \frac{1}{2b^s}+\frac{1}{2}+ \sum_{n=1}^{\infty}\frac{B_{2n}}{2n!}\left(1-\frac{1}{b^{s+2n-1}}\right)\prod_{k=0}^{2(n-1)}(s+k)
\]

On peut sortir une exponentielle du produit si on écrit:

\[
\sum_1^{b-1} \frac{1}{z^s} = \frac{b^{1-s}-1}{1-s} - \frac{1}{2b^s}+\frac{1}{2}+ \sum_{n=1}^{\infty}\frac{B_{2n}}{2n!}\left(1-\frac{1}{b^{s+2n-1}}\right)s\prod_{k=1}^{2(n-1)}k\left(\frac{s}{k}+1\right)
\]
\[
\sum_1^{b-1} \frac{1}{z^s} = \frac{b^{1-s}-1}{1-s} - \frac{1}{2b^s}+\frac{1}{2}+ s\sum_{n=1}^{\infty}\frac{B_{2n}(2n-2)!}{2n!}\left(1-\frac{1}{b^{s+2n-1}}\right)\prod_{k=1}^{2(n-1)}\left(\frac{s}{k}+1\right)
\]

On peut alors simplifier les factorielles:
\[
\sum_1^{b-1} \frac{1}{z^s} = \frac{b^{1-s}-1}{1-s} - \frac{1}{2b^s}+\frac{1}{2}+ s\sum_{n=1}^{\infty}\frac{B_{2n}}{2n(2n-1)}\left(1-\frac{1}{b^{s+2n-1}}\right)\prod_{k=1}^{2(n-1)}\left(\frac{s}{k}+1\right)
\]

Si $Re(s)>1$ alors lorsque $b\to\infty$ l'expression converge vers :

\[
\boxed{\zeta(s)= \frac{1}{s-1} +\frac{1}{2}+ s\sum_{n=1}^{\infty}\frac{B_{2n}}{2n(2n-1)}\prod_{k=1}^{2(n-1)}\left(\frac{s}{k}+1\right)}
\]


\newpage
\subsection{Calcul avec la formule de sommation d'Abel}
\label{eq_zeta_abel}
On rappelle la formule de sommation d'Abel (\ref{eq_abel2}):

\[
\sum_{n=\floor{x}+1}^{\floor{y}} u_n \varphi(n)= U(y) \varphi(y)-U(x) \varphi(x)  - \int_x^y U(t)\varphi'(t) dt \quad\quad avec \quad\quad U(w)=\sum_{n=a}^{\floor{w}}u_n
\]

Si on prend $\varphi(n)=\frac{1}{n^s}$, $u_n=1$, $x=1$ :
\[
\sum_{n=1+1}^{\floor{y}} \frac{1}{n^s}=  \frac{U(y)}{y^s}- \frac{U(1)}{1^s}  - \int_1^y -s \frac{U(t)}{t^{s+1}} dt \quad\quad avec \quad\quad U(w)=\sum_{n=1}^{\floor{w}}u_n
\]
Notons que $U(w)=\floor{w}$
\[
\sum_{n=1}^{\floor{y}} \frac{1}{n^s}=  \frac{\floor{y}}{y^s} + s \int_1^y \frac{\floor{t}}{t^{s+1}} dt
\]

Mais si $\Re(s)>1$ alors :

\[
\lim_{y\to 0^+} \frac{\floor{y}}{y^s}=0
\]

Donc finalement:

\begin{equation}
\boxed{\zeta(s)=s\int_1^{\infty} \frac{\floor{t}}{t^{s+1}} dt \quad \quad avec \quad \quad Re(s)>1}
\end{equation}

Il s'agit ici d'une transformée de Mellin (voir \ref{eq_mellin}), en effet :

\begin{equation}
\boxed{\zeta(s)=s \applyop{M}{\floor{t}}{(-s)} }
\end{equation}

Mais on peut aller plus loin car la partie entière $\floor{t}$ peut se décomposer en $\floor{t}=t-\{t\}$ ou $\{t\}$ est la partie fractionnaire de $t$, donc :

\[
\zeta(s)=s\int_1^{\infty} \frac{t-\{t\}}{t^{s+1}}=s\int_1^{\infty} \frac{t}{t^{s+1}}-s\int_1^{\infty} \frac{\{t\}}{t^{s+1}}
\]
La première intégrale se calcule facilement et :

\[
\zeta(s)=s\left[\frac{t^{1-s}}{1-s}\right]_1^{\infty}-s\int_1^{\infty} \frac{\{t\}}{t^{s+1}}
\]

Mais si $Re(s)>1$ alors :

\begin{equation}
\boxed{\zeta(s)=\frac{s}{s-1}-s\int_1^{\infty} \frac{\{t\}}{t^{s+1}}\quad \quad avec \quad \quad Re(s)>0}
\end{equation}

On voit encore ici apparaitre une transformée de Mellin.

\begin{equation}
\boxed{\zeta(s)=\frac{s}{s-1}-s \applyop{M}{\{t\}}{(-s)} }
\end{equation}


Comme $\{t\}\in[-1,1]$ l'intégrale converge dés que $Re(s)>0$ par conséquent on à trouvé un prolongement.

\subsection{Développement en série de Laurent et nombres de Stieltjes\todo}
\label{eq_zeta_laurent}

Finement
\[
\boxed{\frac{1}{s-1}+\gamma+\sum_{n=1}^{\infty}(-1)^n\frac{\gamma_n}{n!}(s-1)^n\quad\quad avec\quad\quad \gamma_n=\lim_{k\to\infty}\sum_{m=1}^k \left( \frac{ln(m)^n}{m}-\frac{ln(k)^{n+1}}{n+1} \right)}
\]

\newpage
\subsection{Dérivées}

\subsubsection{Expression des dérivées k-ièmes :}
\label{eq_zeta_diff}
Les dérivées k-ième sont triviales et on peut les exprimer comme (avec $\op{D}$ l'opérateur de dérivation) :
\[
\applyop{D}{\zeta(s)}(s) = \sum_{n=1}^{\infty} \applyop{D}{n^{-s}}(s) = \sum_1^{\infty} \applyop{D}{e^{-s ln(n)} }(s)
\]
Finalement:
\[
\applyop{D}{\zeta(s)}(s)=\sum_{n=1}^{\infty} \frac{-ln(n)}{n^s}
\]
D'ou on déduit facilement, que:
\[
\boxed{\applyopp{D}{k}{\zeta(s)}(s)=\sum_{n=1}^{\infty} \frac{(-ln(n))^k}{n^s}\quad avec\quad \Re(s)>1}
\]

\subsubsection{Développement de sur la dérivée logarithmique :}
\label{eq_zeta_d_log}
Nous avons obtenu (voir \ref{eq_zeta_xi_prod}), que la dérivée logarithmique de la fonction zêta, peut s'exprimer par:

\[
\frac{\zeta'(s)}{\zeta(s)} = \frac{1}{1-s}+\frac{ln(\pi)}{2}-\frac{1}{2}\frac{\Gamma'\left(\frac{s}{2}+1\right)}{\Gamma\left(\frac{s}{2}+1\right)}+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}\right)
\]

Si on rappelle la forme de Gauss de la fonction gamma (dérivée logarithmique de la fonction gamma) (voir \ref{eq_gamma_gauss_form2}),

\[
\Gamma(s)=\frac{1}{s} \prod_{k=1}^\infty \left( 1+\frac{s}{k}\right)^{-1}\left(1+\frac{1}{k}\right)^s  
\]

Soit comme $\Gamma(s+1)=s\Gamma(s)$, en fait:

\[
\Gamma(s+1)= \prod_{k=1}^\infty \left( 1+\frac{s}{k}\right)^{-1}\left(1+\frac{1}{k}\right)^s  
\]

Et donc si on prend le logarithme de :

\[
ln\left(\Gamma\left(s+1\right)\right)=\sum_{k=1}^\infty s\ ln\left(1+\frac{1}{k}\right) - ln\left( 1+\frac{s}{k}\right)
\]
Et 
\[
ln\left(\Gamma\left(\frac{s}{2}+1\right)\right)=\sum_{k=1}^\infty \frac{s}{2}\ ln\left(1+\frac{1}{k}\right) - ln\left( 1+\frac{s}{2k}\right)
\]
Et donc la dérivée logarithmique s'exprime par:
\begin{equation}
\boxed{\frac{d}{ds} ln\left(\Gamma\left(\frac{s}{2}+1\right)\right)=\frac{1}{2}\frac{\Gamma'\left(\frac{s}{2}+1\right)}{\Gamma\left(\frac{s}{2}+1\right)}=\sum_{k=1}^\infty \left(\frac{1}{2}\ ln\left(1+\frac{1}{k}\right) - \frac{1}{2k+s}\right)}
\end{equation}

Et par conséquent si on remplace dans la dérivée logarithmique de $\zeta(s)$, on obtient :

\begin{equation}
\boxed{\frac{\zeta'(s)}{\zeta(s)} = \frac{1}{1-s}+\frac{ln(\pi)}{2}-\sum_{k=1}^\infty \left(\frac{1}{2}\ ln\left(1+\frac{1}{k}\right) - \frac{1}{2k+s}\right)+ \sum_{i=1}^\infty \left(\frac{1}{s-\rho_i}\right)}
\end{equation}

On retrouve cette formule dans \cite{ref_sean_li} (page 17). De plus on peut obtenir une seconde expression en retranchant le terme pour $s=0$, en effet :

\[
\frac{\zeta'(0)}{\zeta(0)} = 1+\frac{ln(\pi)}{2}-\sum_{k=1}^\infty \left(\frac{1}{2}\ ln\left(1+\frac{1}{k}\right) - \frac{1}{2k}\right)+ \sum_{i=1}^\infty \left(\frac{1}{-\rho_i}\right)
\]

Donc 

\begin{eqnarray*}
\frac{\zeta'(s)}{\zeta(s)}-\frac{\zeta'(0)}{\zeta(0)}&=&\frac{1}{1-s}-1-\sum_{k=1}^\infty \left(\frac{1}{2k}- \frac{1}{2k+s}\right)+ \sum_{i=1}^\infty \left(\frac{1}{\rho_i}+\frac{1}{s-\rho_i}\right)\\
&=&\frac{1}{1-s}-\frac{1-s}{1-s}-\sum_{k=1}^\infty \left(\frac{2k+s}{2k(2k+s)}- \frac{2k}{2k(2k+s)}\right)+ \sum_{i=1}^\infty \left(\frac{s-\rho_i}{\rho_i(s-\rho_i)}+\frac{\rho_i}{\rho_i(s-\rho_i)}\right)\\
&=&\frac{s}{1-s}-\sum_{k=1}^\infty \left(\frac{s}{2k(2k+s)}\right)+ \sum_{i=1}^\infty \left(\frac{s}{\rho_i(s-\rho_i)}\right)\\
\end{eqnarray*}

Finalement comme $\frac{\zeta'(0)}{\zeta(0)}=ln(2\pi)$ (voir \ref{eq_zeta_prime_log0}) on obtient l'expression suivante:

\begin{equation}
\boxed{ \frac{\zeta'(s)}{\zeta(s)}=ln(2\pi)+\frac{s}{1-s}-\sum_{k=1}^\infty \left(\frac{s}{2k(2k+s)}\right)+ \sum_{i=1}^\infty \left(\frac{s}{\rho_i(s-\rho_i)}\right)}
\end{equation}

On retrouve cette formule dans \cite{ref_sean_li} (page 17).

\newpage
\subsection{Valeurs particulières:}
\subsubsection{Série harmonique et pôle en 1 : $\zeta(1)$}
\label{eq_zeta1}
Calculons:
\[
\zeta(1)=\sum_{n=1}^{\infty} \frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+...
\]
On peut comparer la série à l'intégrale:
\[
\sum_{n=1}^{N} \frac{1}{n} \ge \int_{n=1}^{N+1}\frac{1}{x}dx
\]
Or l'intégrale se calcule facilement et donne :
\[
\int_{n=1}^{N+1}\frac{1}{x}dx=\left[ln(x)\right]_{n=1}^{N+1}=ln(1+N)
\]
Et 
\[
\lim_{N\to\infty} ln(1+N) =\infty
\]
Par conséquent la série harmonique diverge grossièrement.

\begin{equation}
	\boxed{\zeta(1)=\sum_{n=1}^{\infty} \frac{1}{n}=\infty}
\end{equation}

Et si on reprend le développement en série de Laurent (voir \ref{eq_zeta_laurent}), on peut voir que la fonction possède un pôle d'ordre 1 en 1:
\[
\zeta(s)=\frac{1}{s-1}+\gamma+\sum_{n=1}^{\infty}(-1)^n\frac{\gamma_n}{n!}(s-1)^n
\]
Donc 
\[
\lim_{s\to 1}(s-1)\zeta(s)=\lim_{s\to 1} (1+\gamma(s-1)+...)
\]
Et
\begin{equation}
\boxed{\lim_{s\to 1}(s-1)\zeta(s)=1}
\end{equation}

\notes{La non convergence de $\zeta(1)$ fourni une belle démonstration de l'infinitude des nombres premiers car si $\zeta(1)$ était fini le produit d'Euler (voir \ref{eq_zeta_euler_product}) serait fini!. Ceci montre également que les nombres Harmoniques $H_n$ divergent.}\\

\subsubsection{Problème de Bâle (par l'analyse de Fourier): $\zeta(2)$}
\label{eq_zeta2}
On part de $f(z)=z$ sur $z\in[-\pi,\pi]$ de période $T=2\pi$. Si on calcul les coefficients de la série de Fourier correspondante, on obtient :

\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)e^{-i \frac{2\pi}{T}n z} dz=\frac{1}{2\pi}\int_{-\pi}^{\pi} z e^{-inz} dz \quad\quad et \quad\quad C_0=\frac{1}{2\pi}\int_{-\pi}^{\pi} z dz=0
\]
On peut faire une intégration par parties, en posant :
\[
\intp{z}{1}{e^{-inz}}{\frac{e^{-inz}}{-in}}{-\pi}{\pi}
\]
On obtient :
\[
C_n=\frac{1}{2\pi}\left[z \frac{e^{-inz}}{-in}\right]_{-\pi}^{\pi}-\frac{1}{2\pi}\int_{-\pi}^{\pi} \frac{e^{-inz}}{-in} dz
\]
Si on développe le crochet et que on intègre :
\[
C_n=-\frac{1}{2in\pi}\left[z e^{-inz}\right]_{-\pi}^{\pi}-\frac{1}{2\pi}\left[ \frac{e^{-inz}}{i^2n^2} dz \right]_{-\pi}^{\pi} 
\]
On voit que le second terme est nul car $e^{-in\pi}=e^{in\pi}=(-1)^n$, donc :
\[
C_n=-\frac{1}{2in\pi} \pi 2 (-1)^n=\frac{(-1)^n}{n}i
\]
Maintenant d'après le théorème de Parseval \ref{eq_parseval} sur la conservation de l'énergie:

\[
\sum_{n=-\infty}^{\infty}|C_n|^2=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} |f(z)|^2 dz
\]
\[
\sum_{\begin{array}{c}n=-\infty\\n\neq0\\\end{array}}^{\infty}\frac{1}{n^2}=\frac{1}{2\pi}\int_{-\pi}^{\pi} z^2 dz=\frac{1}{2\pi}\left[ \frac{z^3}{3} \right]_{-\pi}^{\pi}=\frac{1}{2\pi}\frac{2\pi^3}{3}
\]
Donc
\[
2\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{3}
\]
Soit

\begin{equation}
\boxed{\zeta(2)=\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{\pi^2}{6}}
\end{equation}

\subsubsection{Valeur en 4 (par l'analyse de Fourier): $\zeta(4)$}
\label{eq_zeta4}
L'analyse de Fourier nous permet de calculer $\zeta(4)$ mais on ne pourra pas aller plus loin avec cette méthode, on verra une autre formule juste après pour exprimer les valeurs aux entiers pairs avec les nombres de Bernoulli.\\

On part de $f(z)=z^2$ sur $z\in[-\pi,\pi]$ de période $T=2\pi$. Si on calcul les coefficients de la série de Fourier correspondante, on obtient :
\[
C_n=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} f(z)e^{-i \frac{2\pi}{T}n z} dz=\frac{1}{2\pi}\int_{-\pi}^{\pi} z^2 e^{-inz} dz \quad\quad et \quad\quad C_0=\frac{1}{2\pi}\int_{-\pi}^{\pi} z^2 dz=\frac{1}{2\pi}\left[ \frac{z^3}{3} \right]_{-\pi}^{\pi}=\frac{\pi^2}{3}
\]
On peut faire une intégration par parties, en posant :
\[
\intp{z^2}{2z}{e^{-inz}}{\frac{e^{-inz}}{-in}}{-\pi}{\pi}
\]
On obtient :
\[
C_n=\frac{1}{2\pi}\left[z^2 \frac{e^{-inz}}{-in}\right]_{-\pi}^{\pi}-\frac{1}{2\pi}\int_{-\pi}^{\pi} 2z\frac{e^{-inz}}{-in} dz
\]
On peut voir que le crochet est nul, donc :
\[
C_n=\frac{1}{\pi{in}}\int_{-\pi}^{\pi} z e^{-inz} dz
\]
Mais d'après le résultat précédent $\int_{-\pi}^{\pi} z e^{-inz} dz=\frac{(-1)^n}{n}i2\pi$, donc:

\[
C_n=\frac{1}{\pi{in}}\frac{(-1)^n}{n}i2\pi
\]
\[
C_n=2\frac{(-1)^n}{n^2}
\]
Maintenant d'après le théorème de Parseval \ref{eq_parseval} sur la conservation de l'énergie :
\[
\sum_{n=-\infty}^{\infty}|C_n|^2=\frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}} |f(z)|^2 dz
\]
On obtient :
\[
\left(\frac{\pi^2}{3}\right)^2+2\sum_{n=1}^{\infty} \frac{4}{n^4} =\frac{1}{2\pi}\int_{-\pi}^{\pi} z^4 dz=\frac{1}{2\pi}\left[ \frac{z^5}{5} \right]_{-\pi}^{\pi}=\frac{1}{2\pi}\frac{2\pi^5}{5}
\]
Soit 
\[
8\sum_{n=1}^{\infty} \frac{1}{n^4}=\frac{\pi^4}{5}-\frac{\pi^4}{9}=\frac{\pi^4(9-5)}{45}=\frac{4\pi^4}{45}
\]
Soit 
\begin{equation}
\boxed{\sum_{n=1}^{\infty} \frac{1}{n^4}=\frac{\pi^4}{90}}
\end{equation}

\subsubsection{Valeurs aux entiers pairs : $\zeta(2k)$}
\label{eq_zeta_even}
Prenons $|z|<1$:\\

\noindent
On part de l'expression de $sin(z)$ sous forme d'un produit de Hadamard-Weierstrass (voir \ref{eq_sinus_product}), soit:
\[
sin(z)= z \prod_{n=1}^\infty \left(1-\frac{z^2}{(n\pi)^2}\right)
\]
Alors, on peut établir que le sinus cardinal à pour expression:
\[
\frac{sin(\pi z)}{\pi z}=\prod_{n=1}^\infty \left(1-\frac{z^2}{n^2}\right)
\]
Si on prend le logarithme
\[
\ln\left(sin(\pi z)\right)=ln(\pi z)+\sum_{n=1}^\infty \ln\left(1-\frac{z^2}{n^2}\right)
\]
L'astuce consiste ici à dériver cette expression par rapport à $z$, on obtient:
\[
\pi cot(\pi z)=\frac{1}{z}+\sum_{n=1}^\infty \frac{2z}{z^2-n^2}
\]
Si on multiplie par $z$
\[
z\pi cot(\pi z)=1+\sum_{n=1}^\infty \frac{2z^2}{z^2-n^2}
\]
Maintenant si on se souvient du DL de la cotangente (voir \ref{dl_cot}), c'est à dire $cot(z)=\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n z^{2n-1}$, on obtient:
\[
z\pi \sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n-1}=1+\sum_{n=1}^\infty \frac{2z^2}{z^2-n^2}
\]
On rentre le terme $\pi z$ et que on réarrange le terme de droite:
\[
\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{n=1}^\infty \frac{z^2}{n^2}\frac{1}{1-\frac{z^2}{n^2}}
\]
Comme $|z|<1$ on a $\frac{z^2}{n^2}<1$ et donc on reconnait la somme d'une suite géométrique $\frac{1}{1-\frac{z^2}{n^2}}=\sum_{k=0}^{\infty} \left(\frac{z^2}{n^2}\right)^k$:

\[
\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{n=1}^\infty \frac{z^2}{n^2}\sum_{k=0}^{\infty} \left(\frac{z^2}{n^2}\right)^k
\]

\[
\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{n=1}^\infty \sum_{k=1}^{\infty} \left(\frac{z^2}{n^2}\right)^k
\]

\[
\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{n=1}^\infty \sum_{k=1}^{\infty} \frac{1}{n^{2k}} z^{2k}
\]
Si on inverse les 2 sommes on obtient les $\zeta(2k)$:
\[
\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{k=1}^{\infty} \sum_{n=1}^\infty \frac{1}{n^{2k}} z^{2k}
\]
\[
\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{k=1}^{\infty}\zeta(2k) z^{2k}
\]
Si on sort le premier terme de la somme de gauche
\[
B_{0}+\sum_{n=1}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=1-2\sum_{k=1}^{\infty}\zeta(2k) z^{2k}
\]
Mais $B{0}=1$, donc:
\[
\sum_{n=1}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n (\pi z)^{2n}=-2\sum_{k=1}^{\infty}\zeta(2k) z^{2k}
\]
\[
\sum_{n=1}^{\infty} \frac{2^{2n-1} \pi^{2n}(-1)^{n+1}B_{2n}}{2n!} z^{2n}=\sum_{k=1}^{\infty}\zeta(2k) z^{2k}
\]
Cette égalité est valide dans le disque $|z|<1$. Mais cela ne nous empêche pas de conclure sur les coefficients qui doivent êtres égaux sur ce domaine. Finalement en égalisant les coefficients:
\[
\zeta(2k)= \frac{2^{2k-1} \pi^{2k}(-1)^{k+1}B_{2k}}{2k!}
\]

On peut simplifier un peu pour finalement obtenir

\begin{equation}
\boxed{\zeta(2k)= \frac{(2\pi)^{2k}(-1)^{k+1}B_{2k}}{2(2k)!}}
\end{equation}

\subsubsection{Valeur en zéro : $\zeta(0)$ }
\label{eq_zeta0}

\textbf{Méthode 1:} D'après la formule obtenus pour les nombres pairs (voir \ref{eq_zeta_even}):
\[
\zeta(2k)= \frac{(2\pi)^{2k}(-1)^{k+1}B_{2k}}{2(2k)!}
\]
Si on prend $k=0$, on obtient:
\[
\zeta(0)= \frac{-B_{0}}{2(0)!}=-\frac{B_0}{2}
\]
Finalement comme $B_0=1$
\begin{equation}
\boxed{\zeta(0)=-\frac{1}{2}}
\end{equation}

\textbf{Méthode 2:} Aussi, d'après la formule obtenus pour les nombres négatif (voir \ref{eq_zeta_neg}):
\[
\zeta(-k)=(-1)^k\frac{B_{k+1}}{k+1}
\]
Si on prend $k=0$, on obtient:
\[
\zeta(0)= B_{1}
\]
Comme $B_{1}=-\frac{1}{2}$, on obtient bien également:
\begin{equation}
\boxed{\zeta(0)=-\frac{1}{2}}
\end{equation}

\textbf{Méthode 3:} Aussi si on prend la limite en zéro de la relation fonctionnelle (voir \ref{eq_zeta_func_sym}):

\begin{eqnarray*}
\zeta(0)&=&\lim_{s\to 0} \pi^{s-\frac{1}{2}}\frac{\Gamma\left(\frac{1-s}{2}\right)}{\Gamma\left(\frac{s}{2}\right)}\zeta(1-s)\\
&=&\lim_{s\to 0} \pi^{-\frac{1}{2}}\frac{\Gamma\left(\frac{1}{2}\right)}{\Gamma\left(\frac{s}{2}\right)}\zeta(1-s)\\
&=&\lim_{s\to 0}\frac{\zeta(1-s)}{\Gamma\left(\frac{s}{2}\right)}\\
\end{eqnarray*}

Mais $\zeta(s)\approx \frac{1}{s-1}$ lorsque $s\to 1$ (voir \ref{eq_zeta1}) et $\Gamma(s)\approx \frac{1}{s}$ lorsque $s\to 0$ (voir \ref{eq_gamma_zero}). Donc

\begin{eqnarray*}
\zeta(0)&=& \lim_{s\to 0}\frac{\frac{1}{(1-s)-1}}{\frac{2}{s}}\\
&=& -\frac{1}{2}\\
\end{eqnarray*}

On obtient bien encore:

\begin{equation}
\boxed{\zeta(0)=-\frac{1}{2}}
\end{equation}

\subsubsection{Valeur de la dérivée en -2k sur les zéros triviaux ($k>0$): $\zeta'(-2k)$\todo}
\label{zeta_prime_neg2k}
Finalement

\begin{equation}
\boxed{\zeta'(-2k)=(-1)^k\frac{(2k)! \zeta(2k+1)}{2^{2k+1}\pi^{2k}}}
\end{equation}

\subsubsection{Valeur de la dérivée logarithmique en zéro: $\frac{\zeta'(0)}{\zeta(0)}$\todo}
\label{eq_zeta_prime_log0}

Finalement :

\[
\boxed{\frac{\zeta'(0)}{\zeta(0)}=ln(2\pi)}
\]

\subsubsection{Valeur de la dérivée en zéro : $\zeta'(0)$\todo}
\label{eq_zeta_prime0}

Si on part de la représentation avec la fonction éta valide sur $\Re(s)>0$ (voir \ref{eq_zeta_eta}):

\[
	\zeta(s)=\frac{\eta(s)}{1-2^{1-s}}
\]
Soit
\[
\zeta(s)=\frac{\sum_1^{\infty} \frac{(-1)^{n-1}}{ n^{s}}}{1-2^{1-s}}
\]
Si on dérive cette expression on obtient:
\[
\zeta'(s)=\frac{\sum_1^{\infty} \frac{(-1)^{n-1}ln(n)}{ n^{s}}}{1-2^{1-s}}+\frac{2^{s+1}ln(2)\sum_1^{\infty} \frac{(-1)^{n-1}}{ n^{s}}}{(2^s-2)^2}
\]



Finalement :

\[
\boxed{\zeta'(0)=-\frac{ln(2\pi)}{2}}
\]



\subsubsection{Valeur en l'infini : $\zeta(\infty)$}
\label{eq_zeta_infty}
La limite lorsque $s\to\infty$ est triviale, car il est clair que tous les termes au delà du premier tendent vers 0:
\begin{equation}
\boxed{\lim_{s\to \infty} \zeta(s)=\lim_{s\to \infty} \sum_{n=1}^{\infty} \frac{1}{n^s}=1}
\end{equation}

Cette limite nous donne un équivalent intéressant pour les nombres de Bernoulli, en effet nous avons obtenu (voir \ref{eq_zeta_even}) que :
\[
\zeta(2k)= \frac{2^{2k-1} \pi^{2k}(-1)^{k+1}B_{2k}}{2k!}
\]
Donc 
\[
\lim_{k\to \infty} \zeta(2k)= \lim_{k\to \infty} \frac{2^{2k-1} \pi^{2k}(-1)^{k+1}B_{2k}}{2k!}
\]
Soit 
\[
\lim_{k\to \infty} \frac{2^{2k-1} \pi^{2k}(-1)^{k+1}B_{2k}}{2k!}=1
\]
Finalement
\begin{equation}
\boxed{\lim_{k\to \infty} B_{2k} = \frac{2k!}{2^{2k-1} \pi^{2k}(-1)^{k+1}}}
\end{equation}

\subsubsection{Valeurs aux entiers négatifs : $\zeta(-k)$}
\label{eq_zeta_neg}

Partons de l'équation fonctionnelle asymétrique ( voir \ref{eq_zeta_func_asym} )

\[
\zeta(s)=\pi^{s-1} 2^s sin\left(\frac{s\pi}{2}\right)\Gamma(1-s)\zeta(1-s)
\]

Si on regarde ce que on obtient pour les entier négatifs impairs , en posant $s=-(2k+1)$: 

\[
\zeta(-(2k+1))=\pi^{-(2k+1)-1} 2^{-(2k+1)} sin\left(\frac{-(2k+1)\pi}{2}\right)\Gamma(1+(2k+1))\zeta(1+(2k+1))
\]

Mais avec $sin\left( \frac{-\pi}{2}-k\pi \right)=(-1)^{k+1}$ et $\Gamma(2(k+1))=(2k+1)! $ (voir \ref{eq_gamma_func}), cela donne :
\[
\zeta(-(2k+1))=\pi^{-(2k+1)-1} 2^{-(2k+1)} (-1)^{k+1} (2k+1)! \zeta(2(k+1))
\]
Et $\zeta(2k)= \frac{2^{2k-1} \pi^{2k}(-1)^{k+1}B_{2k}}{2k!}$ , soit $\zeta(2(k+1))= \frac{2^{2(k+1)-1} \pi^{2(k+1)}(-1)^{(k+1)+1}B_{2(k+1)}}{2(k+1)!}$(voir \ref{eq_zeta_even}), donc :

\[
\zeta(-(2k+1))=\pi^{-2(k+1)} 2^{-(2k+1)} (-1)^{k+1} (2k+1)! \frac{2^{2(k+1)-1} \pi^{2(k+1)}(-1)^{(k+1)+1}B_{2(k+1)}}{2(k+1)!}
\]
Si on réorganise :
\[
\zeta(-(2k+1))=   \frac{2^{2k+1} \pi^{2(k+1)}(2k+1)!}{2^{(2k+1)} \pi^{2(k+1)}  2(k+1)!} (-1)^{(k+1)+1}(-1)^{k+1} B_{2(k+1)}
\]
On simplifie les puissances de $\pi$ et les puissances de $2$:
\[
\zeta(-(2k+1))=   \frac{(2k+1)!}{ 2(k+1)!} (-1)^{2k+3} B_{2(k+1)}
\]
Les factorielles
\[
\zeta(-(2k+1))=   \frac{(2k+1)!}{ 2k+1!2(k+1)} (-1)^{2k+1} B_{2(k+1)}
\]
On peut alors simplifier les factorielles :
\[
\zeta(-(2k+1))=   (-1)^{2k+1} \frac{ B_{2(k+1)}}{2(k+1)}
\]
Maintenant on sait pour les entiers négatifs pairs que $\zeta(-2k)=0$, voir (\ref{eq_zeta_zero_triviaux})
\[
\zeta(-(2k+1))=   (-1)^{2k+1} \frac{ B_{(2k+1)+1}}{(2k+1)+1}
\]
Comme les nombres de Bernoulli sont nuls pour les indices impairs, on peut donc remplacer $2k+1$ par $k$ et finalement on obtient :
\begin{equation}
\boxed{\zeta(-k)=(-1)^k\frac{B_{k+1}}{k+1}}
\end{equation}

\subsection{Zéros}
\subsubsection{ Zéros triviaux : $\zeta(-2k)$ }
\label{eq_zeta_zero_triviaux}
Partons de l'équation fonctionnelle asymétrique (voir \ref{eq_zeta_func_asym} )

\[
\zeta(s)=\pi^{s-1} 2^s sin\left(\frac{s\pi}{2}\right)\Gamma(1-s)\zeta(1-s)
\]

Donc 
\[
\zeta(-2k)=\pi^{-2k-1} 2^{-2k} sin\left( -k\pi \right)\Gamma(1+2k)\zeta(1+2k)
\]

Et quelque'soit $k\in\mathbb{N}$ on a $sin\left( -k\pi \right)=0$ et si $k>0$ alors le membre de droite est un nombre fini, donc :

\begin{equation}
\boxed{\zeta(-2k)=0}
\end{equation}

\notes{Cette équation ne permet pas de calculer $\zeta(0)$ car la pour $k=0$, $\zeta(1)$ diverge dans le membre de droite. $\zeta(0)$ n'est donc pas un zéro de la fonction.}

\subsubsection{Absence de zero en dehors de la bande critique $0<\Re(s)<1$ : \verif}
\label{eq_zeta_no_zero_greater_than_one}

Si on part du produit d'Euler (voir \ref{eq_zeta_euler_product}):

\[
\zeta(s)=\prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)\\
\]

Cette représentation est valide pour $\Re(s)>1$. Pour que ce produit infini soit nul il faut nécessairement que l'un de ces termes soit nul ou bien qu'un nombre infini de termes soient de modules inférieurs à 1, Or:

\begin{eqnarray*}
|p^s|&=&p^{\Re(s)}\\
\end{eqnarray*}

Et comme $p>1$ et si on suppose $\Re(s)>1$ alors on a 

\begin{eqnarray*}
|p^s|&\ge&1\\
\left|\frac{1}{p^s}\right|&\le&1\\
\left|1-\frac{1}{p^s}\right|&\le&1\\
\left|1-\frac{1}{p^s}\right|^{-1}&\ge&1\\
\left|\prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)^{-1}\right|^{-1}&\ge&1\\
\left|\zeta(s)\right|&\ge&1\\
\end{eqnarray*}

Donc $\zeta(s)$ ne s'annule pas pour $\Re(s)>1$. De plus si $\Re(s)<0$ on alors $|\zeta(1-s)|\ge 1$ rappelle la relation fonctionnelle (voir \ref{eq_zeta_func_sym}):

\[
\zeta(s)=\pi^{s-1} 2^s \sin\left(\frac{s\pi}{2}\right)\Gamma(1-s)\zeta(1-s)
\]

Or si $\Re(s)<0$ 

\[
|\pi^{s-1} 2^s|\ge 1
\]
\[
|\zeta(1-s)|\ge 1
\]


\subsubsection{Absence de zero sur la ligne $\Re(s)=1$ (Methode 1)}
\label{eq_zeta_no_zero_on_line}
Si on part de (voir \ref{eq_zeta_euler_product}):

\begin{eqnarray*}
\zeta(s)&=&\prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)^{-1}\\
\end{eqnarray*}

Si on prend le logarithme on obtient :

\begin{eqnarray*}
ln(\zeta(s))&=& - \sum_{p\in\mathbb{P}} ln\left(1-\frac{1}{p^s}\right)\\
&=&  \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{1}{n p^{ns}}\\
\end{eqnarray*}

Maintenant si on pose $s=\sigma+i t$, on obtient :

\begin{eqnarray*}
ln(\zeta(\sigma+i t)) &=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{1}{n p^{n\sigma+i n t}} \\
&=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{1}{n} p^{-n\sigma}p^{-i n t} \\
&=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{1}{n} p^{-n\sigma}e^{-i n t\  \ln(p)} \\
&=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{1}{n} p^{-n\sigma} ( \cos(n t\ \ln(p))+i \sin(n t\ \ln(p)) ) \\
\end{eqnarray*}

Maintenant si on considère le logarithme du module, on obtient:
\begin{eqnarray*}
\ln(|\zeta(\sigma+i t)|) &=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{p^{-n\sigma}}{n}   \cos(n t\ \ln(p)) \\
\end{eqnarray*}

Et maintenant comme dirait G. Becaud si on pose $\theta=nt\ln(p)$ :

\begin{eqnarray*}
3 \ln(|\zeta(\sigma)|)+ 4 \ln(|\zeta(\sigma+i t)|) + \ln(|\zeta(\sigma+i 2 t)|) &=& 3 \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{p^{-n\sigma}}{n} + 4 \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{p^{-n\sigma}}{n}   \cos(n t\ \ln(p)) + \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{p^{-n\sigma}}{n}   \cos(n 2 t\ \ln(p))\\
&=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{p^{-n\sigma}}{n} \left(3 + 4 \cos(n t\ \ln(p)) \cos(n 2 t\ \ln(p)) \right)\\
&=& \sum_{p\in\mathbb{P}} \sum_{n=1}^\infty \frac{p^{-n\sigma}}{n} \left( 2(1+cos(\theta))^2 \right)\\
\end{eqnarray*}

Mais en fait on a trivialement par inspection que :

\[
\frac{p^{-n\sigma}}{n} \left( 2(1+cos(\theta))^2 \right) \ge 0
\]

Soit 
\[
3 \ln(|\zeta(\sigma)|)+ 4 \ln(|\zeta(\sigma+i t)|) + \ln(|\zeta(\sigma+i 2 t)|) \ge 0
\]

Ou bien 

\[
\boxed{|\zeta(\sigma)|^3 |\zeta(\sigma+i t)|^4 |\zeta(\sigma+i 2 t)| \ge 1 } \quad \forall \sigma>1 \quad et\quad \forall t\in\mathbb{R}
\]

Mais si on passe à la limite et si on considère que $\zeta(1+it)=0$ est un zero d'ordre 1 alors:

\[
\lim_{\sigma\to 1^+} |\zeta(\sigma)|^3 |\zeta(\sigma+i t)|^4 |\zeta(\sigma+i 2 t)|\ge1
\] 

Mais en fait $|\zeta(1^+)|^3$ est un pôle d'ordre 3 et $|\zeta(1^++i t)|^4$ est un zéro d'ordre 4 alors pour que l'inégalité soit vérifiée il faudrait que :

\[
\lim_{\sigma\to 1^+} |\zeta(\sigma+i 2 t)|= \infty
\] 

Ce qui est impossible car la fonction $\zeta(s)$ est analytique pour pour $s\ne 1$. Par conséquent $\boxed{\zeta(1+it) \ne 0}$, donc la fonction $\zeta(s)$ ne possède pas de zero sur la ligne $\Re(s)=1$.

\newpage
\subsection{Régularisation}

Si on prend la première définition de la fonction $\zeta(s)$ (voir \ref{eq_zeta}), celle-ci converge uniquement $\Re(s)>1$. Mais on a vu que avec la notion de prolongement analytique (voir \ref{prolongement_analytique}) qu'il \textbf{n'existe qu'une seule manière de prolonger pour tout $s$}. Autrement dit, il n'existe qu'une seule manière d'affecter une valeur à ses séries divergentes. Mais il faut bien comprendre que cette valeur n'est pas celle obtenue au sens de la convergence usuelle, cette valeur est obtenue \textbf{dans le sens plus large de la régularisation Zeta}. On vient en quelque sorte de trouver une nouvelle manière de calculer une somme infinie.\\

Pour zero nous avons obtenu $ \zeta(0)=-\frac{1}{2} $ (voir \ref{eq_zeta0}), la régularisation zeta nous permet alors d'écrire:

\[
1+1+1+1+1+...=-\frac{1}{2}
\]

Pour les entiers négatifs nous avons obtenu (voir \ref{eq_zeta_neg}) $\zeta(-k)= (-1)^k\frac{B_{k+1}}{k+1}$, la régularisation zeta nous permet alors d'écrire, par exemple:

\[
\left\{
\begin{array}{ll}
1^{\ }+2^{\ }+3^{\ }+4^{\ }+5^{\ }+6^{\ }+7^{\ }+8^{\ }+... &= -\frac{1}{12}\\
1^2+2^2+3^2+4^2+5^2+6^2+7^2+8^2+... &= 0\\
1^3+2^3+3^3+4^3+5^3+6^3+7^3+8^3+... &= \frac{1}{120}\\
...
\end{array}
\right.
\]

Il faut bien comprendre qu'il ne s'agit pas ici d'égalités au sens usuel du terme,\textbf{ mais d'une manière unique d'associer une valeur à une série divergente}. Nous pourrions faire pareil avec la série géométrique $\frac{1}{1-z}=\sum_{n=0}^{\infty} z^n$ (voir \ref{eq_dl_geometric}) par exemple on écrirait:\\

Si on prend $z=-2$, on obtient :
\[
	1-2+2^2-2^3+2^4-2^5+2^6-2^7+...=\frac{1}{1-(-2)}= \frac{1}{3}
\]

Si on prend $z=-1$, on obtient :
\[
	1-1+1-1+1-1+1-1+1-1+...=\frac{1}{1-(-1)}= \frac{1}{2}
\]

Si on prend $z=2$, on obtient :
\[
	1+2+2^2+2^3+2^4+2^5+2^6+2^7+...=\frac{1}{1-2}=-1
\]
Si on prend $z=3$, on obtient :
\[
	1+3+3^2+3^3+3^4+3^5+3^6+3^7+...=\frac{1}{1-3}=-\frac{1}{2}
\]

On pourrait en faire de même avec n'importe qu'elle série holomorphe, et considérer que l'égalité reste vraie au delà du domaine de convergence. Cette méthode trouve son sens et s'explique dans le cadre du prolongement analytique (voir \ref{prolongement_analytique} ). Du point de vue de l'information, cela signifie que l'ensemble de l'information sur la fonction est présente dans la série.



\newpage 
\section{La formule explicite de Riemann}

\subsection{Présentation}

Riemann à introduit dans \cite{ref_riemann1} la formule suivante, valide sur $Re(s)>1$, (voir \ref{eq_zeta_J} pour la démonstration):
\[
J(t)=\sum_{n=1}^\infty \frac{\pi(t^\frac{1}{n})}{n}\quad \quad et \quad \quad \frac{ln(\zeta(s))}{s}=\applyop{M}{J(t)}(-s)
\]

La formule de Perron (voir \ref{eq_perron_formula}) donne alors :

\[
J(t)=\applyopp{M}{-1}{ \frac{ln(\zeta(s))}{s} } (-s,t)\quad \quad Soit\quad\quad J(t)=\frac{1}{2i\pi}\int_{a-i\infty}^{a+i\infty} \frac{ln(\zeta(s))}{s}  t^{s} ds
\]

Riemann parvient ensuite astucieusement à inverser cette relation en s'appuyant sur la Transformée de Fourier inverse. Les détails des calculs sont passés sous silence, et finalement Riemann énonce la célèbre formule suivante:
\begin{equation}
\boxed{J(t)=Li(t)-\sum_{\rho} \left(Li(t^{\rho})+Li(t^{1-\rho})\right) -ln(2)+\int_t^\infty\frac{du}{u(u^2-1)ln(u)}}
\end{equation}

Cette ligne fait explicitement le lien entre les nombres premiers et les zéros de la fonction Zêta.

\subsection{Démonstration \workon:}
Pour montrer cette formule nous allons utiliser la Transformée de Mellin (voir \ref{eq_mellin}), et son analogue, la formule de Perron (voir \ref{eq_perron_formula}). La première idée est de trouver une expression de $\frac{ln(\zeta(s))}{s}$ puis de calculer la transformation de Perron-Mellin inverse (voir \ref{eq_perron_formula}) qui donne alors:
\[
J(t)=\applyopp{M}{-1}{ \frac{ln(\zeta(s))}{s} } (-s,t)
\]

Mais on préférera se ramener à la transformation de Perron de la dérivée, cela permettra de simplifier bon nombre de termes constants lors de l'opération de dérivation. On a montré sur la dérivée dans la transformée inverse de Perron (voir \ref{eq_perron_deriv_inverse}) que :

\[
\applyopp{M}{-1}{\hat{f}(s)}{(-s,t)}=-\frac{1}{ln(t)}\applyopp{M}{-1}{\hat{f}'(s)}{(-s,t)}
\]
Dans notre cas cela donne :
\[
\boxed{J(t)=-\frac{1}{ln(t)}\applyopp{M}{-1}{\left(\frac{ln(\zeta(s))}{s}\right)'}{(-s,t)}}
\]

Rappelons maintenant l'expression de $\zeta(s)$ sous la forme d'un produit de HW (voir \ref{eq_zeta_hw}) :
\[
\zeta(s)=\frac{\pi^\frac{s}{2}}{2(s-1)\Gamma\left(\frac{s}{2}+1\right)}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)
\]
Si on prend le logarithme, on obtient:
\[
ln(\zeta(s))= \frac{s}{2}ln(\pi)-ln(2)-ln(s-1)-ln\left( \Gamma\left(\frac{s}{2}+1\right) \right) + \sum_{\rho} ln\left(1-\frac{s}{\rho}\right)
\]
On peut retrouver cette ligne dans \cite{ref_sean_li} (page 9) Si on divise par $s$ :
\[
\frac{ln(\zeta(s))}{s}=\frac{ln(\pi)}{2}-\frac{ln(2)}{s}-\frac{ln(s-1)}{s}-\frac{ln\left( \Gamma\left(\frac{s}{2}+1\right) \right)}{s} + \frac{1}{s} \sum_{\rho} ln\left(1-\frac{s}{\rho}\right)
\]
Si on rappelle la seconde représentation de Gauss de la fonction $\Gamma$ (voir \ref{eq_gamma_gauss_form2})

\[
\Gamma(s)=\frac{1}{s} \prod_{k=1}^\infty \left( 1+\frac{s}{k}\right)^{-1}\left(1+\frac{1}{k}\right)^s 
\]
Ce qui dans notre cas donne :
\[
\Gamma\left(\frac{s}{2}\right)=\frac{2}{s} \prod_{k=1}^\infty \left( 1+\frac{s}{2k}\right)^{-1}\left(1+\frac{1}{k}\right)^{s/2} 
\]
Soit si on prend le logarithme :
\[
ln\left(\frac{s}{2}\Gamma\left(\frac{s}{2}\right)\right)=ln\left( \prod_{k=1}^\infty \left( 1+\frac{s}{2k}\right)^{-1}\left(1+\frac{1}{k}\right)^{s/2}\right)
\]
\[
ln\left(\frac{s}{2}\Gamma\left(\frac{s}{2}\right)\right)= \sum_{k=1}^\infty ln\left( \left( 1+\frac{s}{2k}\right)^{-1}\left(1+\frac{1}{k}\right)^{s/2}\right)
\]
\[
ln\left(\frac{s}{2}\Gamma\left(\frac{s}{2}\right)\right)= \sum_{k=1}^\infty \frac{s}{2}ln\left(1+\frac{1}{k}\right)-ln\left( 1+\frac{s}{2k}\right)
\]
\[
-\frac{1}{s}ln\left(\Gamma\left(\frac{s}{2}+1\right)\right)= \frac{1}{s}\sum_{k=1}^\infty ln\left( 1+\frac{s}{2k}\right)-\frac{1}{2}\sum_{k=1}^\infty ln\left(1+\frac{1}{k}\right)
\]

Si on remplace cela donne:

\[
\frac{ln(\zeta(s))}{s}=\frac{ln(\pi)}{2}-\frac{ln(2)}{s}-\frac{ln(s-1)}{s}+\frac{1}{s}\sum_{k=1}^\infty ln\left( 1+\frac{s}{2k}\right)-\frac{1}{2}\sum_{k=1}^\infty ln\left(1+\frac{1}{k}\right) + \frac{1}{s} \sum_{\rho} ln\left(1-\frac{s}{\rho}\right)
\]

Si on dérive termes à termes cela permet de supprimer tous les termes constants:

\[
\left(\frac{ln(\zeta(s))}{s}\right)'=\left(\frac{-ln(2)}{s}\right)'-\left(\frac{ln(s-1)}{s}\right)'+ \sum_{k=1}^\infty \left(\frac{ln\left( 1+\frac{s}{2k}\right)}{s}\right)'+ \sum_{\rho}\left(\frac{ln\left(1-\frac{s}{\rho}\right)}{s}\right)'
\]

Comme $ln(-A)=ln(A)+i\pi$, on peut écrire:

\[
\left(\frac{ln(\zeta(s))}{s}\right)'=\left(\frac{-ln(2)}{s}\right)'-\left(\frac{i\pi}{s}\right)'-\left(\frac{ln(1-s)}{s}\right)'+ \sum_{k=1}^\infty \left(\frac{ln\left( 1+\frac{s}{2k}\right)}{s}\right)'+ \sum_{\rho}\left(\frac{ln\left(1-\frac{s}{\rho}\right)}{s}\right)'
\]
\[
\left(\frac{ln(\zeta(s))}{s}\right)'=\left(\frac{-ln(-2)}{s}\right)'-\left(\frac{ln(1-s)}{s}\right)'+ \sum_{k=1}^\infty \left(\frac{ln\left( 1+\frac{s}{2k}\right)}{s}\right)'+ \sum_{\rho}\left(\frac{ln\left(1-\frac{s}{\rho}\right)}{s}\right)'
\]

Maintenant si on pose :

\[
\boxed{G_c(s)=\frac{ln(c)}{s}}\quad et \quad \boxed{G'_c(s)=\left(\frac{ln(c)}{s}\right)'}\quad ainsi\ que \quad \boxed{H_c(s)=\frac{ln\left(1+\frac{s}{c}\right)}{s}}\quad et \quad \boxed{H'_c(s)=\left(\frac{ln\left(1+\frac{s}{c}\right)}{s}\right)'}
\]


Cela donne :
\[
\boxed{\left(\frac{ln(\zeta(s))}{s}\right)'=-G'_{-2}\left(s\right)-H'_{-1}(s)+\sum_{k=1}^\infty H'_{2k}\left(s\right)+\sum_{\rho} H'_{-\rho}\left(s\right)}
\]

---------------------

Maintenant il ne reste plus qu'a prendre la transformation de Perron inverse terme à termes, et on devrait obtenir la formule la célèbre formule:

\[
J(t)=-\frac{1}{ln(t)}\applyopp{M}{-1}{ -G'_{-2}\left(s\right)-H'_{-1}(s)+\sum_{k=1}^\infty H'_{2k}\left(s\right)+\sum_{\rho} H'_{-\rho}\left(s\right) }{(-s,t)}
\]
Finalement
\[
\boxed{J(t)=\applyopp{M}{-1}{ -G_{-2}\left(s\right)-H_{-1}(s)+\sum_{k=1}^\infty H_{2k}\left(s\right)+\sum_{\rho} H_{-\rho}\left(s\right) }{(-s,t)}}
\]

\subsubsection{Le terme en $G_c(s)$:}
Considérons le terme:

\[
G_c(s)=\frac{ln(c)}{s}
\]

On cherche alors à calculer les transformée de Perron inverse que on notera $g_c(t)$, d'après le lien avec la transformée de Mellin (voir \ref{eq_perron_formula_inv_link_mellin}):

\begin{eqnarray*}
g_c(t)&=&\applyopp{M}{-1}{G_c(s)}{(-s,t)}\\
&=&\applyopp{M}{-1}{G_c(-s)}{(t)}\\
&=&\applyopp{M}{-1}{-\frac{ln(c)}{s}}{(t)}\\
\end{eqnarray*}

D'après le résultat obtenu (voir \ref{eq_mellin_power})
\[
\applyop{M}{\Heav(t-a)t^b}(s)=-\frac{a^{b+s}}{b+s} \quad\quad avec\quad\quad B=[-\infty,-\Re(b)]
\]
Si on prend $a=1$ et $b=0$ 
\[
\applyop{M}{\Heav(t-1)}(s)=-\frac{1}{s}\quad\quad avec\quad\quad B=[-\infty,0]
\]

Par linéarité de la transformée de Mellin on peut multiplier par la constante $ln(c)$:
\[
\applyop{M}{ln(c)\Heav(t-1)}(s)=-\frac{ln(c)}{s}
\]

Soit finalement :

\[
\boxed{g_c(t)= ln(c) \Heav(t-1)}
\]

\subsubsection{Les termes en $H_c(s)$ (Méthode 1):\workon}
Considérons le terme:
\[
H_c(s)=\frac{ln\left(1+\frac{s}{c}\right)}{s}
\]
On cherche la transformée de Perron inverse de $H_c(s)$, que on note $h_c(t)$, soit :
\begin{eqnarray*}
h_c(t)&=&\applyopp{M}{-1}{H_c(s)}{(-s,t)}\\
&=&\applyopp{M}{-1}{H_c(-s)}{(t)}\\
&=&\applyopp{M}{-1}{ -\frac{ln\left(1-\frac{s}{c}\right)}{s} }{(t)}\\
\end{eqnarray*}

Considérons maintenant la dérivée par rapport à $c$ de $h_c(t)$, cela donne :
\begin{eqnarray*}
\frac{\partial h_c(t)}{\partial c}&=&\applyopp{M}{-1}{\frac{\partial H_c(-s)}{\partial c}}{(t)}\\
&=&\applyopp{M}{-1}{ - \frac{1}{s} \frac{s}{c^2} \frac{1}{1-\frac{s}{c}} }{(t)}\\
&=&\applyopp{M}{-1}{ -\frac{1}{c(c-s)} }{(t)}\\
\end{eqnarray*}

Or cette transformée de Mellin inverse nous est connue, en effet (voir \ref{eq_mellin_power} ) :
\[
\applyop{M}{\Heav(t-a)t^b}(s)=-\frac{a^{b+s}}{b+s}
\]
Si on prend $b=-c$ et $a=1$ et que on divise par $-c$:
\[
\applyop{M}{\Heav(t-1)\frac{t^{-c}}{-c}}(s)=-\frac{1}{c(c-s)}
\]
Donc on peut écrire :
\[
\boxed{ \frac{\partial h_c(t)}{\partial c} =\Heav(t-1)\frac{t^{-c}}{-c}}
\]
Ensuite, on peut remarquer que si $t>1$ :
\[
\frac{\partial h_c(t)}{\partial c} = \frac{t^{-c}}{-c}=
\left\{
\begin{array}{lr}
\int_0^{t}t^{-c-1}dt=\left[ \frac{-t^{-c}}{c}\right]_0^{t}& Re(c)<0 \\
\int_\infty^{t}t^{-c-1}dt=\left[ \frac{-t^{-c}}{c}\right]_\infty^{t}& Re(c)>0 \\
\end{array}
\right.
\]
Maintenant si on considère $g_c(t)$ définie par:
\[
g_c(t)=
\left\{
\begin{array}{lr}
\VP\int_0^{t} -\frac{t^{-c-1}}{ln(t)} dt& Re(c)<0 \\
\VP\int_\infty^{t} -\frac{t^{-c-1}}{ln(t)} dt& Re(c)>0 \\
\end{array}
\right.
\quad\quad
\Rightarrow
\quad\quad
\frac{\partial g_c(t)}{\partial c}=
\frac{\partial h_c(t)}{\partial c}
\]
On constate alors que la dérivée par rapport au paramètre '$c$' coïncide avec celle de $h_c(t)$ et par conséquent, on peut déduire que $h_c(t)$ est de la forme :
\[
h_c(t)=
\left\{
\begin{array}{lr}
g_c(t)+C_1(t)& Re(c)<0\\
g_c(t)+C_2(t)& Re(c)>0\\
\end{array}
\right.
\]
Ou $C_1(t)$ et $C_2(t)$ sont deux constantes qui dépendent potentiellement de $t$, mais :

\begin{eqnarray*}
\lim_{c\to -1 + i\infty} h_c(t)&=&\lim_{c\to -1 + i\infty} g_c(t)+C_1(t)\\
0&=& \lim_{c\to -1+ i\infty} \VP\int_0^{t} \frac{t^{-c-1}}{ln(t)} dt+ C_1(t)\\
\end{eqnarray*}

------------------------------
\[
C_1(t)=-\lim_{c\to -1 \pm i\infty} g(c)=-\lim_{c\to -1\pm i\infty} \VP\int_0^{t} \frac{t^{-c-1}}{ln(t)} dt=-\lim_{c\to -1+i\infty} Ei(c)=\pm i\pi
\]

Et
\[
C_2(t)=-\lim_{c\to\infty} g(c)=-\lim_{c\to \infty} \VP\int_\infty^{t} \frac{t^{-c-1}}{ln(t)} dt=0
\]

Si on résume

\[
\boxed{C_1=\pm i\pi \quad et\quad C_2=0}
\]

Finalement calculons les trois cas auxquels nous sommes confrontés:\\
\begin{itemize}
\item Si on prend $c=-1$, on obtient le logarithme intégral (voir \ref{eq_ln_int}) :
\[
h(-1)=\int_0^{t} \frac{1}{ln(t)} dt \pm i\pi
\]

\[
\boxed{h(-1)=Li(t)\pm i\pi}
\]

\item Si on prend $c=2k$, on obtient:
\[
-\sum_{k=1}^\infty h(2k)=-\sum_{k=1}^\infty \int_\infty^{t} \frac{t^{-2k-1}}{ln(t)}
\]
\[
-\sum_{k=1}^\infty h(2k)=-\int_\infty^{t} \left( \frac{1}{tln(t)}\sum_{k=1}^\infty t^{-2k} \right)
\]
Avec $t>1$ alors $\sum_{k=1}^\infty t^{-2k}=\frac{1}{t^2-1}$, cela donne :
\[
\boxed{-\sum_{k=1}^\infty h(2k)= \int_t^{\infty} \frac{1}{t(t^2-1)ln(t)}}
\]

\item Si on prend $c=-\rho$, on obtient (voir \ref{eq_li_power}):
\[
h(-\rho)=\int_0^{t} \frac{t^{\rho-1}}{ln(t)} dt
\]
\[
\boxed{h(-\rho)=Li(t^\rho)\pm i\pi}
\]


\end{itemize}

\subsubsection{Les termes en $H_c(s)$ (Méthode 2):\workon}
Considérons le terme:
\[
H_c(s)=\frac{ln\left(1+\frac{s}{c}\right)}{s}
\]
On cherche la transformée de Perron inverse de $H_c(s)$, que on note $h_c(t)$, soit :
\begin{eqnarray*}
h_c(t)&=&\applyopp{M}{-1}{H_c(s)}{(-s,t)}\\
&=&\applyopp{M}{-1}{H_c(-s)}{(t)}\\
&=&\applyopp{M}{-1}{ -\frac{ln\left(1-\frac{s}{c}\right)}{s} }{(t)}\\
\applyop{M}{h_c(t)}{(s)}&=& -\frac{ln\left(1-\frac{s}{c}\right)}{s} \\
\end{eqnarray*}

Mais $\applyop{M}{f(t)}{(s)}=-\frac{1}{s}\applyop{M}{f'(t)}{(s+1)}$, soit :

\begin{eqnarray*}
-\frac{1}{s}\applyop{M}{h_c'(t)}{(s+1)}&=& -\frac{ln\left(1-\frac{s}{c}\right)}{s}\\
\applyop{M}{h_c'(t)}{(s+1)}&=& ln\left(1-\frac{s}{c}\right)\\
\applyop{M}{h_c'(t)}{(s)}&=& ln\left(1-\frac{s-1}{c}\right)\\
\end{eqnarray*}

Mais $\applyop{M}{ln(t)^{k}f(t)}{(s)}=\left(\applyop{M}{f(t)}{(s)}\right)^{(k)}$, soit:

\begin{eqnarray*}
\applyop{M}{ln(t) h_c'(t)}{(s)}&=& -\frac{1}{c\left(1-\frac{s-1}{c}\right)}\\
&=& -\frac{1}{\left(c-s+1\right)}\\
\end{eqnarray*}

Mais $\applyop{M}{\Heav(t-1)t^b}(s)=-\frac{1}{b+s}$, soit avec $b=-c-1$ :


\begin{eqnarray*}
\applyop{M}{\Heav(t-1)t^{-c-1}}(s)&=& -\frac{1}{-c+s-1}\\
\applyop{M}{-\Heav(t-1)t^{-c-1}}(s)&=& -\frac{1}{c-s+1}\\
\end{eqnarray*}

Et donc, on obtient:

\begin{eqnarray*}
\applyop{M}{ln(t) h_c'(t)}{(s)} &=& \applyop{M}{-\Heav(t-1)t^{-c-1}}(s) \\
ln(t) h_c'(t)&=& -\Heav(t-1)t^{-c-1} \\
h_c'(t)&=& \frac{-t^{-c-1}}{ln(t)} \Heav(t-1)\\
\end{eqnarray*}

D'ou si $t>1$, on obtient que $h_c(t)$ est une primitive de $-\frac{t^{-c-1}}{ln(t)}$.

\subsubsection{Reconstruction de $J(t)$\workon}

Si on rappel que :
\[
J(t)=\applyopp{M}{-1}{G\left(-2\right)+H(-1)-\sum_{k=1}^\infty H\left(2k\right)-\sum_{\rho} H\left(-\rho\right)}{(t)}
\]
Soit avec les notations introduites précedement pour les Transformées de Mellin inverses:
\[
J(t)=g\left(-2\right)+h(-1)-\sum_{k=1}^\infty h\left(2k\right)-\sum_{\rho} h\left(-\rho\right)
\]

Si $t>1$ on a alors:
\[
J(t)=(-ln(2)\pm i\pi) +(Li(t)\pm i\pi)+\int_t^{\infty} \frac{1}{t(t^2-1)ln(t)}-\sum_{\rho} (Li\left(t^{\rho}\right)\pm i\pi)
\]

En sommant sur les paires de zéros $\rho$ et $1-\rho$ et en choisissant les signes des constantes $i\pi$, on peut effacer toutes ces constantes, qui sont en fait liées au fait que le logarithme est identifié a $ki2\pi$ près:

\[
J(t)=-ln(2) +Li(t)+\int_t^{\infty} \frac{1}{t(t^2-1)ln(t)}-\sum_{\rho} ( Li\left(t^{\rho}\right)+Li\left(t^{1-\rho}\right) )
\]

Si on remet dans l'ordre:

\begin{equation}
\boxed{J(t)=Li(t)-\sum_{\rho} \left( Li\left(t^{\rho}\right)+Li\left(t^{1-\rho}\right)\right)-ln(2)+\int_t^{\infty} \frac{1}{t(t^2-1)ln(t)}}
\end{equation}

\subsection{Étude des fonctions de comptage des nombres premiers}

\subsubsection{Calcul de J(t)}

Observons la fonction $J(t)$:
\[
J(t)=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{n}})}{n}
\]
On peut remarquer que la série n'est pas une série infinie. En fait on peut voir que pour un $t$ donné, les termes sont $t^{\frac{1}{n}}$ sont décroissants, et comme il n'y a pas de nombre premier inférieur à $2$, les termes telque $t^{\frac{1}{n}}<2$ sont tous nuls. Par exemple, si on calcul $J(100)$:

\[
J(100)=\pi(100)+\frac{1}{2}\pi(100^{\frac{1}{2}})+\frac{1}{3}\pi(100^{\frac{1}{3}})+\frac{1}{4}\pi(100^{\frac{1}{4}})+\frac{1}{5}\pi(100^{\frac{1}{5}})+\frac{1}{6}\pi(100^{\frac{1}{6}})+\frac{1}{7}\pi(100^{\frac{1}{7}})+...
\]
\[
J(100)=\pi(100)+\frac{1}{2}\pi(10)+\frac{1}{3}\pi(4.6)+\frac{1}{4}\pi(3.1)+\frac{1}{5}\pi(2.5)+\frac{1}{6}\pi(2.1)+\frac{1}{7}\pi(1.9)+...
\]
\[
J(100)=25+\frac{4}{2}+\frac{2}{3}+\frac{2}{4}+\frac{1}{5}+\frac{1}{6}+0+...
\]
\[
\boxed{J(100)=28.53\bar{3}}
\]

Les termes non nuls sont les termes pour lesquels on a:

\[
t^{\frac{1}{n}}\ge 2
\]
\[
n\le ln(t-2)
\]

Le dernier terme non nul est donc $\floor{ln(t-2)}$ et $J(t)$ se réduit a :

\[
\boxed{J(t)=\sum_{n=1}^{\floor{ln(t-2)}} \frac{\pi(t^{\frac{1}{n}})}{n}}
\]

\subsubsection{Lien entre $J(t)$ et $\pi(t)$}

Si on part de la définition pour la fonction J (voir \ref{eq_zeta_J}) :
\[
J(t)=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{n}})}{n}
\]

La plupart des papiers évoquent le nom de l'inversion de Möbius pour l'étape qui va suivre et la formule inverse arrive ensuite comme par magie. Mais en fait l'inversion de Möbius est ici plus subtile, car si on applique la formule d'inversion de Möbius même généralisée ( voir \ref{eq_mobius_gen} ) il n'est pas possible d'arriver au résultat suivant.\\

Dans \cite{ref_edwards} Edwards présente l'algorithme suivant, d'abord on peut écrire que:

\[
\frac{1}{m}J(t^\frac{1}{m})=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{mn}})}{mn}
\]

Et si on soustrait cette expression de $J(t)$ pour $m=2$, alors on obtient:

\[
J(t)-\frac{1}{2}J(t^\frac{1}{2})=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{n}})}{n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{2n}})}{2n}
\]

Puis pour $m=3$, on soustrait alors tout les multiples de $2$ et de $3$ de l'expression:

\[
J(t)-\frac{1}{2}J(t^\frac{1}{2})-\frac{1}{3}J(t^\frac{1}{3})=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{n}})}{n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{2n}})}{2n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{3n}})}{3n}
\]

Maintenant les multiples de $4$ on déjà été supprimés, on soustrait alors $m=5$:
\[
J(t)-\frac{1}{2}J(t^\frac{1}{2})-\frac{1}{3}J(t^\frac{1}{3})-\frac{1}{5}J(t^\frac{1}{5})=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{n}})}{n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{2n}})}{2n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{3n}})}{3n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{5n}})}{5n}
\]
Les termes multiple de $6$ ont étés soustraits deux fois donc on doit les rajouter une fois:
\[
J(t)-\frac{1}{2}J(t^\frac{1}{2})-\frac{1}{3}J(t^\frac{1}{3})-\frac{1}{5}J(t^\frac{1}{5})+\frac{1}{6}J(t^\frac{1}{6})=\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{n}})}{n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{2n}})}{2n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{3n}})}{3n}-\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{5n}})}{5n}+\sum_{n=1}^\infty \frac{\pi(t^{\frac{1}{6n}})}{6n}
\]

\[
etc...
\]

On se convainc alors que si $m$ est un carré alors le facteur est $0$ si $m$ est facteur de $k$ nombre premier distinct alors le facteur est $(-1)^k$ ce qui correspond à la définition de la transformation de Möbius (voir \ref{eq_mobius}). Et à droite il ne reste que $\pi(t)$, et finalement:

\begin{equation}
\boxed{\pi(t)=\sum_{n=1}^\infty \frac{J(t^{\frac{1}{n}})}{n}\mu(n)}
\end{equation}

Et donc :
\begin{equation}
\boxed{\pi(t)=\sum_{n=1}^\infty \frac{\mu(n)}{n}\left[Li(t^{\frac{1}{n}})-\sum_{\rho} \left( Li\left(t^{\frac{\rho}{n}}\right)+Li\left(t^{\frac{1-\rho}{n}}\right)\right)-ln(2)+\int_{t^{\frac{1}{n}}}^{\infty} \frac{1}{t(t^2-1)ln(t)}\right]}
\end{equation}

\subsubsection{Fonction $R(t)$ de Riemann}

Riemann considère alors la fonction :

\begin{equation}
\boxed{R(t)=\sum_{n=1}^\infty \frac{\mu(n)}{n} Li(t^{\frac{1}{n}})}
\end{equation}

Cette fonction ne fait pas intervenir les zéros de la fonction zêta mais reste néanmoins une très bonne approximation de la fonction $\pi(t)$:

\[
\boxed{\pi(t)\approx R(t)}
\]

Si on veut prendre en compte les fluctuations introduites par les zéros de la fonction $\zeta$ :

\[
\boxed{\pi(t)\approx R(t)-\sum_{\rho} \left( R(t^{\rho})+R(t^{1-\rho}) \right) }
\]

Le terme $n=1$ donne l'approximation :

\[
\boxed{\pi(t)\approx Li(t)}
\]

Maintenant si on trace graphiquement les différentes approximations obtenues:

\begin{center}
\begin{tikzpicture}
\begin{axis}[
title=Approximation de $\pi(t)$ par $R(t)$ ,
xlabel={$t$},
ylabel={$ $},
legend entries={$\pi(t)$,$R(t)$},
legend pos=south east
]
\addplot [mark=none] table [x=a, y=b, col sep=comma] {./fig/fr.csv};
\addplot [blue,mark=none] table [x=a, y=c, col sep=comma] {./fig/fr.csv};
\end{axis}
\end{tikzpicture}

\begin{tabular}{cc}
\begin{tikzpicture}
\begin{axis}[
title=Approximation de $\pi(t)$ par $R(t)-\sum_{\rho} \left( R(t^{\rho})+R(t^{1-\rho}) \right)$ ,
xlabel={$t$},
ylabel={$\pi(t)$},
legend entries={$\pi(t)$,$R(t)-\sum_{\rho}$},
legend pos=south east
]
\addplot [mark=none] table [x=a, y=b, col sep=comma] {./fig/fr.csv};
\addplot [blue,mark=none] table [x=a, y=d, col sep=comma] {./fig/fr.csv};
\end{axis}
\end{tikzpicture}
&
\begin{tikzpicture}
\begin{axis}[
title=Approximation de $\pi(t)$ par $R(t)-\sum_{\rho} \left( R(t^{\rho})+R(t^{1-\rho}) \right)$ ,
xlabel={$t$},
ylabel={$\pi(t)$},
legend entries={$\pi(t)$,$R(t)-\sum_{\rho}$},
legend pos=south east
]
\addplot [mark=none] table [x=a, y=b, col sep=comma] {./fig/fr.csv};
\addplot [blue,mark=none] table [x=a, y=i, col sep=comma] {./fig/fr.csv};
\end{axis}
\end{tikzpicture}
\end{tabular}
\end{center}

\newpage
\section{Formule explicite de von Mangoldt}

\subsection{Démonstration}

Nous avons établit (voir \ref{eq_zeta_mangoldt_sum} et \ref{eq_zeta_mangoldt}) que la fonction sommatoire de von Mangoldt $\Psi(t)$ est liée à la dérivée logarithmique de la fonction zêta par une transformée de Mellin-Perron (voir \ref{eq_mellin_inv}), rappelons ici ces résultats:

\[
\Lambda(n)=
\left\{
\begin{array}{ll}
ln(p) & Si\quad n=p^k\quad et\quad k\ge 1\\
0 & Sinon\\
\end{array}\right.
\quad\quad et \quad\quad \Psi(t)=\sum_{n\le t} \Lambda(n) \quad\quad et\quad\quad \frac{\zeta'(s)}{\zeta(s)}=-s  \applyop{M}{\Psi(t)}{(-s)}
\]

Si on prend la transformation de Mellin-Perron inverse (voir \ref{eq_perron_formula}) :

\begin{eqnarray*}
 \Psi(t)&=&\applyop{M^{-1}}{-\frac{1}{s} \frac{\zeta'(s)}{\zeta(s)}}{(-s,t)}\\
 &=&\frac{1}{2i\pi}\int_{c-i\infty}^{c+i\infty} -\frac{\zeta'(s)}{\zeta(s)} \frac{t^{s}}{s} ds\\
\end{eqnarray*}

De plus nous avons obtenu (voir \ref{eq_zeta_d_log}), que la dérivée logarithmique de la fonction zêta, peut s'exprimer par:

\[
\frac{\zeta'(s)}{\zeta(s)}=ln(2\pi)+\frac{s}{1-s}-\sum_{k=1}^\infty \left(\frac{s}{2k(2k+s)}\right)+ \sum_{i=1}^\infty \left(\frac{s}{\rho_i(s-\rho_i)}\right)
\]
Si on divise par $s$ on obtient:

\[
-\frac{\zeta'(s)}{s\zeta(s)}=-\frac{ln(2\pi)}{s}-\frac{1}{1-s}+\sum_{k=1}^\infty \left(\frac{1}{2k(2k+s)}\right)- \sum_{i=1}^\infty \left(\frac{1}{\rho_i(s-\rho_i)}\right)
\]

Donc :

\begin{eqnarray*}
\Psi(t)&=&\applyop{M^{-1}}{-\frac{ln(2\pi)}{s}-\frac{1}{1-s}+\sum_{k=1}^\infty \left(\frac{1}{2k(2k+s)}\right)- \sum_{i=1}^\infty \left(\frac{1}{\rho_i(s-\rho_i)}\right)}{(-s,t)}\\
&=&\applyop{M^{-1}}{\frac{ln(2\pi)}{s}-\frac{1}{1+s}+\sum_{k=1}^\infty \left(\frac{1}{2k(2k-s)}\right)+ \sum_{i=1}^\infty \left(\frac{1}{\rho_i(s+\rho_i)}\right)}{(t)}\\
&=&ln(2\pi)\applyop{M^{-1}}{\frac{1}{s}}{(t)}-\applyop{M^{-1}}{\frac{1}{1+s}}{(t)}+\sum_{k=1}^\infty \frac{1}{2k}\applyop{M^{-1}}{\frac{1}{2k-s}}{(t)}+ \sum_{i=1}^\infty \frac{1}{\rho_i}\applyop{M^{-1}}{\frac{1}{s+\rho_i}}{(t)}\\
\end{eqnarray*}

Mais nous avons obtenu (voir \ref{eq_mellin_power}), que:

\[
\applyop{M}{\Heav(t-a)t^b}(s)=-\frac{a^{b+s}}{b+s}
\]

Si on considère respectivement:

\[
\left\{
\begin{array}{lll}
a=1 & b=0 & \applyop{M}{\Heav(t-1)}(s)=-\frac{1}{s}\\
a=1 & b=1 & \applyop{M}{\Heav(t-1)t}(s)=-\frac{1}{1+s}\\
a=1 & b=-2k & \applyop{M}{\Heav(t-1)t^{-2k}}(s)=\frac{1}{2k-s}\\
a=1 & b=\rho_i & \applyop{M}{\Heav(t-1)t^{\rho_i}}(s)=-\frac{1}{\rho_i+s}\\
\end{array}
\right.
\quad\Rightarrow\quad
\left\{
\begin{array}{lcl}
\applyop{M^{-1}}{ \frac{1}{s}        }{(t)} &=&-\Heav(t-1)          \\
\applyop{M^{-1}}{ \frac{1}{1+s}      }{(t)} &=& -\Heav(t-1)t         \\
\applyop{M^{-1}}{ \frac{1}{2k-s}     }{(t)} &=& \Heav(t-1)t^{-2k}   \\
\applyop{M^{-1}}{ \frac{1}{s+\rho_i} }{(t)} &=& -\Heav(t-1)t^{\rho_i}\\
\end{array}
\right.
\]

Cela donne si $t>1$
\begin{eqnarray*}
\Psi(t)&=&-ln(2\pi)+t+\sum_{k=1}^\infty \frac{t^{-2k}}{2k}- \sum_{i=1}^\infty \frac{t^{\rho_i}}{\rho_i}\\
&=&t-\sum_{\rho} \frac{t^{\rho}}{\rho}-ln(2\pi)+\sum_{k=1}^\infty \frac{t^{-2k}}{2k}\\
\end{eqnarray*}

Mais si on se rappele la série de Taylor du logarithme (voir \ref{eq_dl_ln})

\begin{eqnarray*}
ln(1-t)&=&-\sum_{n=1}^{\infty} \frac{t^n}{n}\\
-\frac{1}{2}ln(1-t^{-2})&=&\sum_{n=1}^{\infty} \frac{t^{-2n}}{2n}\\
\end{eqnarray*}

Et finalement si on remplace, on obtient:

\begin{equation}
\boxed{\Psi(t)=t-\sum_{\rho}\frac{t^{\rho}}{\rho} -\ln(2\pi)-\frac{1}{2}\ln(1-t^{-2}) } 
\end{equation}

Ce qui démontre la célèbre formule!

\subsection{Théorème des nombres premiers \workon}

Si on part de :

\[
\Psi(t)=t-\sum_{\rho}\frac{t^{\rho}}{\rho} -\ln(2\pi)-\frac{1}{2}\ln(1-t^{-2})
\]

Comme on a montré que si $\rho$ est un zero de $\zeta(s)$ alors $\Re(\rho)<1$ (voir \ref{eq_zeta_no_zero_on_line}). Il s'en suit directement que 

\[
\boxed{\lim_{t\to\infty}\Psi(t)=t}
\]



\newpage
\section{Annexe : Coefficients binomiaux généralisés}
\label{coef_bin}
\subsection{Définition}
On peut définir les coefficients binomiaux généralisés par le binôme de Newton:

\begin{equation}
\boxed{(x+y)^n=\sum_{k=0}^{\infty} C^k_n x^{n-k} y^{k} \quad avec \quad n\in\mathbb{C}}
\end{equation}

On montre en calculant la série de Taylor (voir \ref{dl_bin}) que les coefficients binomiaux $C^k_n$ s'écrivent alors sous la forme d'un produit:
\begin{equation}
\boxed{C^k_n=\prod_{j=1}^{k} \left(\frac{n+1}{j}-1\right)\quad avec \quad n\in\mathbb{C},\ k\in\mathbb{N}}
\end{equation}

Il est important de noter que dans le cas de puissances entières c'est à dire $n\in\mathbb{N}$ si $k>n$ alors $C^k_n=0$. (Il y a forcément le terme $j=n+1$ qui est nul dans le produit). Dans le binôme il n'est donc pas utile d'avoir une somme infinie et on obtient:

\begin{equation}
\boxed{(x+y)^n=\sum_{k=0}^{n} C^k_n x^{n-k} y^{k} \quad avec \quad n\in\mathbb{N}}
\end{equation}

La forme clause du produit bien que peut efficace du point de vue calculatoire s'exprime joliment en fonction de factorielles (voir \ref{eq_coef_bin_clause}): 
\begin{equation}
\boxed{C^k_n=\frac{n!}{k!(n-k)!}\quad avec \quad n\in\mathbb{N},\ k\in\mathbb{N}}
\end{equation}

Cette forme nous permet de généraliser totalement les coefficients binomiaux avec la fonction gamma $\Gamma(n+1)=n!$ (voir \ref{eq_gamma_rec}):

\begin{equation}
\boxed{C^k_n=\frac{\Gamma(n+1)}{\Gamma(k+1)\Gamma(n-k+1)}\quad avec \quad n\in\mathbb{C},\ k\in\mathbb{C}}
\end{equation}
\subsection{Démonstration de la forme clause}
\label{eq_coef_bin_clause}
On part de la forme en produit:
\[
C^k_n=\prod_{j=1}^{k} \left(\frac{n+1}{j}-1\right)=\prod_{j=1}^{k} \left(\frac{n+1-j}{j}\right)
\]
On peut alors sortir une factorielle
\[
C^k_n=\frac{1}{k!}\prod_{j=1}^{k} \left(n+1-j\right)
\]
On peut réécrire le produit comme
\[
C^k_n=\frac{1}{k!}(n)(n-1)...(n+1-k)
\]
\[
C^k_n=\frac{1}{k!}\frac{(n)(n-1)...1}{(n-k)(n-k-1)...1}
\]
Et on obtient bien
\begin{equation}
\boxed{C^k_n=\frac{n!}{k!(n-k)!}}
\end{equation}

\subsection{Formules diverses}
\subsubsection{Sommation sur $k$}
\label{eq_bin_sum}
Si on part du binôme :
\[
(x+y)^n=\sum_{k=0}^{n} C^k_n x^{n-k} y^{k}
\] 
Et si simplement on prend $x=1$ et $y=1$, on obtient:
\[
\boxed{\sum_{k=0}^{n} C^k_n = 2^n}
\] 
\notes{Cela nous donne une manière élégante de décomposer une puissance de $2$ en une somme d'entiers}

\subsubsection{Symétrie sur $k$}
\label{eq_bin_sym}
Considérons le cas $n,k\in\mathbb{N}$, calculons :

\[
C^{n-k}_n=\frac{n!}{(n-k)!(n-(n-k))!}=\frac{n!}{(n-k)!k!}
\]

Les coefficients binomiaux possedent donc une symétrie triviale:

\[
\boxed{C^{n-k}_n=C^k_n}
\]

\subsubsection{Relation fonctionnelle }
\label{eq_bin_func}
\[
\frac{k}{n}C^k_n=\frac{k}{n}\frac{n!}{k!(n-k)!}
\]
\[
\frac{k}{n}C^k_n=\frac{(n-1)!}{(k-1)!(n-k)!}
\]

Finalement
\[
\boxed{C^k_n=\frac{n}{k}C^{k-1}_{n-1}}
\]

\subsubsection{Triangle de Pascal}
\label{eq_bin_pascal}
Considérons le cas $n,k\in\mathbb{N}$, calculons :

\[
C^k_n+C^{k+1}_{n}=\frac{n!}{k!(n-k)!}+\frac{n!}{(k+1)!(n-k-1)!}
\]
Essayons de mettre sous le même dénominateur :
\[
C^k_n+C^{k+1}_{n}=\frac{n!(k+1)}{(k+1)!(n-k)!}+\frac{n!(n-k)}{(k+1)!(n-k)!}
\]
Soit 
\[
C^k_n+C^{k+1}_{n}=\frac{n!(1+n)}{(k+1)!(n-k)!}
\]
\[
C^k_n+C^{k+1}_{n}=\frac{(n+1)!}{(k+1)!(n-k)!}
\]
Finalement
\[
\boxed{C^k_n+C^{k+1}_{n}=C^{k+1}_{n+1}}
\]

Pour les cas entiers cela conduit au triangle de Pascal : 
\begin{center}
\begin{tabular}{|c|cccccccccc|}
\hline
$n | k$&0&1&2&3&4&5&6&7&8&9\\
\hline
0 & 1 & & & & & & & & & \\
1 & 1 & 1 & & & & & & & & \\
2 & 1 & 2 & 1 & & & & & & & \\
3 & 1 & 3 & 3 & 1 & & & & & & \\
4 & 1 & 4 & 6 & 4 & 1 & & & & & \\
5 & 1 & 5 & 10 & 10 & 5 & 1 & & & & \\
6 & 1 & 6 & 15 & 20 & 15 & 6 & 1 & & & \\
7 & 1 & 7 & 21 & 35 & 35 & 21 & 7 & 1 & & \\
8 & 1 & 8 & 28 & 56 & 70 & 56 & 28 & 8 & 1 & \\
9 & 1 & 9 & 36 & 84 & 126 & 126 & 84 & 36 & 9 & 1\\
\hline
\end{tabular}
\end{center}

\subsubsection{Approximation gaussienne des coefficients }
\label{eq_bin_gauss}

En fonction de $k$, on obtient une forme très similaire à une gaussienne. En fait on peut approximer une distribution binomiale $B(n,k)$ par une distribution normale $N(n,k)$ lorsque $n$ est grand. En fait on peut montrer que la moyenne d'une distribution binomiale est $nm$ et la variance est $np(1-p)$, donc : 

\[
B(n,k)=C_n^k p^k (1-p)^{n-k}\quad et\quad N(n,k)=\frac{1}{\sqrt{2\pi np(1-p)}} e^{-\frac{(k-np)^2}{2np(1-p)}}
\]

Si on prend $p=\frac{1}{2}$

\[
B(n,k)=C_n^k \frac{1}{2^n} \quad et\quad N(n,k)=\frac{1}{\sqrt{\pi \frac{n}{2}}} e^{-\frac{(k-\frac{n}{2})^2}{\frac{n}{2}}}
\]

Finalement on obtient 

\[
\boxed{C_n^k\approx \frac{2^n}{\sqrt{\pi \frac{n}{2}}} e^{-\frac{(k-\frac{n}{2})^2}{\frac{n}{2}}}}
\]

\notes{ On peut également montrer ce résultat à partir de la formule de Stirling \ref{eq_gamma_stirling}. }

\newpage 
\section{Annexe : Le théorème d'intégration de Green\workon}
\label{misc_green}

Soit un champ de vecteur en deux dimensions $\vec{F}(x,y)=\left(\begin{array}{c}U(x,y)\\ V(x,y)\end{array}\right)$, C un lacet et $\vec{dr}=\left(\begin{array}{c}dx\\ dy \end{array}\right)$. Le théorème de Green nous enseigne que \textbf{la circulation du champ $\vec{F}$ le long de C est équivalente au flux de $\vec{F}$ sur la surface S.}

\[
\boxed{\oint_C \vec{F} \cdot \vec{dr} \oint_C Udx +Vdy=\int\int_S \left( \frac{V}{\partial x}-\frac{U}{\partial y}\right)dx dy}
\]
\subsection{Démonstration\todo}

Soit un contour simple $C$ pouvant se décomposer en deux fonctions:


On peut alors généraliser, car quelque soit le contour $\Gamma$ choisit on peut toujours le décomposer en une somme de contours simples.

\newpage
\section{Annexes}

\subsection{Mémo sur l'opérateur dérivation}

\begin{itemize}
\item Tableau récapitulatif des propriétés :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|}
\hline
\textbf{Propriétés} & \textbf{Preuve}\\
\hline
$\applyop{D}{af(z)+bg(z)}{(z)}=a\applyop{D}{f(z)}{(z)}+b\applyop{D}{g(z)}{(z)}$&\ref{derive_linearite}\\
$\applyop{D}{f(z)g(z)}{(z)}=g(z)\applyop{D}{f(z)}{(z)}+f(z)\applyop{D}{g(z)}{(z)}$&\ref{derive_produit}\\
$\applyop{D}{\frac{1}{f(z)}}{(z)}= \frac{\applyop{D}{f(z)}{(z)}}{f(z)^2}$&\ref{derive_inverse}\\
$\applyop{D}{\frac{f(z)}{g(z)}}{(z)}=\frac{g(z)\applyop{D}{f(z)}{(z)}-f(z)\applyop{D}{g(z)}{(z)}}{g(z)^2}$&\ref{derive_quotient}\\
$\applyop{D}{f(g(z))}{(z)}=f(z)\applyop{D}{g(z)}{(z)}-\applyop{D}{f(z)}{(z)}$&\ref{derive_composition}\\
$\applyop{D}{f(z)^a}{}=af(z)^{a-1}\applyop{D}{f(z)}{(z)}$&\ref{derive_puissance}\\
\hline
\end{tabular}}
\end{center}
\vspace{1cm}

\item Tableau récapitulatif des dérivées usuelles :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Fonction} $f(z)$ & \textbf{Dérivée} $\applyop{D}{f(z)}{(z)}$ & \textbf{Preuve}\\
\hline
$z$&$1$&\ref{deriv_func_identity}\\
$\frac{1}{z}$&$-\frac{1}{z^2}$&\ref{deriv_func_inverse}\\
$z^a$&$az^{a-1}$&\ref{deriv_func_power}\\
$\frac{1}{1-z}$ & $\frac{1}{(1-z)^2}$&\ref{deriv_func_geom}\\
$\sum_{n=0}^m a_n z^n$&$\sum_{n=1}^m n a_n z^{n-1}$&\ref{deriv_func_polynome}\\

\hline
\end{tabular}}
\end{center}
\end{itemize}

\newpage
\subsection{Mémo sur les nombres de Bernoulli: $B_{2n}$}
\label{table_bern}

\begin{itemize}
\item Les nombres de Bernoulli impairs sont nuls, sauf $B_1=-\frac{1}{2}$.
\item Les nombres de Bernoulli diverges rapidement (voir \ref{eq_zeta_infty}):
\[
\lim_{k\to \infty} B_{2k} = \frac{2k!}{2^{2k-1} \pi^{2k}(-1)^{k+1}}
\]

\item Fonction génératrice (voir \ref{eq_bernoulli})
\[
\frac{z}{e^z-1}=\sum_{n=0}^{\infty} \frac{B_n}{n!} z^n
\]

\item Relation de récurrence (voir \ref{eq_bernoulli}):
\[
B_n=-\frac{1}{n+1}\sum_{k=0}^{n-1} C_{n+1}^{k} B_k 
\]

\item Tableau des 25 premiers nombres pairs de Bernoulli :
\begin{center}
\begin{tabular}{|l|l|}
\hline
Fraction & Approximation\\
\hline
\begin{tabular}{lll}
Nombre & Numérateur & Dénominateur\\
\hline
$B_{0}$&$=1 $&$1$\\
$B_{2}$&$=1$&$6$\\
$B_{4}$&$=-1$&$30$\\
$B_{6}$&$=1$&$42$\\
$B_{8}$&$=-1$&$30$\\
$B_{10}$&$=5$&$66$\\
$B_{12}$&$=-691$&$2730$\\
$B_{14}$&$=7$&$6$\\
$B_{16}$&$=-3617$&$510$\\
$B_{18}$&$=43867$&$798$\\
$B_{20}$&$=-174611$&$330$\\
$B_{22}$&$=854513$&$138$\\
$B_{24}$&$=-236364091$&$2730$\\
$B_{26}$&$=8553103$&$6$\\
$B_{28}$&$=-23749461029$&$870$\\
$B_{30}$&$=8615841276005$&$14322$\\
$B_{32}$&$=-7709321041217$&$510$\\
$B_{34}$&$=2577687858367$&$6$\\
$B_{36}$&$=-26315271553053477373$&$1919190$\\
$B_{38}$&$=2929993913841559$&$6$\\
$B_{40}$&$=-261082718496449122051$&$13530$\\
$B_{42}$&$=1520097643918070802691$&$1806$\\
$B_{44}$&$=-27833269579301024235023$&$690$\\
$B_{46}$&$=596451111593912163277961$&$282$\\
$B_{48}$&$=-5609403368997817686249127547$&$46410$\\
$B_{50}$&$=495057205241079648212477525$&$66$\\
\end{tabular}
&
\begin{tabular}{lll}
Nombre & Mantisse tronquée & exposant \\
\hline
$B_{0 }$&$= 1.000 $&$10^{0 }$\\
$B_{2 }$&$= 1.666 $&$10^{-1}$\\
$B_{4 }$&$=-3.333 $&$10^{-2}$\\
$B_{6 }$&$= 2.380 $&$10^{-2}$\\
$B_{8 }$&$=-3.333 $&$10^{-2}$\\
$B_{10}$&$= 7.575 $&$10^{-2}$\\
$B_{12}$&$=-2.531 $&$10^{-1}$\\
$B_{14}$&$= 1.166 $&$10^{0 }$\\
$B_{16}$&$=-7.092 $&$10^{0 }$\\
$B_{18}$&$= 5.497 $&$10^{1 }$\\
$B_{20}$&$=-5.291 $&$10^{2 }$\\
$B_{22}$&$= 6.192 $&$10^{3 }$\\
$B_{24}$&$=-8.658 $&$10^{4 }$\\
$B_{26}$&$= 1.425 $&$10^{6 }$\\
$B_{28}$&$=-2.729 $&$10^{7 }$\\
$B_{30}$&$= 6.015 $&$10^{8 }$\\
$B_{32}$&$=-1.511 $&$10^{10}$\\
$B_{34}$&$= 4.296 $&$10^{11}$\\
$B_{36}$&$=-1.371 $&$10^{13}$\\
$B_{38}$&$= 4.883 $&$10^{14}$\\
$B_{40}$&$=-1.929 $&$10^{16}$\\
$B_{42}$&$= 8.416 $&$10^{17}$\\
$B_{44}$&$=-4.033 $&$10^{19}$\\
$B_{46}$&$= 2.115 $&$10^{21}$\\
$B_{48}$&$=-1.208 $&$10^{23}$\\
$B_{50}$&$= 7.500 $&$10^{24}$\\
\end{tabular}\\
\hline
\end{tabular}
\end{center}

\end{itemize}



\newpage
\subsection{Mémo sur des séries de Taylor}

\begin{itemize}


\item Tableau Récapitulatif des développements usuels en zéro :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|c|l|l|c|c|}
\hline
\textbf{Fonction} & \textbf{Série} &\textbf{ Premiers termes} & \textbf{Rayon} & \textbf{Preuve}\\
\hline
$e^z$ & $\sum_{n=0}^{\infty}\frac{z^{n}}{n!}$ & $1+\frac{z}{1}+\frac{z^2}{2}+\frac{z^3}{6}+\frac{z^4}{24}+...+O(z^5)$ & $\infty$ & \ref{dl_exponentiel}\\
$a^z$&$\sum_{0}^\infty \frac{ln(a)^n}{n!} z^n$&&$\infty$&\ref{dl_power_a}\\
$sinh(z)$&$\sum_{n=0}^{\infty}\frac{z^{1+2 n}}{(1+2 n)!}$&$z+\frac{z^3}{6}+\frac{z^5}{120}+...+O(z^7)$&$\infty$&\ref{dl_sinh}\\
$sin(z)$&$\sum_{n=0}^{\infty}\frac{z^{1+2 n}}{(1+2 n)!}(-1)^n$&$z-\frac{z^3}{6}+\frac{z^5}{120}-...+O(z^7)$&$\infty$&\ref{dl_sin}\\
$cosh(z)$&$\sum_{n=0}^{\infty}\frac{z^{2n}}{2n!}$&$1+\frac{z^2}{2}+\frac{z^4}{24}+...+O(z^6)$&$\infty$&\ref{dl_cosh}\\
$cos(z)$&$\sum_{n=0}^{\infty}\frac{z^{2 n}}{(2 n)!}(-1)^n$&$1-\frac{z^2}{2}+\frac{z^4}{24}-...+O(z^6)$&$\infty$&\ref{dl_cos}\\
$\frac{z}{e^z-1}$ & $\sum_{n=0}^{\infty} \frac{B_n}{n!} z^n$&&$2\pi$&\ref{eq_bernoulli}\\
$coth(z)$&$\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!} z^{2n-1}$&$\frac{1}{z}+\frac{z}{3}-\frac{z^3}{45}+\frac{2z^5}{945}+...+O(z^6)$&$\pi$&\ref{dl_coth}\\
$cot(z)$&$\sum_{n=0}^{\infty} \frac{2^{2n} B_{2n}}{2n!}(-1)^n z^{2n-1}$&$\frac{1}{z}-\frac{z}{3}-\frac{z^3}{45}-\frac{2z^5}{945}+...+O(z^6)$&$\pi$&\ref{dl_cot}\\
$tanh(z)$&$\sum_{n=1}^{\infty} \frac{(2^{4n}- 2^{2n})B_{2n}}{2n!} z^{2n-1}$&$z-\frac{z^3}{3}+\frac{2z^5}{15}+...+O(z^6)$&$\frac{\pi}{2}$&\ref{dl_tanh}\\
$tan(z)$&$\sum_{n=1}^{\infty} \frac{(2^{2n}-2^{4n})B_{2n}}{2n!} (-1)^n z^{2n-1}$&$z+\frac{z^3}{3}+\frac{2z^5}{15}+...+O(z^6)$&$\frac{\pi}{2}$&\ref{dl_tan}\\
$\frac{1}{1-z}$&$\sum_{n=0}^{\infty} z^n$&&1&\ref{eq_dl_geometric}\\
$ln(1+z)$&$\sum_{n=1}^{\infty} \frac{(-1)^{n+1} z^n}{n}$&$z-\frac{z^2}{2}+\frac{z^3}{3}-\frac{z^4}{4}+...+O(z^5)$&1&\ref{eq_dl_ln}\\
$ln(1-z)$&$-\sum_{n=1}^{\infty} \frac{z^n}{n}$&$-z-\frac{z^2}{2}-\frac{z^3}{3}-\frac{z^4}{4}-...+O(z^5)$&1&\ref{eq_dl_ln}\\
$(b+z)^a$&$\sum_{n=0}^\infty b^{a-n}C_a^n z^n$&&&\ref{dl_bin}\\
$\sqrt{1+z}$&$\sum_{n=0}^\infty C_\frac{1}{2}^n z^n$&$1+\frac{z}{2}-\frac{z^2}{8}+\frac{z^3}{16}-\frac{5z^4}{128}+...+O(z^5)$&1&\ref{dl_sqrtroot}\\
$\frac{1}{\sqrt{1+z}}$&$\sum_{n=0}^\infty C_{-\frac{1}{2}}^n z^n$&$1-\frac{z}{2}+\frac{3z^2}{8}-\frac{5z^3}{16}+\frac{35z^4}{128}+...+O(z^5)$&1&\ref{dl_invsqrtroot}\\
$arctan(z)$&$\sum_{n=0}^{\infty} (-1)^n \frac{z^{2n+1}}{2n+1}$&$z-\frac{z^3}{3}+\frac{z^5}{5}-\frac{z^7}{7}+...+O(z^9)$&&\ref{dl_arctan}\\
$arcsin(z)$&$\sum_{n=0}^\infty C_{-\frac{1}{2}}^n (-1)^n \frac{z^{2n+1}}{2n+1}$&&&\ref{dl_arcsin}\\
$arccos(z)$&$\frac{\pi}{2}-\sum_{n=0}^\infty C_{-\frac{1}{2}}^n (-1)^n \frac{z^{2n+1}}{2n+1}$&&&\ref{dl_arccos}\\
$\frac{z}{1-z-z^2}$&$\sum_{n=0}^{\infty}F_n z^z$&$z+z^2+2z^3+3z^4+5z^5+...+O(z^7)$&&\ref{eq_residus_fib}\\
$erf(z)$&$\frac{2}{\sqrt{\pi}} \sum_{n=1}^{\infty}  \frac{(-1)^n z^{2n+1}}{n!(2n+1)}$& & $\infty$ &\ref{dl_erf}\\
\hline
\end{tabular}}
\end{center}


\end{itemize}

\newpage
\subsection{Mémo sur les transformées de Fourier}

\begin{itemize}
\item Tableau récapitulatif des propriétés :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|}
\hline
\textbf{Propriétés} & \textbf{Preuve}\\
\hline
$\applyop{F}{a f(t)+b g(t)}(s)=a\applyop{F}{f(t)}(s)+b\applyop{F}{g(t)}(s)$&\ref{eq_fourier_lin}\\
$\applyop{F}{f(a t)}(s)=\frac{1}{a}\applyop{F}{f(t)}(s)$&\ref{eq_fourier_cont}\\
$\applyop{F}{f(a+t)}(s)=e^{is a}\applyop{F}{f(t)}(s)	$&\ref{eq_fourier_trans}\\
$\applyop{F}{f(t)e^{iat}}(s)=\applyop{F}{f(t)}(s-a)$&\ref{eq_fourier_mod}\\
$\applyop{F}{f(t)\star g(t)}(s)=\applyop{F}{f(t)}(s)\applyop{F}{g(t)}(s)$&\ref{eq_fourier_conv}\\
$\applyop{F}{f'(t)}(s)=i s \applyop{F}{f(t)}(s) $&\ref{eq_fourier_deriv}\\
$\applyop{F}{f^{(n)}(t)}(s)=(i s)^n\applyop{F}{f(t)}(s)$&\ref{eq_fourier_deriv}\\
\hline
\end{tabular}}
\end{center}
\vspace{1cm}

\item Tableau récapitulatif des transformées usuelles :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Fonction} $f(t)$ & \textbf{Transformée de Fourier} $\applyop{F}{f(t)}{(s)}$ & \textbf{Preuve}\\
\hline
$\Pi_T$&$\frac{2sin(s \frac{T}{2})}{s}$&\ref{eq_fourier_porte}\\
$\frac{e^{-\frac{t^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}$& $e^{-\frac{(\sigma s)^2}{2}}$ &\ref{eq_fourier_gaussienne}\\
\hline
\multicolumn{3}{|c|}{Distributions}\\
\hline
$\delta(t-a)$& $e^{-isa}$& \ref{eq_fourier_dirac}\\
$\delta(t)$& $\mathbbm{1}(s)$& \ref{eq_fourier_dirac}\\
$\mathbbm{1}(s)$&$2\pi \delta(s)$&\ref{eq_fourier_int1} ou \ref{eq_fourier_int2}\\
$cos(at)$&$\pi ( \delta(s-a)+\delta(s+a) )$&\ref{eq_fourier_cos}\\
$sin(at)$&$-i\pi ( \delta(s-a)-\delta(s+a) )$&\ref{eq_fourier_sin}\\
\hline
\end{tabular}}
\end{center}
\vspace{1cm}

\item Références : \cite{ref_bateman1} et \cite{ref_bateman2}

\end{itemize}

\newpage
\subsection{Mémo sur les transformées de Mellin}


\begin{itemize}

\item Tableau récapitulatif des propriétés :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|}
\hline
\textbf{Propriétés} & \textbf{Preuve}\\
\hline
$\applyop{M}{a f(t) + b g(t)}{(s)}=a\applyop{M}{f(t)}{(s)} + b\applyop{M}{g(t)}{(s)}$&\ref{eq_mellin_lin}\\
$\applyop{M}{f(at)}{(s)}=a^{-s}\applyop{M}{f(t)}{(s)}$&\ref{eq_mellin_cont}\\
$\applyop{M}{f'(t)}{(s)}=-(s-1)\applyop{M}{f(t)}{(s-1)}$&\ref{eq_mellin_deriv}\\
$\applyop{M}{f(t)}{(s)}=-\frac{1}{s}\applyop{M}{f'(t)}{(s+1)}$&\ref{eq_mellin_deriv}\\
$\applyop{M}{f^{(k)}(t)}{(s)}=(-1)^k \left(\prod_{n=1}^{k}(s-n)\right)\applyop{M}{f(t)}{(s-k)}$&\ref{eq_mellin_deriv}\\
$\applyop{M}{ln(t)^{k}f(t)}{(s)}=\left(\applyop{M}{f(t)}{(s)}\right)^{(k)}$&\ref{eq_mellin_mul_ln}\\
$\applyop{M}{t^a f(t)}{(s)}=\applyop{M}{f(t)}{(s+a)}$&\ref{eq_mellin_mul_power}\\
$\applyop{M}{f(t^a)}{(s)}=\frac{1}{|a|}\applyop{M}{f(t)}{\left(\frac{s}{a}\right)}$&\ref{eq_mellin_var_power}\\
\hline
$\applyopp{M}{-1}{\hat{f}(s)}{(t)}=\frac{1}{ln(t)}\applyopp{M}{-1}{\hat{f}'(s)}{(t)}$&\ref{eq_mellin_deriv_inverse}\\
\hline
\end{tabular}}
\end{center}
\vspace{1cm}

\item Tableau récapitulatif des transformées usuelles :\\
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Fonction} $f(t)$ & \textbf{Transformée de Mellin} $\applyop{M}{f(t)}{(s)}$ & \textbf{Bande de validité} $B$ & \textbf{Preuve}\\
\hline
$\delta(t-a)$&$a^{s-1}$&  &\ref{eq_mellin_delta}\\
$\Heav(t-a)t^b$&$-\frac{a^{b+s}}{b+s}$&$ B=[-\infty,-\Re(b)] $&\ref{eq_mellin_power}\\
$e^{-at}$&$a^{-s}\Gamma(s)$&$B=[0,\infty]$&\ref{eq_mellin_exp}\\
$e^{-at^b}$&$\frac{a^{-\frac{s}{b}}}{b}\Gamma\left(\frac{s}{b}\right)$&$B=[0,\infty]$&\ref{eq_mellin_exp_gen}\\
$e^{iat}$&$\frac{e^{i\frac{s\pi}{2}}}{a^s}\Gamma(s) $&$B=[0,1]$&\ref{eq_mellin_exp_img}\\
$sin(t)$&$\Gamma(s)sin\left( \frac{s\pi}{2} \right)$& &\ref{eq_mellin_sin}\\ 
$cos(t)$&$\Gamma(s)cos\left( \frac{s\pi}{2} \right)$& &\ref{eq_mellin_cos}\\
$(t+1)^{-1}$&$\frac{\pi}{sin(\pi s)}$& &\ref{eq_mellin_deriv_log}\\
$ln(t+1)$&$\frac{\pi}{s\ sin(\pi s)}$&&\ref{eq_mellin_log}\\
$\frac{1}{e^{t}-1}$&$\Gamma(s) \zeta(s)$&&\ref{eq_zeta_gamma}\\
\hline
\end{tabular}}
\end{center}
\vspace{1cm}

\item Références : \cite{ref_bateman1} et \cite{ref_bateman2}

\end{itemize}

\newpage
\subsection{Mémo sur les convolutions de Dirichlet}
\begin{center}
\begin{tabular}{|c|c||c||c|}
\hline
$f$&$g$&$f\star g$& \textbf{Preuve}\\
\hline
$f$&$\delta_1$&$f$&\ref{eq_diriclet_delta}\\
\hline
$\mathbbm{1}$&$\mu$&$\delta_1$&\ref{eq_dirichlet_mu}\\
\hline
$\mathbbm{1}$&$\mathbbm{1}$&$d$&\ref{eq_dirichlet_d}\\
$d$&$\mu$&$\mathbbm{1}$&\\
\hline
$I$&$\mathbbm{1}$&$\sigma$&\ref{eq_dirichlet_sigma}\\
$\sigma$&$\mu$&$I$&\\
\hline
$\varphi$&$\mathbbm{1}$&$I$&\ref{eq_dirichlet_phi}\\
$I$&$\mu$&$\varphi$&\\
\hline
\end{tabular}
\end{center}

\subsection{Mémo sur la fonction Zêta de Riemann}

\begin{center}
\begin{tabular}{cc}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|c|}
\hline
\multicolumn{3}{|c|}{Représentations}\\
\hline
Représentation $\zeta(s)$ & Domaine &  Preuve\\
\hline
$\displaystyle \sum_{n=1}^{\infty}n^{-s}$& $\Re(s)>1$ & \ref{eq_zeta}\\
$\displaystyle\prod_{p\in\mathbb{P}} \left(1-\frac{1}{p^s}\right)^{-1}$&$\Re(s)>1$&\ref{eq_zeta_euler_product}\\
$\displaystyle \frac{\eta(s)}{\left(1-2^{1-s}\right)}$&$\Re(s)>0$&\ref{eq_zeta_eta}\\
$\displaystyle \frac{e^{\left(ln(2\pi)-\frac{1}{2}\gamma-1\right)s}}{2(s-1)\Gamma\left(\frac{s}{2}+1\right)}\prod_{i=0}^\infty\left(1-\frac{s}{\rho_i}\right)e^{\frac{s}{\rho_i}}$&$\mathbb{C}-1$&\ref{eq_zeta_hw}\\

\hline
\end{tabular}}
&
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|}
\hline
\multicolumn{2}{|c|}{Équations fonctionnelles}\\
\hline
Équation & Preuve \\
\hline
$\displaystyle \pi^{-\frac{s}{2}}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\pi^{-\frac{1-s}{2}}\Gamma\left(\frac{1-s}{2}\right)\zeta(1-s)$ & \ref{eq_zeta_func_sym}\\
$\displaystyle \zeta(s)=\pi^{s-1} 2^s sin\left(\frac{s\pi}{2}\right)\Gamma(1-s)\zeta(1-s)$ & \ref{eq_zeta_func_asym}\\
\hline
\end{tabular}}
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{cc}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|c|}
\hline
\multicolumn{3}{|c|}{Transformées de Mellin-Perron}\\
\hline
Équation & Fonction &  Preuve\\
\hline
$\displaystyle s \applyop{M}{J(t)}(-s) = ln(\zeta(s)) $& $J(t)$ &\ref{eq_zeta_J}\\
$\displaystyle s \applyop{M}{M(t)}(-s) = \frac{1}{\zeta(s)} $& $M(t)$ &\ref{eq_zeta_mertens}\\
$\displaystyle  s \applyop{M}{\Psi(t)}(-s) = -\frac{\zeta'(s)}{\zeta(s)} $& $\Psi(t)$ &\ref{eq_zeta_mangoldt_sum}\\
$\displaystyle \applyop{M}{\frac{1}{e^{t}-1}}{(s)} = \Gamma(s) \zeta(s)$& $\frac{1}{e^{t}-1}$ &\ref{eq_zeta_gamma}\\
$\displaystyle s \applyop{M}{\floor{t}}{(-s)} = \zeta(s)$& $\floor{t}$ &\ref{eq_zeta_abel}\\
$\displaystyle \frac{s}{s-1}-s \applyop{M}{\{t\}}{(-s)}= \zeta(s)$& $\{t\}$ &\ref{eq_zeta_abel}\\
\hline
\end{tabular}}
&
{\renewcommand{\arraystretch}{2}
\begin{tabular}{|l|c|c|}
\hline
\multicolumn{3}{|c|}{Liens avec les fonctions arithmétiques}\\
\hline
Équation & Fonction &  Preuve\\
\hline
$\displaystyle \frac{ln(\zeta(s))}{s}= \int_2^{\infty} \frac{\pi(x)}{x(x^s-1)} dx $& $\pi(n)$ &\ref{eq_zeta_pi}\\
$\displaystyle \frac{1}{\zeta(s)}=\sum_{n=1}^\infty \frac{\mu(n)}{n^s}$& $\mu(n)$&\ref{eq_zeta_mu}\\
$\displaystyle \zeta^2(s)=\sum_{j=1}^\infty \frac{d(n)}{n^s}$&$d(n)$&\ref{eq_zeta_d}\\
$\displaystyle \zeta(s)\zeta(s-1)=\sum_{j=1}^\infty \frac{\sigma(n)}{n^s}$&$\sigma(n)$&\ref{eq_zeta_sigma}\\
$\displaystyle \frac{\zeta(s-1)}{\zeta(s)}=\sum_{n=1}^{\infty}\frac{\varphi(n)}{n^s}$&$\varphi(n)$&\ref{eq_zeta_ind_euler}\\
$\displaystyle \frac{\zeta'(s)}{\zeta(s)}=-\sum_{n=1}^{\infty} \frac{\Lambda(n)}{n^s}$&$\Lambda(n)$&\ref{eq_zeta_mangoldt}\\
\hline
\end{tabular}}
\end{tabular}
\end{center}

\newpage
\subsection{Sphères de Riemann}
\begin{center}
\begin{tabular}{ccc}
$\zeta(s)$&$\Gamma(s)$&$\psi(s)$ \\
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/zeta_form3_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/f_gamma_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/f_digamma_sphere}{0}{9}}\\
$sin(s)$&$cos(s)$&$tan(s)$ \\
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_sin_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_cos_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_tan_sphere}{0}{9}}\\
$sinh(s)$&$cosh(s)$&$tanh(s)$ \\
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_sinh_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_cosh_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_tanh_sphere}{0}{9}}\\
$exp(s)$&$\hat{\xi}(s)$&\\
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/func_exp_sphere}{0}{9}}&
{\animategraphics[loop,controls,width=5cm]{10}{./img/cpp/spheres/f_xi_hat_sphere}{0}{9}}&\\
\end{tabular}

\end{center}

\newpage
\subsection{Chronologie de quelques protagonistes}
\newcommand{\sizechrono}{2.2cm}
\begin{center}
\begin{tabular}{lr}
\includegraphics[width=\sizechrono]{./img/frise/Wallis.png}&\includegraphics[width=\sizechrono]{./img/frise/Bernoulli.png}\\
Wallis - John (1616-1703)& Bernoulli - Jacques (1654-1705)\\
\includegraphics[width=\sizechrono]{./img/frise/Taylor.png}&\includegraphics[width=\sizechrono]{./img/frise/Euler.png}\\ Taylor - Brook (1685-1731)& Euler - Leonhard (1707-1783)\\
\includegraphics[width=\sizechrono]{./img/frise/Fourier.png}&\includegraphics[width=\sizechrono]{./img/frise/Fresnel.png}\\
 Fourier - Jean Baptiste Joseph (1768-1830)& Fresnel - Augustin Jean (1788-1827)\\
 \includegraphics[width=\sizechrono]{./img/frise/Gauss.png}&\includegraphics[width=\sizechrono]{./img/frise/Cauchy.png}\\ 
Gauss - Johann Carl Friedrich (1777-1855)&Cauchy - Augustin Louis (1789-1857)\\
\includegraphics[width=\sizechrono]{./img/frise/Dirichlet.png}&\includegraphics[width=\sizechrono]{./img/frise/Weierstrass.png}\\
Dirichlet - Johann Peter Gustav Lejeune (1805-1859)& Weierstrass - Karl Theodor Wilhelm (1815-1897)\\
\includegraphics[width=\sizechrono]{./img/frise/Riemann.png}&\includegraphics[width=\sizechrono]{./img/frise/Jensen.png}\\
 Riemann - Bernhard (1826-1866)& Jensen - Johan (1859-1925) \\
\includegraphics[width=\sizechrono]{./img/frise/Mellin.png}&\includegraphics[width=\sizechrono]{./img/frise/Hadamard.png}\\
 Mellin - Robert Hjalmar (1854-1933)& Hadamard - Jacques Salomon (1865-1963)\\
\end{tabular}\\
Et bien d'autres...
\end{center}

\newpage
\bibliography{Analyse} % mon fichier de base de données s'appelle Analyse.bib

\end{document}
